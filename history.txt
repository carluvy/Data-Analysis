 1/1:
def locate_card(cards, query):
    pass
 1/2:
def locate_card(cards, query):
    pass
 1/3:
def locate_card(cards, query):
    pass

cards = [13, 11, 10, 7, 4, 3, 1, 0]
query = 7
output = 3
 1/4:
def locate_card(cards, query):
    pass

cards = [13, 11, 10, 7, 4, 3, 1, 0]
query = 7
output = 3

result = locate_card(cards, query)
print(result)
 1/5:
def locate_card(cards, query):
    pass

cards = [13, 11, 10, 7, 4, 3, 1, 0]
query = 7
output = 3

result = locate_card(cards, query)
print(result)

result == output
 1/6: locate_card(**test['input']) == test['output']
 1/7:
test = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 7
    },
    'output': 3
}

locate_card(**test['input']) == test['output']
 1/8:
test = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 7
    },
    'output': cards[3]
}

locate_card(**test['input']) == test['output']
 1/9:
test = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 7
    },
    'output': cards[3]
}

locate_card(**test['input']) == test['output']
1/10:
def locate_card(cards, query):
    pass

cards = [13, 11, 10, 7, 4, 3, 1, 0]
query = 7
output = cards[3]

result = locate_card(cards, query)
print(result)

result == output
1/11:
def locate_card(cards, query):
    pass

cards = [13, 11, 10, 7, 4, 3, 1, 0]
query = 7
output = cards[3]

result = locate_card(cards, query)
print(result)

result == output
1/12:
test = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 7
    },
    'output': 3
}

locate_card(**test['input']) == test['output']
1/13:
def locate_card(cards, query):
    cards = [13, 11, 10, 7, 4, 3, 1, 0]
    query = 7
    output = cards[3]

result = locate_card(cards, query)
print(result)

result == output
1/14:
test = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 7
    },
    'output': 3
}

locate_card(**test['input']) == test['output']
1/15: tests.append(test)
1/16:
test = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 7
    },
    'output': 3
}

tests = []
tests.append(test)

locate_card(**test['input']) == test['output']
1/17:
test = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 7
    },
    'output': 3
}

test_1 = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 3
    },
    'output': 5
}

tests = []
tests.append(test)
tests.append(test_1)

locate_card(**test['input']) == test['output']

tests
1/18:
test = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 7
    },
    'output': 3
}

test_1 = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 3
    },
    'output': 5
}

test_2 = {
    'input': { 
        'cards': [13, 11, 10, 7, 4, 3, 1, 0], 
        'query': 13
    },
    'output': 0
}

tests = []
tests.append(test)
tests.append(test_1)
tests.append(test_2)

locate_card(**test['input']) == test['output']

tests
 2/1:
x = 0
while x<20:
    x = x +3
print(x)
 4/1:
x = 0
while x<20:
    x+=3
print(x)
 5/1: runfile('C:/Users/Carla/.spyder-py3/temp.py', wdir='C:/Users/Carla/.spyder-py3')
 5/2: runfile('C:/Users/Carla/.spyder-py3/temp.py', wdir='C:/Users/Carla/.spyder-py3')
 5/3: runfile('C:/Users/Carla/.spyder-py3/temp.py', wdir='C:/Users/Carla/.spyder-py3')
 5/4: runcell(0, 'C:/Users/Carla/.spyder-py3/untitled0.py')
 5/5: runcell(0, 'C:/Users/Carla/.spyder-py3/untitled0.py')
 6/1: runcell(0, 'C:/Users/Carla/.spyder-py3/untitled0.py')
 6/2: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 6/3: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 6/4: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 6/5: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 6/6: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 6/7: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 6/8: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 6/9: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/10: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/11: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/12: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/13: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/14: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/15: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/16: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/17: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/18: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/19: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/20: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
6/21: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 8/1: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 8/2: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 8/3: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 8/4: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 8/5: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 8/6: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 8/7: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 8/8: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 8/9: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
8/10: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 9/1: runfile('C:/Users/Carla/.spyder-py3/temp.py', wdir='C:/Users/Carla/.spyder-py3')
 9/2: runfile('C:/Users/Carla/.spyder-py3/temp.py', wdir='C:/Users/Carla/.spyder-py3')
 9/3: runfile('C:/Users/Carla/.spyder-py3/temp.py', wdir='C:/Users/Carla/.spyder-py3')
 9/4: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 9/5: runfile('C:/Users/Carla/.spyder-py3/temp.py', wdir='C:/Users/Carla/.spyder-py3')
 9/6: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
 9/7: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/1: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/2: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/3: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/4: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/5: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/6: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/7: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/8: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/9: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/10: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/11: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/12: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/13: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/14: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/15: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/16: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
10/17: runfile('C:/Users/Carla/.spyder-py3/untitled0.py', wdir='C:/Users/Carla/.spyder-py3')
13/1:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')
   

cost_per_brand = df.groupby(['Year', 'Brand']).agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')
13/2:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby(['Year', 'Brand']).agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')
13/3:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby(['Brand']).agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')
13/4:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby(['Brand']).agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie')
13/5:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby(['Brand']).agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie', subplots=True)
13/6:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby(['Brand']).agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie', subplots=True)
13/7:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie', subplots=True)
13/8:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')
13/9:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby(['TV Viewers','Brand']).agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')
13/10:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby(['TV Viewers','Brand']).agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='line')
13/11:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='line')

cost_per_brand = df.groupby('TV Viewers').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')
13/12:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('TV Viewers').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='line')
13/13:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='line')
13/14:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='line')
13/15:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='line')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='line')
13/16:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='line')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/17:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='scatter')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/18:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='box')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/19:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='box')
13/20:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/21:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie', label="")

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/22:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie', label='')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/23:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie', label='')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/24:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie', subplots=True, label='')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/25:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie', subplots=False)

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/26:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie', subplots=True)

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/27:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='pie', subplots=True)

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='hist')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/28:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/29:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(tv_viewers_by_brand=('TV Viewers', 'sum'))
cost_per_brand.plot(kind='bar')

cost_per_brand = df.groupby('Brand').agg(youtube_viewers_by_brand=('Youtube Views', 'sum'))
cost_per_brand.plot(kind='bar')
13/30:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

tv_viewers_per_brand = df.groupby('Brand').agg(tv_viewers_by_brand=('TV Viewers', 'sum'))
tv_viewers_per_brand.plot(kind='bar')

youtube_viewers_per_brand = df.groupby('Brand').agg(youtube_viewers_by_brand=('Youtube Views', 'sum'))
youtube_viewers_per_brand.plot(kind='bar')

youtube_likes_per_brand = df.groupby('Brand').agg(youtube_likes_by_brand=('Youtube Likes', 'sum'))
youtube_likes_per_brand.plot(kind='bar')
13/31:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

tv_viewers_per_brand = df.groupby('Brand').agg(tv_viewers_by_brand=('TV Viewers', 'sum'))
tv_viewers_per_brand.plot(kind='bar')

youtube_viewers_per_brand = df.groupby('Brand').agg(youtube_viewers_by_brand=('Youtube Views', 'sum'))
youtube_viewers_per_brand.plot(kind='bar')

youtube_likes_per_brand = df.groupby('Brand').agg(youtube_likes_by_brand=('Youtube Likes', 'sum'))
youtube_likes_per_brand.plot(kind='bar')

youtube_likes_per_year = df.groupby('Year').agg(youtube_views_by_year=('Youtube Views', 'sum'))
youtube_likes_per_year.plot(kind='line')
13/32:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

tv_viewers_per_brand = df.groupby('Brand').agg(tv_viewers_by_brand=('TV Viewers', 'sum'))
tv_viewers_per_brand.plot(kind='bar')

youtube_viewers_per_brand = df.groupby('Brand').agg(youtube_viewers_by_brand=('Youtube Views', 'sum'))
youtube_viewers_per_brand.plot(kind='bar')

youtube_likes_per_brand = df.groupby('Brand').agg(youtube_likes_by_brand=('Youtube Likes', 'sum'))
youtube_likes_per_brand.plot(kind='bar')

youtube_likes_per_year = df.groupby(['Brand','Year']).agg(youtube_views_by_year=('Youtube Views', 'sum'))
youtube_likes_per_year.plot(kind='line')
13/33:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

tv_viewers_per_brand = df.groupby('Brand').agg(tv_viewers_by_brand=('TV Viewers', 'sum'))
tv_viewers_per_brand.plot(kind='bar')

youtube_viewers_per_brand = df.groupby('Brand').agg(youtube_viewers_by_brand=('Youtube Views', 'sum'))
youtube_viewers_per_brand.plot(kind='bar')

youtube_likes_per_brand = df.groupby('Brand').agg(youtube_likes_by_brand=('Youtube Likes', 'sum'))
youtube_likes_per_brand.plot(kind='bar')

youtube_likes_per_year = df.groupby(['Brand','Year']).agg(youtube_views_by_year=('Youtube Views', 'sum'))
youtube_likes_per_year.plot(kind='hist')
13/34:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

tv_viewers_per_brand = df.groupby('Brand').agg(tv_viewers_by_brand=('TV Viewers', 'sum'))
tv_viewers_per_brand.plot(kind='bar')

youtube_viewers_per_brand = df.groupby('Brand').agg(youtube_viewers_by_brand=('Youtube Views', 'sum'))
youtube_viewers_per_brand.plot(kind='bar')

youtube_likes_per_brand = df.groupby('Brand').agg(youtube_likes_by_brand=('Youtube Likes', 'sum'))
youtube_likes_per_brand.plot(kind='bar')

youtube_likes_per_year = df.groupby('Year').agg(youtube_views_by_year=('Youtube Views', 'sum'))
youtube_likes_per_year.plot(kind='hist')
13/35:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

tv_viewers_per_brand = df.groupby('Brand').agg(tv_viewers_by_brand=('TV Viewers', 'sum'))
tv_viewers_per_brand.plot(kind='bar')

youtube_viewers_per_brand = df.groupby('Brand').agg(youtube_viewers_by_brand=('Youtube Views', 'sum'))
youtube_viewers_per_brand.plot(kind='bar')

youtube_likes_per_brand = df.groupby('Brand').agg(youtube_likes_by_brand=('Youtube Likes', 'sum'))
youtube_likes_per_brand.plot(kind='bar')

youtube_likes_per_year = df.groupby('Year').agg(youtube_views_by_year=('Youtube Views', 'sum'))
youtube_likes_per_year.plot(kind='bar')
13/36:
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('superbowl_commercials.csv')


cost_per_brand = df.groupby('Brand').agg(cost_by_brand=('Estimated Cost', 'sum'))
cost_per_brand.plot(kind='bar')

tv_viewers_per_brand = df.groupby('Brand').agg(tv_viewers_by_brand=('TV Viewers', 'sum'))
tv_viewers_per_brand.plot(kind='bar')

youtube_viewers_per_brand = df.groupby('Brand').agg(youtube_viewers_by_brand=('Youtube Views', 'sum'))
youtube_viewers_per_brand.plot(kind='bar')

youtube_likes_per_brand = df.groupby('Brand').agg(youtube_likes_by_brand=('Youtube Likes', 'sum'))
youtube_likes_per_brand.plot(kind='bar')

youtube_views_per_year = df.groupby('Year').agg(youtube_views_by_year=('Youtube Views', 'sum'))
youtube_views_per_year.plot(kind='bar')

tv_views_per_year = df.groupby('Year').agg(tv_views_by_year=('TV Viewers', 'sum'))
tv_views_per_year.plot(kind='bar')
15/1:
import numpy as np
age_list = [45, 67, 89, 34, 23, 56]
np.mean(age_list)
15/2:
import numpy as np
age_list = [45, 67, 89, 34, 23, 56]
np.mean(age_list)
15/3:
import numpy as np
age_list = [45, 67, 89, 34, 23, 56]
np.mean(age_list)
np.max(age_list)
15/4: np.max(age_list)
15/5:
import numpy as np
age_list = [45, 67, 89, 34, 23, 56]
np.mean(age_list)
15/6: max(age_list)
15/7: np.cos(age_list)
15/8: np.logs(age_list)
16/1:
import pandas as pd
% matplotlib inline
df_census = pd.read_csv('census_income_data.csv')
df_census.info
16/2:
import pandas as pd
% matplotlib inline
df_census = pd.read_csv('daily_engagement.csv')
df_census.info
16/3:
import pandas as pd
% matplotlib inline
df_census = pd.read_csv('daily_engagement.csv')
df_census.info
16/4:
import pandas as pd
% matplotlib inline
df_census = pd.read_csv('daily_engagement.csv')
df_census.info
16/5:
import pandas as pd
%matplotlib inline
df_census = pd.read_csv('daily_engagement.csv')
df_census.info
16/6:
df_census.hist(figsize= (8,8));
df_census['age'].hist();
df_census['age'].plot(kind='hist');
16/7:
df.columns
df_census.hist(figsize= (8,8));
df_census['age'].hist();
df_census['age'].plot(kind='hist');
16/8:
df_census.columns
df_census.hist(figsize= (8,8));
df_census['age'].hist();
df_census['age'].plot(kind='hist');
16/9:
import pandas as pd
%matplotlib inline
df_census = pd.read_csv('daily_engagement.csv')
df_census.info
df_census.head()
16/10:
df_census.columns
df_census.hist(figsize= (8,8));
df_census['total_minutes_visited'].hist();
df_census['tital_minutes_visited'].plot(kind='hist');
16/11:
df_census.columns
df_census.hist(figsize= (8,8));
df_census['total_minutes_visited'].hist();
df_census['total_minutes_visited'].plot(kind='hist');
18/1: import pandas as pd
18/2:
df = pd.read_csv('wine-red.csv')
df.head()
18/3:
df = pd.read_csv('winequality-red.csv')
df.head()
19/1: import pandas as pd
19/2:
df = pd.read_csv('winequality-red.csv')
df.head()
19/3:
df = pd.read_csv('winequality-red.csv', sep=';')
df.head()
19/4:
df = pd.read_csv('winequality-white.csv', sep=';')
df.head()
19/5:
df_red_wine = pd.read_csv('winequality-red.csv', sep=';')
df_red_wine.head()
19/6:
df_white_wine = pd.read_csv('winequality-white.csv', sep=';')
df_red_wine.head()
19/7:
df_white_wine = pd.read_csv('winequality-white.csv', sep=';')
df_white_wine.head()
19/8:
#number of samples in each dataset
#number of samples in winequality-red
df_red_wine.info()
19/9: df_red_wine.unique()
19/10: df_red_wine.nunique()
19/11: df_red_wine['quality'].nunique()
19/12:
# mean density of the red wine dataset
df_red_wine.describe()
19/13: df_red_wine.isnull()
19/14: count(df_red_wine.isnull())
19/15: df_red_wine.isnull().count()
19/16: df_red_wine.nunique()
19/17:
#number of unique values for quality in red wine dataset
df_red_wine.quality.nunique()
19/18: df_red_wine.isduplicate()
19/19: df_red_wine.isduplicated()
19/20: df_red_wine.duplicated()
19/21: df_red_wine.duplicated().count()
19/22: df_red_wine.duplicated()
19/23:
dups = df_red_wine.duplicated()
len(dups)
19/24: sum(df_red_wine.duplicated())
19/25: df_red_wine.duplicated()
19/26: df_red_wine.duplicated(keep='false')
19/27: df_red_wine.duplicated(keep='false')
19/28: df_red_wine.duplicated(keep=False)
19/29:
# mean density of the red wine dataset
df_red_wine.describe()
19/30: df_red_wine.duplicated()
19/31: sum(df_red_wine.duplicated())
19/32:
#number of samples in each dataset
#number of samples and columns in the white wine dataset
df_white_wine.info()
19/33:
#number of unique values for quality in red wine dataset
df_white_wine.nunique()
19/34: df_white_wine.quality.nunique()
19/35: sum(df_white_wine.duplicated())
19/36:
import pandas as pd
import time
19/37:
#number of samples in each dataset
#number of samples and columns in the white wine dataset
start = time.time()
df_white_wine.info()
time.time()- start, 'seconds'
19/38:
#number of samples in each dataset
#number of samples and columns in the white wine dataset
start = time.time()
df_white_wine.info()
time.time()- start
20/1:
import numpy as np
import pandas as pd
20/2:
red_df = pd.read_csv('winequality_red.csv', sep=';')
red_df = pd.read_csv('winequality_white.csv', sep=';')
red_df.head()
20/3:
red_df = pd.read_csv('winequality-red.csv', sep=';')
red_df = pd.read_csv('winequality-white.csv', sep=';')
red_df.head()
20/4: color_ red = np.array(red_df)
20/5: color_red = np.array(red_df)
20/6:
color_red = np.array(red_df)
color_red
20/7:
color_red = np.array(red_df)
color_red
20/8:
color_red = np.array(red_df)
np.repeat(color_red, len(red_df))
20/9:
color_red = np.array(red_df)
np.repeat(color_red, 1966)
20/10:
color_red = np.array(red_df)
color_white = np.array(white_df)
20/11:
# load red and white wine dataset

red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
red_df.head()
20/12:
# load red and white wine dataset

red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
red_df.head()
white_df.head()
20/13:
color_red = np.array(red_df)

color_white = np.array(white_df)
20/14: red_df['color']= color_red
20/15:
# create color array for red dataframe
color_red = np.array(red_df)

# create color array for white dataframe
color_white = np.array(white_df)
20/16:
red_df['color']= color_red
red_df.head()
20/17:
# create color array for red dataframe
color_red = np.array(red_df, repeat=1966)

# create color array for white dataframe
color_white = np.array(white_df)
20/18:
# create color array for red dataframe
color_red = np.array(red_df, 1966)

# create color array for white dataframe
color_white = np.array(white_df)
20/19:
# create color array for red dataframe
color_red = np.array(red_df)
color_red.repeat(red_df, 1966)

# create color array for white dataframe
color_white = np.array(white_df)
20/20:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(red_df, 1966)

# create color array for white dataframe
color_white = np.array(white_df)
20/21:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(red_df, red_df.shape())

# create color array for white dataframe
color_white = np.array(white_df)
20/22:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(red_df, red_df.shape)

# create color array for white dataframe
color_white = np.array(white_df)
20/23:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(red_df, 1599)

# create color array for white dataframe
color_white = np.array(white_df)
20/24:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, 1599)

# create color array for white dataframe
color_white = np.array(white_df)
20/25:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, 1599)

# create color array for white dataframe
color_white = np.array(white_df)
20/26:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, 2)

# create color array for white dataframe
color_white = np.array(white_df)
20/27:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, 2)

# create color array for white dataframe
color_white = np.array(white_df)
20/28:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, 2)

# create color array for white dataframe
color_white = np.array(white_df)
20/29:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, 2)

# create color array for white dataframe
color_white = np.array(white_df)
20/30:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat('red', 2)

# create color array for white dataframe
color_white = np.array(white_df)
20/31:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat('red', 2)

# create color array for white dataframe
color_white = np.array(white_df)
20/32:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat('red', 2)

# create color array for white dataframe
color_white = np.array(white_df)
20/33:
# create color array for red dataframe
color_red = np.array(red_df)
color_red

# create color array for white dataframe
color_white = np.array(white_df)
20/34:
# create color array for red dataframe
color_red = np.array(red_df)
color_red

# create color array for white dataframe
color_white = np.array(white_df)
20/35:
# create color array for red dataframe
color_red = np.array(red_df)
color_red

# create color array for white dataframe
color_white = np.array(white_df)
20/36:
# load red and white wine dataset

red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
20/37:
# create color array for red dataframe
color_red = np.array(red_df)
color_red

# create color array for white dataframe
color_white = np.array(white_df)
20/38: red_df.head()
20/39:
# create color array for red dataframe
color_red = np.array(red_df)
color_red

# create color array for white dataframe
# color_white = np.array(white_df)
20/40:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, 2)

# create color array for white dataframe
# color_white = np.array(white_df)
20/41:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, "red")

# create color array for white dataframe
# color_white = np.array(white_df)
20/42:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, red_df)

# create color array for white dataframe
# color_white = np.array(white_df)
20/43:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, len(red_df))

# create color array for white dataframe
# color_white = np.array(white_df)
20/44:
# create color array for red dataframe
color_red = np.array(red_df)
np.repeat(color_red, len(red_df), 'red')

# create color array for white dataframe
# color_white = np.array(white_df)
20/45:
# create color array for red dataframe

color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe
# color_white = np.array(white_df)
20/46:
# create color array for red dataframe

color_red = np.repeat('red', red_df.shape[0])
color_red

# create color array for white dataframe
# color_white = np.array(white_df)
20/47:
red_df['color']= color_red
red_df.head()
20/48:
white_df['color']= color_white
white_df.head()
20/49:
# create color array for red dataframe

color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe

color_white = np.repeat('white', white_df.shape[0])
20/50:
white_df['color']= color_white
white_df.head()
20/51: combined = [red_df, white_df]
20/52:
combined = [red_df, white_df]
combined
20/53:
combined = [red_df, white_df]
combined.head()
20/54:
combined = [red_df, white_df]
type(combined)
20/55:
combined = [red_df, white_df]
pd.concat(combined)
20/56:
combined = [red_df, white_df]
com = pd.concat(combined)
com.head()
20/57:
combined = [red_df, white_df]
com = pd.concat(combined)
com.tail()
20/58:
# append dataframes

wine_df = red_df.append(white_df, ignore_index=True)
20/59:
# append dataframes

wine_df = red_df.append(white_df, ignore_index=True)
wine_df
20/60:
# append dataframes
combined = [red_df, white_df]
wine_df = pd.concat([red_df, white_df])
wine_df
wine_df.tail()
20/61:
# append dataframes
wine_df = pd.concat([red_df, white_df])
wine_df.info()
20/62:
# append dataframes
wine_df = pd.concat(red_df, white_df)
wine_df.info()
20/63:
# append dataframes
wine_df = pd.concat([red_df, white_df])
wine_df.info()
20/64:
# append dataframes

wine_df = red_df.append(white_df, ignore_index=True)
wine_df.info()
20/65:
# append dataframes
wine_df = pd.concat([red_df, white_df], ignore_index=True)
wine_df.info()
20/66: pd.to_csv('winequality_edited.csv', index=False)
20/67: wine_df.to_csv('winequality_edited.csv', index=False)
20/68: pd.read_csv('winequality_edited.csv')
20/69:
df = pd.read_csv('winequality_edited.csv')
df.info()
20/70:
# append dataframes
wine_df = pd.concat([red_df, white_df], ignore_index=True)
wine_df.head()
20/71:
# append dataframes
wine_df = pd.concat([red_df, white_df], ignore_index=True, sort=True)
wine_df.head()
20/72: red_df.info()
20/73:
white_df['color']= color_white
white_df.info()
20/74:

red_df.rename(columns={'total_sulfur-dioxide':'total_sulfur_dioxide'}, inplace=True)
20/75:

red_df.rename(columns={'total_sulfur-dioxide':'total_sulfur_dioxide'}, inplace=True)
red_df.info()
20/76:
# create color array for red dataframe

color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe

color_white = np.repeat('white', white_df.shape[0])
20/77:
red_df['color']= color_red
red_df.head()
20/78:
# create color array for red dataframe

color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe

color_white = np.repeat('white', white_df.shape[0])
20/79:
red_df['color']= color_red
red_df.head()
20/80:
white_df['color']= color_white
white_df.info()
20/81:
# append dataframes
wine_df = pd.concat([red_df, white_df], ignore_index=True, sort=True)
wine_df.head()
20/82: wine_df.to_csv('winequality_edited.csv', index=False)
20/83:
df = pd.read_csv('winequality_edited.csv')
df.info()
20/84: wine_df.to_csv('winequality_edited.csv', index=False)
20/85:
df = pd.read_csv('winequality_edited.csv')
df.info()
20/86:
df = pd.read_csv('winequality_edited.csv')
df.head()
21/1:
# import numpy and pandas
import numpy as np
import pandas as pd

# load red and white wine datasets
red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
22/1:
# import numpy and pandas
import numpy as np
import pandas as pd

# load red and white wine datasets
red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
22/2: red_df.rename(columns={'total_sulfur-dioxide':'total_sulfur_dioxide'}, inplace=True)
22/3:
# create color array for red dataframe
color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe
color_white = np.repeat('white', white_df.shape[0])
22/4:
red_df['color'] = color_red
red_df.head()
22/5:
# import numpy and pandas
import numpy as np
import pandas as pd

# load red and white wine datasets
red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
22/6: # red_df.rename(columns={'total_sulfur-dioxide':'total_sulfur_dioxide'}, inplace=True)
22/7:
# create color array for red dataframe
color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe
color_white = np.repeat('white', white_df.shape[0])
22/8:
red_df['color'] = color_red
red_df.head()
22/9:
white_df['color'] = color_white
white_df.head()
22/10:
# append dataframes
wine_df = red_df.append(white_df) 

# view dataframe to check for success
wine_df.head()
22/11: wine_df.to_csv('winequality_edited.csv', index=False)
22/12: pd.read_csv()
22/13: pd.read_csv('winequality_edited.csv')
22/14: red_df.rename(columns={'total_sulfur-dioxide':'total_sulfur_dioxide'}, inplace=True)
22/15:
# create color array for red dataframe
color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe
color_white = np.repeat('white', white_df.shape[0])
22/16:
red_df['color'] = color_red
red_df.head()
22/17:
white_df['color'] = color_white
white_df.head()
22/18:
# append dataframes
wine_df = red_df.append(white_df) 

# view dataframe to check for success
wine_df.head()
22/19: wine_df.to_csv('winequality_edited.csv', index=False)
22/20: pd.read_csv('winequality_edited.csv')
22/21:
# import numpy and pandas
import numpy as np
import pandas as pd

# load red and white wine datasets
red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
red_df.shape
22/22:
# import numpy and pandas
import numpy as np
import pandas as pd

# load red and white wine datasets
red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
red_df.columns
22/23:
# import numpy and pandas
import numpy as np
import pandas as pd

# load red and white wine datasets
red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
red_df.columns
white_df.columns
20/87:
# load red and white wine dataset

red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
20/88:

# red_df.rename(columns={'total_sulfur-dioxide':'total_sulfur_dioxide'}, inplace=True)
# red_df.info()
20/89:
# create color array for red dataframe

color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe

color_white = np.repeat('white', white_df.shape[0])
20/90:
red_df['color']= color_red
red_df.head()
20/91:
white_df['color']= color_white
white_df.info()
20/92:
# append dataframes
wine_df = pd.concat([red_df, white_df], ignore_index=True, sort=True)
wine_df.head()
20/93:

red_df.rename(columns={'total_sulfur-dioxide':'total_sulfur_dioxide'}, inplace=True)
red_df.info()
20/94:
# create color array for red dataframe

color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe

color_white = np.repeat('white', white_df.shape[0])
20/95:
red_df['color']= color_red
red_df.head()
20/96:
white_df['color']= color_white
white_df.info()
20/97:
# append dataframes
wine_df = pd.concat([red_df, white_df], ignore_index=True, sort=True)
wine_df.head()
20/98:

red_df.rename(columns={'total_sulfur-dioxide':'total_sulfur_dioxide'}, inplace=True)
red_df.info()
20/99:
# create color array for red dataframe

color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe

color_white = np.repeat('white', white_df.shape[0])
20/100:
# create color array for red dataframe

color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe

color_white = np.repeat('white', white_df.shape[0])
20/101:
red_df['color']= color_red
red_df.head()
20/102:
white_df['color']= color_white
white_df.info()
20/103:
# append dataframes
wine_df = pd.concat([red_df, white_df], ignore_index=True, sort=True)
wine_df.head()
20/104:
# append dataframes

wine_df = red_df.append(white_df, ignore_index=True)
wine_df.info()
20/105: wine_df.to_csv('winequality_edited.csv', index=False)
20/106:
df = pd.read_csv('winequality_edited.csv')
df.head()
20/107:
df = pd.read_csv('winequality_edited.csv')
df.info()
20/108:
# append dataframes

wine_df = red_df.append(white_df, ignore_index=True, sort=True)
wine_df.info()
20/109: wine_df.to_csv('winequality_edited.csv', index=False)
20/110:
df = pd.read_csv('winequality_edited.csv')
df.info()
20/111:
# load red and white wine dataset

red_df = pd.read_csv('winequality-red.csv')
white_df = pd.read_csv('winequality-white.csv', sep=';')
20/112: red_df.info()
20/113:
red_df.info()
white_df.info()
20/114:
# red_df.info()
white_df.info()
20/115:
# load red and white wine dataset

red_df = pd.read_csv('winequality-red.csv')
white_df = pd.read_csv('winequality-white.csv')
20/116:
# red_df.info()
white_df.info()
20/117:
# load red and white wine dataset

red_df = pd.read_csv('winequality-red.csv', sep=';')
white_df = pd.read_csv('winequality-white.csv', sep=';')
20/118:
# red_df.info()
white_df.info()
20/119:

red_df.rename(columns={'total sulfur dioxide':'total_sulfur_dioxide', 'fixed acidity': 'fixed_acidity'}, inplace=True)
red_df.info()
20/120:

red_df.rename(columns={'total sulfur dioxide':'total_sulfur_dioxide', 'fixed acidity': 'fixed_acidity',
                       'volatile acidity': 'volatile_acidity', 'citric acid': 'citric_acid',
                       'residual sugar': 'residual_sugar', 'free sulfur dioxide': 'free_sulfur_dioxide',
                      'total sulfur dioxide': 'total_sulfur_dioxide'}, inplace=True)
red_df.info()
20/121: white_df.info()
20/122:

red_df.rename(columns={'total sulfur dioxide':'total_sulfur_dioxide', 'fixed acidity': 'fixed_acidity',
                       'volatile acidity': 'volatile_acidity', 'citric acid': 'citric_acid',
                       'residual sugar': 'residual_sugar', 'free sulfur dioxide': 'free_sulfur_dioxide',
                      'total sulfur dioxide': 'total_sulfur_dioxide'}, inplace=True)
red_df.info()
20/123:
white_df.rename(columns={'total sulfur dioxide':'total_sulfur_dioxide', 'fixed acidity': 'fixed_acidity',
                       'volatile acidity': 'volatile_acidity', 'citric acid': 'citric_acid',
                       'residual sugar': 'residual_sugar', 'free sulfur dioxide': 'free_sulfur_dioxide'}, inplace=True)
white_df.info()
20/124:
# create color array for red dataframe

color_red = np.repeat('red', red_df.shape[0])

# create color array for white dataframe

color_white = np.repeat('white', white_df.shape[0])
20/125:
red_df['color']= color_red
red_df.head()
20/126:
white_df['color']= color_white
white_df.info()
20/127:
# append dataframes
wine_df = pd.concat([red_df, white_df], ignore_index=True, sort=True)
wine_df.head()
20/128:
# append dataframes

wine_df = red_df.append(white_df, ignore_index=True, sort=True)
wine_df.info()
20/129: wine_df.to_csv('winequality_edited.csv', index=False)
20/130:
df = pd.read_csv('winequality_edited.csv')
df.info()
23/1:
import pandas as pd
wine_df = pd.read_csv('winequality_edited.csv')
23/2:
import pandas as pd
wine_df = pd.read_csv('winequality_edited.csv')
23/3:
import pandas as pd
% matplotlib inline
23/4:
import pandas as pd
%matplotlib inline
23/5:
wine_df = pd.read_csv('winequality_edited.csv')
wine_df.head()
23/6: wine_df.hist(figsize=(15,15));
23/7: pd.plotting.scatter_matrix(wine_df, figsize = (15,15));
23/8: pd.plotting.scatter_matrix(wine_df, figsize = (15,15));
24/1:
# Load dataset
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv('winequality_edited.csv')
23/9: wine_df.plot(x="volatile_acidity", y="quality", kind="scatter");
23/10: wine_df.plot(x="alcohol", y="quality", kind="scatter");
25/1:
def return_42():
    return 7 * 6
25/2:
def return_42():
    result = 7 * 6
    return result
25/3:
def return_42():
    result = 7 * 6
    print(result)
25/4:
def return_42():
    result = 7 * 6
    print result
25/5:
def return_42():
    result = 7 * 6
    print(result)
25/6:
def return_42():
    result = 7 * 6
    print(result)
25/7:
if __name__ == '__main__':
    print(return_42)
25/8:
if __name__ == '__main__':
    r = return_42()
    print(r)
26/1: import pandas as pd
26/2: df = pd.read_csv('winequality_edited.csv')
26/3:
df = pd.read_csv('winequality_edited.csv')
df.groupby('color').mean()
26/4:
df = pd.read_csv('winequality_edited.csv')
df.groupby(['quality','color'], as_index=False['ph']).mean()
26/5:
wine_df = pd.read_csv('winequality_edited.csv')
wine_df.groupby(['quality','color'], as_index=False)['ph'].mean()
26/6:
wine_df = pd.read_csv('winequality_edited.csv')
wine_df.groupby(['quality','color'], as_index=False)['PH'].mean()
26/7:
wine_df = pd.read_csv('winequality_edited.csv')
wine_df.groupby(['quality','color'], as_index=False)['pH'].mean()
26/8:
wine_df = pd.read_csv('winequality_edited.csv')
wine_df.groupby([color'], as_index=False)['quality'].mean()
26/9:
wine_df = pd.read_csv('winequality_edited.csv')
wine_df.groupby(['color'], as_index=False)['quality'].mean()
26/10: wine_df.groupby('color').mean()
26/11:
import pandas as pd
wine_df = pd.read_csv('winequality_edited.csv')
26/12: acidity_levels = pd.cut(np.array(wine_df),3)
26/13:
import pandas as pd
import numpy as np
wine_df = pd.read_csv('winequality_edited.csv')
26/14: acidity_levels = pd.cut(np.array(wine_df),3)
26/15: acidity_levels = pd.cut(np.array(wine_df.pH),3)
26/16:
acidity_levels = pd.cut(np.array(wine_df.pH),3)
acidity_levels
26/17:
acidity_levels = pd.cut(np.array(wine_df.pH),4, labels=['High', 'Moderately High', 'Medium', 'Low'])
acidity_levels
26/18:
acidity_levels = pd.cut(np.array(wine_df.pH),4, labels=['High', 'Moderately High', 'Medium', 'Low'])
acidity_levels.describe()
26/19: acidity_levels.groupby(['quality']).mean()
26/20: wine_df.groupby([acidity_levels]).mean()
26/21: wine_df.groupby([acidity_levels]).describe()
26/22: wine_df.groupby('color').describe()
26/23: wine_df.groupby('pH').describe()
26/24: wine_df.pH.describe()
26/25: wind_df.info()
26/26: wine_df.info()
26/27:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [2.7, 3.1, 3.2, 3.3, 4.0 ] # Fill in this list with five values you just found
# Labels for the four acidity level groups
bin_names = ['High','Moderately High','Medium', 'Low' ] # Name each acidity level category
# Creates acidity_levels column
df['acidity_levels'] = pd.cut(df['pH'], bin_edges, labels=bin_names)

# Checks for successful creation of this column
df.head()
26/28:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [2.7, 3.1, 3.2, 3.3, 4.0 ] # Fill in this list with five values you just found
# Labels for the four acidity level groups
bin_names = ['High','Moderately High','Medium', 'Low' ] # Name each acidity level category
# Creates acidity_levels column
wine_df['acidity_levels'] = pd.cut(df['pH'], bin_edges, labels=bin_names)

# Checks for successful creation of this column
wine_df.head()
26/29:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [2.7, 3.1, 3.2, 3.3, 4.0 ] # Fill in this list with five values you just found
# Labels for the four acidity level groups
bin_names = ['High','Moderately High','Medium', 'Low' ] # Name each acidity level category
# Creates acidity_levels column
wine_df['acidity_levels'] = pd.cut(df['pH'], bin_edges, labels=bin_names)

# Checks for successful creation of this column
wine_df.tail()
26/30:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [2.7, 3.1, 3.2, 3.3, 4.0 ] # Fill in this list with five values you just found
# Labels for the four acidity level groups
bin_names = ['Low','Medium','Medium High', 'High' ] # Name each acidity level category
# Creates acidity_levels column
wine_df['acidity_levels'] = pd.cut(df['pH'], bin_edges, labels=bin_names)

# Checks for successful creation of this column
wine_df.tail()
26/31:
wine_df['acidity_levels']= pd.cut(wine_df.pH),4, labels=['High', 'Moderately High', 'Medium', 'Low']
wine_df.describe()
26/32:
# wine_df['acidity_levels']= pd.cut(np.array(wine_df.pH),4, labels=['High', 'Moderately High', 'Medium', 'Low'])
# wine_df.describe()
26/33: wine_df.groupby([acidity_levels]).mean()
27/1: wine_df.groupby('color').mean().quality
27/2:
import pandas as pd
import numpy as np
wine_df = pd.read_csv('winequality_edited.csv')
27/3:

wine_df.groupby(['color'], as_index=False)['quality'].mean()
27/4: wine_df.groupby('color').mean().quality
27/5:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [2.72, 3.11, 3.21, 3.32, 4.01 ] # Fill in this list with five values you just found
# Labels for the four acidity level groups
bin_names = ['Low','Medium','Medium High', 'High' ] # Name each acidity level category
# Creates acidity_levels column
wine_df['acidity_levels'] = pd.cut(wine_df['pH'], bin_edges, labels=bin_names)

# Checks for successful creation of this column
wine_df.tail()
27/6:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [2.72, 3.11, 3.21, 3.32, 4.01 ] # Fill in this list with five values you just found
# Labels for the four acidity level groups
bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
# Creates acidity_levels column
wine_df['acidity_levels'] = pd.cut(wine_df['pH'], bin_edges, labels=bin_names)

# Checks for successful creation of this column
wine_df.tail()
27/7:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [2.72, 3.11, 3.21, 3.32, 4.01 ] # Fill in this list with five values you just found
# Labels for the four acidity level groups
bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
# Creates acidity_levels column
wine_df['acidity_levels'] = pd.cut(wine_df['pH'], bin_edges, labels=bin_names)

# Checks for successful creation of this column
wine_df.tail()
27/8: wine_df.groupby([acidity_levels]).mean()
27/9:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [2.72, 3.11, 3.21, 3.32, 4.01 ] # Fill in this list with five values you just found
# Labels for the four acidity level groups
bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
# Creates acidity_levels column
wine_df['acidity_levels'] = pd.cut(wine_df['pH'], bin_edges, labels=bin_names)

# Checks for successful creation of this column
wine_df.tail()
27/10: wine_df.groupby([acidity_levels]).mean()
27/11: wine_df.groupby(['acidity_levels']).mean()
27/12: wine_df.groupby(['acidity_levels']).describe()
27/13:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [2.72, 3.11, 3.21, 3.32, 4.01 ] # Fill in this list with five values you just found
# Labels for the four acidity level groups
bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
# Creates acidity_levels column
wine_df['acidity_levels'] = pd.cut(wine_df['pH'], bin_edges, labels=bin_names)

# Checks for successful creation of this column
wine_df.tail()
27/14: wine_df.groupby(['acidity_levels']).mean()
27/15:
# Save changes for the next section
wine_df.to_csv('winequality_edited.csv', index=False)
26/34: alc = wine_df.query('alcohol > 4')
26/35:
alc = wine_df.query('alcohol > 4')
alc
26/36:
alc = wine_df.query('alcohol > 9')
alc
26/37:
alc = wine_df.query('alcohol >19')
alc
26/38:
alc = wine_df.query('alcohol >15')
alc
26/39:
alc = wine_df.query('alcohol >14')
alc
26/40: acid_l = wine_df.query('acidity_level == "Low"')
26/41:
acid_l = wine_df.query('acidity_level == "Low"')
acid_l
26/42:
acid_l = wine_df.query('acidity_levels == "Low"')
acid_l
26/43:
acid_l = wine_df.query('acidity_levels == "Low"')
acid_l.head()
26/44:
# Save changes for the next section
wine_df.to_csv('winequality_edited.csv', index=False)
28/1: import pandas as pd
28/2: wine_df.alcohol.describe()
28/3: import pandas as pd
28/4: wine_df = pd.read_csv('winequality_edited.csv')
28/5: wine_df.alcohol.describe()
29/1: low_alcohol = wine_df.query('alcohol' < wine_df.alcohol.median())
29/2: wine_df = pd.read_csv('winequality_edited.csv')
29/3: import pandas as pd
29/4: wine_df = pd.read_csv('winequality_edited.csv')
29/5: wine_df.alcohol.describe()
29/6: low_alcohol = wine_df.query('alcohol' < wine_df.alcohol.median())
29/7: low_alcohol = wine_df.query('alcohol < "wine_df.alcohol.median()"')
29/8: low_alcohol = wine_df.query('alcohol < wine_df.alcohol.median()')
29/9: wine_df.alcohol.describe()
29/10: low_alcohol = wine_df.query('alcohol < wine_df.alcohol.median()')
29/11: import pandas as pd
29/12: wine_df = pd.read_csv('winequality_edited.csv')
29/13: wine_df.alcohol.describe()
29/14: low_alcohol = wine_df.query('alcohol < wine_df.alcohol.median()')
29/15: low_alcohol = wine_df.query('alcohol < "wine_df.alcohol.median()"')
29/16: low_alcohol = wine_df.query('alcohol < "wine_df.alcohol.mean()"')
29/17: low_alcohol = wine_df.query('alcohol < "10.49"')
29/18: low_alcohol = wine_df.query('alcohol < 10.49')
29/19:
low_alcohol = wine_df.query('alcohol < 10.49')
low_alcohol
29/20:
low_alcohol = wine_df.query('alcohol < 10.49')
low_alcohol.head()
28/6:
# select samples with alcohol content less than the median
low_alcohol = wine_df.query('alcohol < 10.49')

# select samples with alcohol content greater than or equal to the median
high_alcohol =  wine_df.query('alcohol >= 10.49')
# ensure these queries included each sample exactly once
num_samples = df.shape[0]
num_samples == low_alcohol['quality'].count() + high_alcohol['quality'].count() # should be True
28/7:
# select samples with alcohol content less than the median
low_alcohol = wine_df.query('alcohol < 10.49')

# select samples with alcohol content greater than or equal to the median
high_alcohol =  wine_df.query('alcohol >= 10.49')
# ensure these queries included each sample exactly once
num_samples = wine_df.shape[0]
num_samples == low_alcohol['quality'].count() + high_alcohol['quality'].count() # should be True
29/21: wine_df.shape[0]
29/22: wine_df.shape[1]
29/23: wine_df.shape[2]
29/24: wine_df.shape()
29/25: wine_df.shape
29/26: wine_df.shape[0].'pH'
29/27: wine_df.shape[0]
29/28: low_alcohol['quality'].count()
29/29: wine_df['quality'].count()
29/30: low_alcohol['quality'].count()
29/31: high_alcohol['quality'].count()
29/32: low_alcohol['quality'].count()
29/33:
# select samples with alcohol content less than the median
low_alcohol = wine_df.query('alcohol < 10.49')

# select samples with alcohol content greater than or equal to the median
high_alcohol =  wine_df.query('alcohol >= 10.49')
# ensure these queries included each sample exactly once
num_samples = wine_df.shape[0]
num_samples == low_alcohol['quality'].count() + high_alcohol['quality'].count() # should be True
29/34: high_alcohol['quality'].count()
29/35: high_alcohol['quality'].count()
29/36: low_alcohol['quality'].count()
29/37: low_alcohol['quality'].mean()
29/38: ### Do sweeter wines receive better ratings?
29/39: wine_df.residual_sugar.median()
29/40: wine_df.describe()
29/41: wine_df.residual_sugar.describe()
29/42:
# get the median amount of residual sugar
wine-df.residual_sugar.median()
29/43:
# get the median amount of residual sugar
wine-df.residual_sugar.median()
29/44:
# get the median amount of residual sugar
wine_df.residual_sugar.median()
29/45:
# select samples with residual sugar less than the median
low_sugar =wine_df.query('residual_sugar < wine_df.residual_sugar.median()')

# select samples with residual sugar greater than or equal to the median
high_sugar =

# ensure these queries included each sample exactly once
num_samples == low_sugar['quality'].count() + high_sugar['quality'].count() # should be True
29/46:
# select samples with residual sugar less than the median
low_sugar =wine_df.query('residual_sugar < wine_df.residual_sugar.median()')

# select samples with residual sugar greater than or equal to the median
# high_sugar =
# 
# # ensure these queries included each sample exactly once
# num_samples == low_sugar['quality'].count() + high_sugar['quality'].count() # should be True
29/47:
# get the median amount of residual sugar
wine_df.residual_sugar.median()
29/48:
# select samples with residual sugar less than the median
low_sugar =wine_df.query('residual_sugar < "wine_df.residual_sugar.median()"')

# select samples with residual sugar greater than or equal to the median
# high_sugar =
#
# # ensure these queries included each sample exactly once
# num_samples == low_sugar['quality'].count() + high_sugar['quality'].count() # should be True
29/49:
# select samples with residual sugar less than the median
low_sugar =wine_df.query('residual_sugar < 3')

# select samples with residual sugar greater than or equal to the median
# high_sugar =
#
# # ensure these queries included each sample exactly once
# num_samples == low_sugar['quality'].count() + high_sugar['quality'].count() # should be True
29/50:
# select samples with residual sugar less than the median
low_sugar =wine_df.query('residual_sugar < 3.0')

# select samples with residual sugar greater than or equal to the median
# high_sugar =
#
# # ensure these queries included each sample exactly once
# num_samples == low_sugar['quality'].count() + high_sugar['quality'].count() # should be True
29/51:
# select samples with residual sugar less than the median
low_sugar =wine_df.query('residual_sugar < 3.0')

# select samples with residual sugar greater than or equal to the median
high_sugar =wine_df.query('residual_sugar >= 3.0')
#
# # ensure these queries included each sample exactly once
num_samples == low_sugar['quality'].count() + high_sugar['quality'].count() # should be True
29/52:
# get mean quality rating for the low sugar and high sugar groups
low_sugar.mean()
29/53:
# get mean quality rating for the low sugar and high sugar groups
low_sugar['quality'].mean()
29/54: high_sugar['quality'].mean()
29/55: low_alcohol['quality'].median()
29/56: low_alcohol['quality'].mean()
29/57: low_alcohol['quality'].count()
29/58: high_alcohol['quality'].mean()
29/59: low_alcohol['quality'].mean()
29/60: wine_df.alcohol.median()
29/61: wine_df.alcohol.describe()
29/62:
# select samples with alcohol content less than the median
low_alcohol = wine_df.query('alcohol < 10.3')

# select samples with alcohol content greater than or equal to the median
high_alcohol =  wine_df.query('alcohol >= 10.3')
# ensure these queries included each sample exactly once
num_samples = wine_df.shape[0]
num_samples == low_alcohol['quality'].count() + high_alcohol['quality'].count() # should be True
29/63: high_alcohol['quality'].count()
29/64:
# select samples with alcohol content less than the median
low_alcohol = wine_df.query('alcohol < 10.3')

# select samples with alcohol content greater than or equal to the median
high_alcohol =  wine_df.query('alcohol >= 10.3')
# ensure these queries included each sample exactly once
num_samples = wine_df.shape[0]
num_samples == low_alcohol['quality'].count() + high_alcohol['quality'].count() # should be True
29/65: low_alcohol['quality'].mean(), high_alcohol['quality'].mean()
29/66: high_alcohol['quality'].mean()
29/67:
# get mean quality rating for the low sugar and high sugar groups
low_sugar['quality'].mean(), high_sugar['quality'].mean()
30/1:
import pandas as pd
import matplotlib.pyplot as plt
% matplotlib inline
30/2:
import pandas as pd
import matplotlib.pyplot as plt
%% matplotlib inline
30/3:
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
30/4:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
30/5: wine_df = pd.read_csv('winequality_edited.csv')
30/6: counts = wine_df.groupby(['quality', color]).count()
30/7:
counts = wine_df.groupby(['quality', 'color']).count()
counts
30/8:
counts = wine_df.groupby(['quality', 'color']).count()['pH']
counts
30/9:
totals = wine_df.groupby('color').count()['pH']
proportions = counts / totals
proportions.plot(kind='bar', title='Proportion by Wine Color and Quality', color=colors, alpha=.7);
plt.xlabel('Quality and Color', fontsize=18)
plt.ylabel('Proportion', fontsize=18)
30/10:
counts = wine_df.groupby(['quality', 'color']).count()['pH']
colors = ['red', 'white']
counts.plot(kind='bar', title='Counts by Wine Color and Quality', color=colors, alpha=.7);
plt.xlabel('Quality and Color', fontsize=18)
plt.ylabel('Count', fontsize=18)
30/11:
totals = wine_df.groupby('color').count()['pH']
proportions = counts / totals
proportions.plot(kind='bar', title='Proportion by Wine Color and Quality', color=colors, alpha=.7);
plt.xlabel('Quality and Color', fontsize=18)
plt.ylabel('Proportion', fontsize=18)
31/1: plt.bar([1, 2, 3], [224, 620, 425]);
31/2:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
31/3: plt.bar([1, 2, 3], [224, 620, 425]);
31/4:
# plot bars
plt.bar([1, 2, 3], [224, 620, 425])

# specify x coordinates of tick labels and their labels
plt.xticks([1, 2, 3], ['a', 'b', 'c']);
31/5: plt.bar([1, 2, 3], [224, 620, 425], tick_label=['a', 'b', 'c']);
31/6:
plt.bar([1, 2, 3], [224, 620, 425], tick_label=['a', 'b', 'c'])
plt.title('Some Title')
plt.xlabel('Some X Label')
plt.ylabel('Some Y Label');
33/1:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
33/2: df = pd.read_csv('winequality_edited.csv')
33/3:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df = pd.read_csv('winequality_edited.csv')
33/4:
# Use query to select each group and get its mean quality
median = df['alcohol'].median()
low = df.query('alcohol < {}'.format(median))
high = df.query('alcohol >= {}'.format(median))

mean_quality_low = low['quality'].mean()
mean_quality_high = high['quality'].mean()
33/5:
# Create a bar chart with proper labels
locations = [1, 2]
heights = [mean_quality_low, mean_quality_high]
labels = ['Low', 'High']
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Quality Ratings by Alcohol Content')
plt.xlabel('Alcohol Content')
plt.ylabel('Average Quality Rating');
33/6:
# Use query to select each group and get its mean quality
low = df.query('residual_sugar < {}'.format(median))
high = df.query('residual_sugar >= {}'.format(median))

mean_quality_low = low['quality'].mean()
mean_quality_high = high['quality'].mean()
33/7:
# Use query to select each group and get its mean quality
low = df.query('residual_sugar < {}'.format(median))
high = df.query('residual_sugar >= {}'.format(median))

mean_quality_low = low['quality'].mean()
mean_quality_high = high['quality'].mean()
33/8:
# Use query to select each group and get its mean quality
median = df['residual_sugar'].median()
low = df.query('residual_sugar < {}'.format(median))
high = df.query('residual_sugar >= {}'.format(median))

mean_quality_low = low['quality'].mean()
mean_quality_high = high['quality'].mean()
33/9:
# Create a bar chart with proper labels
locations = [1, 2]
heights = [mean_quality_low, mean_quality_high]
labels = ['Low', 'High']
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Quality Ratings by Residual Sugar')
plt.xlabel('Residual Sugar Content')
plt.ylabel('Average Quality Rating');
33/10:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_level', 'quality']).mean()
33/11:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_level', 'quality']).mean()
mean
33/12:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()
mean
33/13:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()
mean.head()
33/14:
# Create a bar chart with proper labels
plt.bar(mean)
33/15:
# Create a bar chart with proper labels
plt.bar(mean, [1,2]);
33/16:
# Create a bar chart with proper labels
plt.bar([1,2], [mean]);
33/17:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()
mean['High']
33/18:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()
33/19:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()
mean.head()
33/20: low_acid = df.groupby(['acidity_levels', 'quality']).mean()['Low']
33/21: low_acid = df.groupby(['acidity_levels', 'quality'])['Low'].mean()
33/22: low_acid = df.groupby(['acidity_levels', 'Low']).mean()
33/23: low_acid = df.groupby(['acidity_levels', 'quality']).mean()
33/24:
low_acid = df.groupby(['acidity_levels', 'quality']).mean()
low_acid
33/25:
low_acid = df.groupby(['acidity_levels', 'quality']).mean()
low_acid.head()
33/26:
# Create a bar chart with proper labels
plt.bar([1,2,3,4], [mean]);
33/27:
# Create a bar chart with proper labels
plt.bar([1,2,3,4], mean);
33/28:
low_acid = df.acidity_levels.groupby(['Low', 'quality']).mean()
low_acid.head()
33/29:
low_acid = df.acidity_levels.mean()
low_acid.head()
33/30:
low_acid = df.acidity_levels
low_acid.head()
33/31:
low_acid = df.acidity_levels['Low']
low_acid
33/32:
low_acid = df.acidity_levels
low_acid
33/33:
low_acid = mean['Low']
low_acid
33/34:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels'['Low'], 'quality']).mean()
mean.head()
33/35:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

mean.head()
33/36: type(mean)
33/37: mean[0]
33/38: mean.Low
33/39: mean.describe()
33/40:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

mean.apply[list]
33/41:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

grouped = df.groupby('acidity_levels')
grouped
33/42:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

grouped = df.groupby('acidity_levels').m(ean
grouped
33/43:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

grouped = df.groupby('acidity_levels').mean()
grouped
33/44:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

grouped = df.groupby('acidity_levels', 'quality').mean()
grouped
33/45:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

grouped = df.groupby(['acidity_levels', 'quality']).mean()
grouped
33/46:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

grouped = df.groupby(['acidity_levels', 'quality']).mean()
grouped.apply(list)
33/47:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

grouped = df.groupby(['acidity_levels', 'quality']).mean()
grouped.apply(list).head()
33/48:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

grouped = df.groupby(['acidity_levels', 'quality']).mean()
grouped.apply(list).head()
grouped['Low']
33/49:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

grouped = df.groupby(['acidity_levels', 'quality']).mean()
grouped.apply(list).head()
grouped[0]
33/50:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby(['acidity_levels', 'quality']).mean()

grouped = df.groupby(['acidity_levels', 'quality']).mean()
grouped.apply(list).head()
33/51:
# Create a bar chart with proper labels
plt.bar([1,2,3,4], grouped);
33/52:
# Create a bar chart with proper labels
plt.bar([1,2,3,4], [grouped]);
33/53:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby('acidity_levels').mean().quality

grouped = df.groupby(['acidity_levels', 'quality']).mean()
grouped.apply(list).head()
33/54:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby('acidity_levels').mean().quality

grouped = df.groupby(['acidity_levels', 'quality']).mean()
grouped.apply(list).head()
mean
33/55:
# Create a bar chart with proper labels
plt.bar([1,2,3,4], [mean]);
33/56:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby('acidity_levels').mean().quality
33/57:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby('acidity_levels').mean().quality
mean
33/58: mean.describe()
33/59:
# Create a bar chart with proper labels
plt.bar([1,2] [mean]);
33/60: mean.low
33/61: mean['low']
33/62: mean['Low']
33/63:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby('acidity_levels').mean().quality

low = mean['Low']
medium = mean['Medium']
medium_high = mean['Medium High']
high = mean['High']
33/64:
# Create a bar chart with proper labels
plt.bar([1,2] low, medium, medium_high, high);
33/65:
# Create a bar chart with proper labels
plt.bar([1,2], [low, medium, medium_high, high]);
33/66:
# Create a bar chart with proper labels
plt.bar([1,2,3,4], [low, medium, medium_high, high]);
33/67:
# Create a bar chart with proper labels
plt.bar([1,2,3,4], [low, medium, medium_high, high]);
locations = [1,2,3,4]
heights = [low, medium, medium_high, high]
labels = ['Low', 'Medium', 'Medium High', 'High']
plt.bar(locations, heights, tick_labels=labels)
plt.title('Average Quality Ratings by Acidity Levels')
plt.xlabel('Acidity Levels')
plt.ylabel('Average Quality Rating')
33/68:
# Create a bar chart with proper labels
plt.bar([1,2,3,4], [low, medium, medium_high, high]);
locations = [1,2,3,4]
heights = [low, medium, medium_high, high]
labels = ['Low', 'Medium', 'Medium High', 'High']
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Quality Ratings by Acidity Levels')
plt.xlabel('Acidity Levels')
plt.ylabel('Average Quality Rating')
33/69: mean['Medium']
33/70: mean['Medium High']
33/71: mean['High']
33/72:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby('acidity_levels').mean().quality

low = mean['Low']
medium = mean['Medium']
medium_high = mean['Medium High']
high = mean['High']

mean
33/73: plt.line(mean)
33/74: plt.(mean)
33/75: plt.(mean)
33/76: plt(mean)
33/77:
# Create a bar chart with proper labels
plt.bar([1,2,3,4], [low, medium, medium_high, high]);
locations = [1,2,3,4]
heights = [low, medium, medium_high, high]
labels = ['Low', 'Medium', 'Medium High', 'High']
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Quality Ratings by Acidity Levels')
plt.xlabel('Acidity Levels')
plt.ylabel('Average Quality Rating');
33/78: plt(heights)
33/79: plt.plot(heights)
33/80: plt.plot(mean)
33/81: plt.plot(heights)
33/82: plt.plot(heights);
33/83: plt.plot(heights, labels);
33/84: plt.plot(labels, heights);
33/85: plt.plot(heights);
33/86: plt.plot(mean);
33/87:
# Create a bar chart with proper labels
plt.bar([4,3,2,1], [low, medium, medium_high, high]);
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
labels = ['Low', 'Medium', 'Medium High', 'High']
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Quality Ratings by Acidity Levels')
plt.xlabel('Acidity Levels')
plt.ylabel('Average Quality Rating');
33/88:
# Use groupby to get the mean quality for each acidity level
mean = df.groupby('acidity_levels').quality.mean()

low = mean['Low']
medium = mean['Medium']
medium_high = mean['Medium High']
high = mean['High']

mean
33/89:
# Create a bar chart with proper labels
plt.bar([4,3,2,1], [low, medium, medium_high, high]);
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
labels = ['Low', 'Medium', 'Medium High', 'High']
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Quality Ratings by Acidity Levels')
plt.xlabel('Acidity Levels')
plt.ylabel('Average Quality Rating');
33/90: plt.plot(mean);
33/91:
# Create a bar chart with proper labels
plt.bar([4,3,2,1], [low, medium, medium_high, high]);
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
# labels = ['Low', 'Medium', 'Medium High', 'High']
labels = mean.index.str.replace('_', ' ').str.title()
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Quality Ratings by Acidity Levels')
plt.xlabel('Acidity Levels')
plt.ylabel('Average Quality Rating');
34/1:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
34/2:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df = pd.read_csv('all_alpha_08.csv')
34/3:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df = pd.read_csv('all_alpha_08.csv')
34/4:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df = pd.read_csv('all_alpha_08.csv')
df
34/5:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/6: df_08
34/7: df_18
34/8: df_08.head()
34/9: df_18.head()
34/10: df_18.head()
34/11: df_08.info()
34/12: df_18.info()
34/13: df_08.duplicated()
34/14: df_08.duplicated().sum()
34/15: df_08.duplicated().count()
34/16: df_08.duplicated()
34/17: df_08.duplicated().count()
34/18: df_08.duplicated().sum()
34/19: df_08.isnull()
34/20: df_08.isnull().sum()
34/21: df_08.isnull().count()
34/22: df_08.isnull().sum()
34/23: df_08[df_08.isnull().any(axis=1)]
34/24: df_08[df_08.isnull().any(axis=1)].sum()
34/25: df_08[df_08.isnull().any(axis=1)].count()
34/26: df_08[df_08.isnull().any(axis=1)]
34/27: df_08[df_08.isnull().any(axis=1)].sum()
34/28: df_08[df_08.isnull().any(axis=1)]
34/29: df_08.isnull()
34/30: df_08.isnull().sum()
34/31: df_08.isnull().values()
34/32: df_08.isnull().values
34/33: df_08.isnull().values.sum()
34/34: df_08.isnull().values.any()
34/35: df_08.isnull().values.any(axis=1)
34/36: df_08[df_08.isnull().values.any(axis=1)]
34/37: df_18.duplicated()
34/38: df_18.duplicated().sum()
34/39: df_18[df_18.isnull().any(axis=1)]
34/40: df_18[df_18.isnull().values.any(axis=1)]
34/41: df_18[df_18.isnull().any(axis=1)]
34/42: df_08.isnull()
34/43: df_08.nunique()
34/44:
df_18[df_18.isnull().any(axis=1)]
df_08.info()
34/45: df_08.Model.sum()
34/46: df_08.Model.count()
34/47: df_08.Model.count()-df_08.Model.nunique()
34/48: df_08.SmartWay.nunique()
34/49: df_08.SmartWay.nunique().sum()
34/50: df_08.SmartWay.sum()
34/51: df_08.SmartWay.nunique().count()
34/52: df_08.SmartWay.nunique().sum()
34/53: df_08.SmartWay.nunique()
34/54: non_un = df_08.Model.nunique()
34/55:
non_un = df_08.Model.nunique()
df_08['not_unique']=non_un
34/56:
non_un = df_08.Model.nunique()
df_08['not_unique']=non_un
df_08.info()
34/57:
non_un = df_08.Model.nunique()
df_08['not_unique']=non_un
df_08.non_un
34/58:
non_un = df_08.Model.nunique()
df_08['not_unique']=non_un
df_08.not_unique
34/59:
non_un = df_08.Model.nunique()
df_08['not_unique']=non_un
df_08.not_unique.sum()
34/60:
non_un = df_08.Model.nunique()
df_08['not_unique']=non_un
df_08.not_unique.count()
34/61:
non_un = df_08.Model.nunique()
df_08['not_unique']=non_un
df_08.not_unique
34/62: df_08.info()
34/63: df_08.nunique()
34/64: df_08.dropduplicates()
34/65: df_08.drop_duplicates()
34/66: df_08.Model.drop_duplicates()
34/67: df_08.nunique()
34/68:
fr = df_08.Model.drop_duplicates()
fr
34/69:
df_08.Model.drop_duplicates()
df_08.info()
34/70:
df_08.Model.drop_duplicates()
df_08.duplicated()
34/71:
df_08.Model.drop_duplicates()
df_08.duplicated().sum()
34/72:
df_08.Model.drop_duplicates()
df_08.nunique().sum()
34/73:
df_08.Model.drop_duplicates()
df_08.nunique()
34/74: df_08.Model.drop_duplicates().nunique()
34/75: df_08.nunique()
34/76: df_08.nunique()
34/77: df_08.Model.count()-df_08.Model.nunique()
34/78: df_08.nunique()
34/79: df_08.info()
34/80:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/81: df_08.info()
34/82: df_08.duplicated().sum()
34/83: df_08[df_08.isnull().values.any(axis=1)]
34/84: df_08[df_08.isnull().any(axis=1)]
34/85: df_18.info()
34/86: df_18.duplicated().sum()
34/87:
df_18[df_18.isnull().any(axis=1)]
df_08.info()
34/88: df_08.nunique()
34/89: df_08.nunique()
34/90: df_18.nunique()
34/91: df_08.head()
34/92: df_18.head()
34/93: df_08.groupby('Fuel').sum()
34/94: df_08.groupby('Fuel').count()
34/95: df_08.query('Fuel')
34/96: df_08.query('Fuel')
34/97: df_08.query('Fuel').sum()
34/98: df_08.query('Fuel == "Gasoline"').sum()
34/99: df_08.query('Fuel == "Gasoline"')
34/100: df_18.groupby('Fuel').sum()
35/1: fuel_18_df.Fuel.unique()
34/101: df_08.Fuel.unique()
34/102: df_18.Fuel.unique()
34/103: df_08.head()
34/104: df_18.head9
34/105: df_18.head()
34/106:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
34/107: df_08.head(1)
34/108: df_18.head(1)
34/109:
# drop columns from 2018 dataset
df_08.drop(7,8,9,17 inplace=True)

# confirm changes
34/110:
# drop columns from 2018 dataset
df_08.drop([7,8,9,17] inplace=True)

# confirm changes
34/111:
# drop columns from 2018 dataset
df_08.drop([7,8,9,17], inplace=True)

# confirm changes
34/112:
# drop columns from 2018 dataset
df_08.drop([7,8,9,17], inplace=True)

# confirm changes
df_18.head(1)
34/113:
# drop columns from 2018 dataset
df_08.drop([7,8,9,17], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/114:
# drop columns from 2018 dataset
df_08.drop(df.columns[[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/115:
# drop columns from 2018 dataset
df_08.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/116:
# drop columns from 2018 dataset
df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/117: df_18.head(1)
34/118:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/119: df_08.duplicated().sum()
34/120: df_08[df_08.isnull().values.any(axis=1)]
34/121: df_08.head(1)
34/122: df_18.head(1)
34/123:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
34/124:
# drop columns from 2018 dataset
# df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)
df_18.drop(df.column[[7,8,9,17]], axis=1, inplace=True)
# df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/125:
# drop columns from 2018 dataset
# df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)
df_18.drop(df_18.column[[7,8,9,17]], axis=1, inplace=True)
# df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/126:
# drop columns from 2018 dataset
# df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)
df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
# df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/127:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/128:
# drop columns from 2018 dataset
# df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)
df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
# df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/129:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/130:
# drop columns from 2018 dataset
df_18.drop([7,8,9,17], axis=1, inplace=True)
# df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
# df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/131:
# drop columns from 2018 dataset
df_18.drop([[7,8,9,17]], axis=1, inplace=True)
# df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
# df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/132:
# drop columns from 2018 dataset
# df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
df_18.drop(df.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/133:
# drop columns from 2018 dataset
# df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
df_18.drop(df_18.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/134:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/135:
# drop columns from 2018 dataset
# df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
df_18.drop(df_18.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/136:
# rename Sales Area to Cert Region
df_08.rename(columns={'Sales Area': 'Cert Region'}, inplace=True)


# confirm changes
df_08.head(1)
34/137:
# replace spaces with underscores and lowercase labels for 2008 dataset
df_08.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_08.head(1)
34/138:
# replace spaces with underscores and lowercase labels for 2018 dataset
df_18.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_18.head(1)
34/139:
# confirm column labels for 2008 and 2018 datasets are identical
df_08.columns == df_18.columns
34/140:
# replace spaces with underscores and lowercase labels for 2018 dataset
df_18.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_18.head(1)
34/141:
# rename Sales Area to Cert Region
df_08.rename(columns={'Sales Area': 'Cert Region'}, inplace=True)


# confirm changes
df_08.head(1)
34/142:
# confirm column labels for 2008 and 2018 datasets are identical
df_08.columns == df_18.columns
34/143:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/144: df_08.info()
34/145: df_08.duplicated().sum()
34/146: df_08[df_08.isnull().values.any(axis=1)]
34/147: df_08[df_08.isnull().any(axis=1)]
34/148: df_18.info()
34/149: df_18.duplicated().sum()
34/150:
df_18[df_18.isnull().any(axis=1)]
df_08.info()
34/151: df_08.nunique()
34/152: df_08.nunique()
34/153: df_08.head()
34/154: df_18.head()
34/155: df_08.groupby('Fuel').sum()
34/156: df_08.Fuel.unique()
34/157:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
34/158:
# drop columns from 2018 dataset
# df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
df_18.drop(df_18.iloc[:,[7,8,9,17]], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/159:
# rename Sales Area to Cert Region
df_08.rename(columns={'Sales Area': 'Cert Region'}, inplace=True)


# confirm changes
df_08.head(1)
34/160:
# replace spaces with underscores and lowercase labels for 2008 dataset
df_08.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_08.head(1)
34/161:
# replace spaces with underscores and lowercase labels for 2018 dataset
df_18.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_18.head(1)
34/162:
# confirm column labels for 2008 and 2018 datasets are identical
df_08.columns == df_18.columns
34/163:
# make sure they're all identical like this
(df_08.columns == df_18.columns).all()
34/164:
# save new datasets for next section
df_08.to_csv('data_08_v1.csv', index=False)
df_18.to_csv('data_18_v1.csv', index=False)
34/165:
# load datasets
import pandas as pd

df_08 = pd.read_csv('data_08_v1.csv')
df_18 = pd.read_csv('data_08_v1.csv')
34/166: df_08.shape()
34/167: df_08.shape
34/168: df_18.shape
34/169:
df_18.shape
df_08.head(1)
34/170:
# filter datasets for rows following California standards
df_08 = df_08.query('cert_region == "CA"')
df_18 = df_18.query('cert_region == "CA"')
34/171:
# confirm only certification region is California
df_08['cert_region'].unique()
34/172:
# confirm only certification region is California
df_18['cert_region'].unique()
34/173: df_08.drop(['cert_region'], axis=1, inplace=True)
34/174: df_08.shape
34/175:
# filter datasets for rows following California standards
df_08 = df_08.query('cert_region == "CA"')
df_18 = df_18.query('cert_region == "CA"')
34/176:
# load datasets
import pandas as pd

df_08 = pd.read_csv('data_08_v1.csv')
df_18 = pd.read_csv('data_08_v1.csv')
34/177:
# filter datasets for rows following California standards
df_08 = df_08.query('cert_region == "CA"')
df_18 = df_18.query('cert_region == "CA"')
34/178:
# confirm only certification region is California
df_08['cert_region'].unique()
34/179:
# confirm only certification region is California
df_18['cert_region'].unique()
34/180:
# drop certification region columns form both datasets

df_08.drop(['cert_region'], axis=1, inplace=True)
df_18.drop(['cert_region'], axis=1, inplace=True)
34/181: df_08.shape
34/182: df_18.shape
34/183:
# view missing value count for each feature in 2008
df_08.isnull().sum()
34/184:
# view missing value count for each feature in 2018
df_18.isnull().sum()
34/185:
# drop rows with any null values in both datasets
df_08.dropna(inplace=True)
df_18.dropna(inplace=True)
34/186:
# checks if any of columns in 2008 have null values - should print False
df_08.isnull().sum().any()
34/187:
# checks if any of columns in 2018 have null values - should print False
df_18.isnull().sum().any()
34/188:
# print number of duplicates in 2008 and 2018 datasets

df_08.duplicated().sum(), df_18.duplicated().sum()
34/189:
# drop duplicates in both datasets
df_08.drop_duplicates(inplace=True)
df_18.drop_duplicates(inplace=True)
34/190:
# print number of duplicates again to confirm dedupe - should both be 0
df_08.duplicated().sum(), df_18.duplicated().sum()
34/191:
# save progress for the next section
df_08.to_csv('data_08_v2.csv', index=False)
df_18.to_csv('data_18_v2.csv', index=False)
34/192: df_08.shape
34/193: df_08.info()
36/1:
# load datasets
import pandas as pd

df_08 = pd.read_csv('all_alpha_08.csv') 
df_18 = pd.read_csv('all_alpha_18.csv')
36/2:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
df.shape
36/3:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
df_08.shape
36/4:
# load datasets
import pandas as pd

df_08 = pd.read_csv('all_alpha_08.csv') 
df_18 = pd.read_csv('all_alpha_18.csv')
36/5:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
df_08.shape
36/6:
# drop columns from 2018 dataset
df_18.drop(['Stnd', 'Stnd Description', 'Underhood ID', 'Comb CO2'], axis=1, inplace=True)

# confirm changes
df_18.head(1)
df_18.shape()
36/7:
# load datasets
import pandas as pd

df_08 = pd.read_csv('all_alpha_08.csv') 
df_18 = pd.read_csv('all_alpha_18.csv')
36/8:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
36/9:
# drop columns from 2018 dataset
df_18.drop(['Stnd', 'Stnd Description', 'Underhood ID', 'Comb CO2'], axis=1, inplace=True)

# confirm changes
df_18.head(1)
df_18.shape
34/194:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/195:
# drop columns from 2018 dataset
# df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
df_18.drop(['Stnd', 'Stnd Description', 'Underhood ID', 'Comb CO2'], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/196: df_18.shape
34/197:
# rename Sales Area to Cert Region
df_08.rename(columns={'Sales Area': 'Cert Region'}, inplace=True)


# confirm changes
df_08.head(1)
34/198:
# replace spaces with underscores and lowercase labels for 2008 dataset
df_08.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_08.head(1)
34/199:
# replace spaces with underscores and lowercase labels for 2018 dataset
df_18.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_18.head(1)
34/200:
# confirm column labels for 2008 and 2018 datasets are identical
df_08.columns == df_18.columns
34/201:
# replace spaces with underscores and lowercase labels for 2018 dataset
df_18.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_18.head(1)
34/202:
# confirm column labels for 2008 and 2018 datasets are identical
df_08.columns == df_18.columns
34/203:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/204: df_08.head(1)
34/205: df_18.head(1)
34/206:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
34/207:
# drop columns from 2018 dataset
# df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
df_18.drop(['Stnd', 'Stnd Description', 'Underhood ID', 'Comb CO2'], axis=1, inplace=True)

# confirm changes
df_18.head(1)
34/208: df_18.shape
34/209:
# rename Sales Area to Cert Region
df_08.rename(columns={'Sales Area': 'Cert Region'}, inplace=True)


# confirm changes
df_08.head(1)
34/210:
# replace spaces with underscores and lowercase labels for 2008 dataset
df_08.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_08.head(1)
34/211:
# replace spaces with underscores and lowercase labels for 2008 dataset
df_08.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_08.head(1)
34/212:
# replace spaces with underscores and lowercase labels for 2018 dataset
df_18.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_18.head(1)
34/213:
# confirm column labels for 2008 and 2018 datasets are identical
df_08.columns == df_18.columns
34/214:
# make sure they're all identical like this
(df_08.columns == df_18.columns).all()
34/215:
# save new datasets for next section
df_08.to_csv('data_08_v1.csv', index=False)
df_18.to_csv('data_18_v1.csv', index=False)
34/216:
# load datasets
import pandas as pd

df_08 = pd.read_csv('data_08_v1.csv')
df_18 = pd.read_csv('data_08_v1.csv')
34/217: df_08.info()
34/218: df_18.info()
34/219:
# confirm only certification region is California
df_08['cert_region'].unique()
34/220:
# confirm only certification region is California
df_18['cert_region'].unique()
34/221:
# drop certification region columns form both datasets

df_08.drop(['cert_region'], axis=1, inplace=True)
df_18.drop(['cert_region'], axis=1, inplace=True)
34/222: df_08.shape
34/223: df_18.shape
34/224:
# drop certification region columns form both datasets

df_08.drop(['cert_region'], axis=1, inplace=True)
df_18.drop(['cert_region'], axis=1, inplace=True)
34/225:
# load datasets
import pandas as pd

df_08 = pd.read_csv('data_08_v1.csv')
df_18 = pd.read_csv('data_08_v1.csv')
34/226:
# filter datasets for rows following California standards
df_08 = df_08.query('cert_region == "CA"')
df_18 = df_18.query('cert_region == "CA"')
34/227: df_08.info()
34/228: df_18.info()
34/229:
# confirm only certification region is California
df_08['cert_region'].unique()
34/230:
# confirm only certification region is California
df_18['cert_region'].unique()
34/231:
# drop certification region columns form both datasets

df_08.drop(['cert_region'], axis=1, inplace=True)
df_18.drop(['cert_region'], axis=1, inplace=True)
34/232: df_08.shape
34/233: df_18.shape
34/234: df_08.shape, df_18.shape
34/235:
# save new datasets for next section
df_08.to_csv('data_08_v1.csv', index=False)
df_18.to_csv('data_18_v1.csv', index=False)
34/236: df_08.shape, df_18.shape
34/237:
# make sure they're all identical like this
(df_08.columns == df_18.columns).all()
34/238:
# replace spaces with underscores and lowercase labels for 2018 dataset
df_18.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_18.head(1)
34/239:
# confirm column labels for 2008 and 2018 datasets are identical
df_08.columns == df_18.columns
34/240:
# make sure they're all identical like this
(df_08.columns == df_18.columns).all()
34/241:
# save new datasets for next section
df_08.to_csv('data_08_v1.csv', index=False)
df_18.to_csv('data_18_v1.csv', index=False)
34/242: df_08.shape, df_18.shape
34/243: df_08.shape
34/244:
# load datasets
import pandas as pd

df_08 = pd.read_csv('data_08_v1.csv')
df_18 = pd.read_csv('data_08_v1.csv')
34/245: df_08.shape
34/246:
df_18.shape
df_08.head(1)
34/247: df_18.shape
37/1:
# load datasets
import pandas as pd

df_08 = pd.read_csv('data_08_v1.csv')
df_18 = pd.read_csv('data_18_v1.csv')
37/2:
# view dimensions of dataset
df_08.shape
37/3:
# view dimensions of dataset
df_18.shape
34/248:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/249: df_08.head(1)
34/250: df_18.head(1)
34/251:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
36/10:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
36/11:
# load datasets
import pandas as pd

df_08 = pd.read_csv('all_alpha_08.csv') 
df_18 = pd.read_csv('all_alpha_18.csv')
36/12:
# view 2008 dataset
df_08.head(1)
36/13:
# view 2018 dataset
df_18.head(1)
36/14:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
34/252:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
34/253:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df_08 = pd.read_csv('all_alpha_08.csv')
df_18 = pd.read_csv('all_alpha_18.csv')
34/254:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
34/255:
# drop columns from 2018 dataset
# df_18.drop(df_18.columns[[7,8,9,17]], axis=1, inplace=True)
df_18.drop(['Stnd', 'Stnd Description', 'Underhood ID', 'Comb CO2'], axis=1, inplace=True)

# confirm changes
df_18.head(1)
36/15:
# drop columns from 2018 dataset
df_18.drop(['Stnd', 'Stnd Description', 'Underhood ID', 'Comb CO2'], axis=1, inplace=True)

# confirm changes
df_18.head(1)
df_18.shape
36/16:
# drop columns from 2018 dataset
df_18.drop(['Stnd', 'Stnd Description', 'Underhood ID', 'Comb CO2'], axis=1, inplace=True)

# confirm changes
df_18.head(1)
df_18.shape
36/17:
# load datasets
import pandas as pd

df_08 = pd.read_csv('all_alpha_08.csv') 
df_18 = pd.read_csv('all_alpha_18.csv')
36/18:
# view 2018 dataset
df_18.head(1)
36/19:
# view 2008 dataset
df_08.head(1)
36/20:
# drop columns from 2008 dataset
df_08.drop(['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'], axis=1, inplace=True)

# confirm changes
df_08.head(1)
36/21:
# drop columns from 2018 dataset
df_18.drop(['Stnd', 'Stnd Description', 'Underhood ID', 'Comb CO2'], axis=1, inplace=True)

# confirm changes
df_18.head(1)
df_18.shape
34/256: df_18.shape
34/257:
# rename Sales Area to Cert Region
df_08.rename(columns={'Sales Area': 'Cert Region'}, inplace=True)


# confirm changes
df_08.head(1)
36/22:
# rename Sales Area to Cert Region
df_08.rename(columns={'Sales Area': 'Cert Region'}, inplace=True)

# confirm changes
df_08.head(1)
36/23:
# rename Sales Area to Cert Region
df_08.rename(columns={'Sales Area': 'Cert Region'}, inplace=True)

# confirm changes
df_08.head(1)
36/24:
# replace spaces with underscores and lowercase labels for 2008 dataset
df_08.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_08.head(1)
34/258:
# replace spaces with underscores and lowercase labels for 2008 dataset
df_08.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_08.head(1)
34/259:
# replace spaces with underscores and lowercase labels for 2018 dataset
df_18.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_18.head(1)
36/25:
# replace spaces with underscores and lowercase labels for 2018 dataset
df_18.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)

# confirm changes
df_18.head(1)
36/26:
# confirm column labels for 2008 and 2018 datasets are identical
df_08.columns == df_18.columns
34/260:
# confirm column labels for 2008 and 2018 datasets are identical
df_08.columns == df_18.columns
34/261:
# make sure they're all identical like this
(df_08.columns == df_18.columns).all()
36/27:
# make sure they're all identical like this
(df_08.columns == df_18.columns).all()
36/28:
# make sure they're all identical like this
(df_08.columns == df_18.columns).all()
34/262:
# save new datasets for next section
df_08.to_csv('data_08_v1.csv', index=False)
df_18.to_csv('data_18_v1.csv', index=False)
36/29:
# save new datasets for next section
df_08.to_csv('data_08_v1.csv', index=False)
df_18.to_csv('data_18_v1.csv', index=False)
36/30: df_08.shape, df_18.shape
34/263:
# save new datasets for next section
df_08.to_csv('data_08_v1.csv', index=False)
df_18.to_csv('data_18_v1.csv', index=False)
34/264: df_08.shape, df_18.shape
36/31:
l = pd.read_csv('data_08_v1.csv')
l.shape
36/32:
m = pd.read_csv('data_18_v1.csv')
m.shape
34/265:
l = pd.read_csv('data_08_v1.csv')
l.shape
34/266:
m = pd.read_csv('data_18_v1.csv')
m.shape
34/267:
# load datasets
import pandas as pd

df_08 = pd.read_csv('data_08_v1.csv')
df_18 = pd.read_csv('data_08_v1.csv')
34/268: df_08.shape
34/269: df_18.shape
34/270:
# load datasets
import pandas as pd

df_08 = pd.read_csv('data_08_v1.csv')
df_18 = pd.read_csv('data_18_v1.csv')
34/271: df_08.shape
34/272: df_18.shape
34/273:
df_18.shape
df_08.head(1)
37/4:
# load datasets
import pandas as pd

df_08 = pd.read_csv('data_08_v1.csv')
df_18 = pd.read_csv('data_18_v1.csv')
37/5:
# view dimensions of dataset
df_08.shape
37/6:
# view dimensions of dataset
df_18.shape
37/7:
# view dimensions of dataset
df_08.shape
34/274:
# filter datasets for rows following California standards
df_08 = df_08.query('cert_region == "CA"')
df_18 = df_18.query('cert_region == "CA"')
34/275: df_08.info()
34/276: df_18.info()
34/277: df_08.shape
34/278: df_18.shape
34/279:
# drop certification region columns form both datasets

df_08.drop(['cert_region'], axis=1, inplace=True)
df_18.drop(['cert_region'], axis=1, inplace=True)
34/280: df_08.shape
34/281: df_18.shape
34/282:
# view missing value count for each feature in 2008
df_08.isnull().sum()
34/283:
# drop rows with any null values in both datasets
df_08.dropna(inplace=True)
df_18.dropna(inplace=True)
34/284:
# checks if any of columns in 2008 have null values - should print False
df_08.isnull().sum().any()
34/285:
# checks if any of columns in 2018 have null values - should print False
df_18.isnull().sum().any()
34/286:
# print number of duplicates in 2008 and 2018 datasets

df_08.duplicated().sum(), df_18.duplicated().sum()
34/287:
# drop duplicates in both datasets
df_08.drop_duplicates(inplace=True)
df_18.drop_duplicates(inplace=True)
34/288:
# print number of duplicates again to confirm dedupe - should both be 0
df_08.duplicated().sum(), df_18.duplicated().sum()
34/289:
# save progress for the next section
df_08.to_csv('data_08_v2.csv', index=False)
df_18.to_csv('data_18_v2.csv', index=False)
34/290: df_08.shape, df_18.shape
39/1: df_08.dtypes()
39/2:
import pandas as pd

df_08 = pd.read_csv('data_08_v2.csv')
df_18 = pd.read_csv('data_18_v2.csv')
39/3: df_08.dtypes()
39/4: df_08.dtypes
39/5: df_18.dtypes
39/6: df_08.head(1)
39/7: df_18.head(1)
39/8: df_08.head(2)
39/9: df_08.head(1)
39/10: df_08.dtypes == df_18.dtypes
39/11: df_08.head(1)
39/12: df_18.head(1)
39/13: df_08.dtypes
39/14: df_18.dtypes
41/1: import
41/2: import pandas as pd
41/3: import numpy as np
41/4: import matplotlib.pyplot as plt
41/5: df = pd.read_csv('census_income_data')
41/6: df = pd.read_csv('daily_engagements.csv')
41/7: df = pd.read_csv('daily_engagement.csv')
41/8: df.head()
41/9: ls
41/10: df['lessons_completed'].hist()
41/11: plt.show()
42/1:
import pandas as pd
import numpy as np
% matplotlib inline
42/2:
import pandas as pd
import numpy as np
%matplotlib inline
42/3:
def load_data(raw):
    data = pd.read_csv(raw)
    return data
42/4:
def load_data(raw):
    data = pd.read_csv(raw)
    return data
42/5:
def load_data(raw):
    data = pd.read_csv(raw)
    return data
42/6:
def raw_data(raw):
    csv_file = pd.read_csv(raw)
    return csv_file
42/7:
def load_data():
    data = raw_data('tmdb.csv')
    return data
42/8:
movies_data = load_data()
movies_data.head()
42/9:
def load_data():
    data = raw_data('tmdb-movies.csv')
    return data
42/10:
movies_data = load_data()
movies_data.head()
42/11:
movies_data = load_data()
movies_data.tail()
42/12: movies_data.info()
43/1:
> **Tip**: Welcome to the Investigate a Dataset project! You will find tips in quoted sections like this to help organize your approach to your investigation. Before submitting your project, it will be a good idea to go back through your report and remove these sections to make the presentation of your work as tidy as possible. First things first, you might want to double-click this Markdown cell and change the title so that it reflects your dataset and investigation.

# Project: Investigate a Dataset (Replace this with something more specific!)

## Table of Contents
<ul>
<li><a href="#intro">Introduction</a></li>
<li><a href="#wrangling">Data Wrangling</a></li>
<li><a href="#eda">Exploratory Data Analysis</a></li>
<li><a href="#conclusions">Conclusions</a></li>
</ul>
44/1: # Project: Investigating TMDB Movies Dataset
44/2:
# Import Packages
import pandas as pd
import numpy as np
%matplotlib inline
45/1: movies_data.dtypes
45/2:
# Loading TMDB Movies Data
def load_data():
    data = raw_data('tmdb-movies.csv')
    return data
45/3:
# Preview the Data
movies_data = load_data()
movies_data.tail()
45/4:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
45/5:
# A Function to Load Data from CSV files
def raw_data(raw):
    csv_file = pd.read_csv(raw)
    return csv_file
45/6:
# Loading TMDB Movies Data
def load_data():
    data = raw_data('tmdb-movies.csv')
    return data
45/7:
# Preview the Data
movies_data = load_data()
movies_data.tail()
45/8: movies_data.info()
45/9: movies_data.dtypes
45/10:
# Preview the last five rows of the Dataset
movies_data.tail()
45/11:
# Preview the first five rows of the Dataset
movies_data = load_data()
movies_data.head()
45/12:
# View how many columns and rows are in the dataset.
movies_data.info()
45/13:
# Check for null values in the dataset
movies_data.isna()
45/14:
# Check for null values in the dataset
movies_data.isna().sum()
45/15:
# Check for null values in the dataset
movies_data.isnull().sum()
45/16:
# View rows with duplicate values
movies_data.duplicated().sum()
45/17:
# View non-null unique values in the dataset
movies_data.nunique()
45/18:
# View rows with duplicate values
movies_data.duplicated()
45/19:
# View rows with duplicate values
n = movies_data.duplicated()
n
45/20:
# View number of rows with duplicate values
n = movies_data.duplicated()
45/21:
# View number of rows with duplicate values
movies_data.duplicated()
45/22:
# View number of rows with duplicate values
movies_data.duplicated().sum()
45/23:
# Find duplicate rows in all columns
movies_data = movies_data[movies_data.duplicated()]
45/24:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/25:
# Preview the last five rows of the Dataset
df.tail()
45/26:
# View how many columns and rows are in the dataset.
df.info()
45/27:
# View the data types of the columns of the Dataset
df.dtypes
45/28:
# View number of rows with duplicate values
df.duplicated().sum()
45/29:
# Find duplicate rows in all columns
df = df[df.duplicated()]
df
45/30: dups = df.original_title['TEKKEN']
45/31: dups = df.original_title == 'TEKKEN'
45/32: df.original_title == 'TEKKEN'
45/33:
dups = df.groupby('original_title', 'TEKKEN')
dups
45/34: df.loc[df['original_title'] == 'TEKKEN']
45/35:
# Find duplicate rows in all columns
df[df.duplicated()]
45/36:
# Find duplicate rows in all columns
df_dups = df[df.duplicated()]
df_dups
45/37:
# View number of rows with duplicate values
df.duplicated().sum()
45/38:
# A Function to Load Data from CSV files
def raw_data(raw):
    csv_file = pd.read_csv(raw)
    return csv_file
45/39:
# Loading TMDB Movies Data
def load_data():
    data = raw_data('tmdb-movies.csv')
    return data
45/40:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/41:
# Preview the last five rows of the Dataset
df.tail()
45/42:
# View how many columns and rows are in the dataset.
df.info()
45/43:
# View the data types of the columns of the Dataset
df.dtypes
45/44:
# View number of rows with duplicate values
df.duplicated().sum()
45/45:
# Find duplicate rows in all columns
df_dups = df[df.duplicated()]
df_dups
45/46: df.loc[df['original_title'] == 'TEKKEN']
45/47:
# Find duplicate rows in all columns
df_duplicates = df[df.duplicated()]
df_duplicates
45/48:
# Filter duplicate rows in all columns
df_duplicates = df[df.duplicated()]
df_duplicates
45/49:
# Check for missing values in the dataset
df.isnull().sum()
45/50: pd.to_datetime(df.release_date)
45/51:
pd.to_datetime(df.release_date)
df.head()
45/52:
pd.to_datetime(df.release_date)
df.release_date.dtype
45/53:
# Filter duplicate rows in all columns
df_duplicates = df[df.duplicated()]
df_duplicates
df.release_date.dtype
45/54:
# Filter duplicate rows in all columns
df_duplicates = df[df.duplicated()]
df_duplicates
df.dtype
45/55:
# Filter duplicate rows in all columns
df_duplicates = df[df.duplicated()]
df_duplicates
df.dtypes
45/56:
# Filter duplicate rows in all columns
df_duplicates = df[df.duplicated()]
df_duplicates
45/57:
pd.to_datetime(df.release_date)
df.release_date.dtypes
45/58:
pd.to_datetime(df.release_date)
df.dtypes
45/59:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/60:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/61:
# Preview the last five rows of the Dataset
df.tail()
45/62:
# View how many columns and rows are in the dataset.
df.info()
45/63:
# View the data types of the columns of the Dataset
df.dtypes
45/64:
# View number of rows with duplicate values
df.duplicated().sum()
45/65:
# Filter duplicate rows in all columns
df_duplicates = df[df.duplicated()]
df_duplicates
45/66:
# Filter duplicate row in all columns
df_duplicates = df[df.duplicated()]
df_duplicates
45/67:
# Check for missing values in the dataset
df.isnull().sum()
45/68: ###### The dataset has a total of 10,866 rows, and 21 columns
45/69: df.columns
45/70:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
45/71:
# Check for missing values in the dataset
df.isnull().sum()
45/72:
# Check for missing values in the dataset
df.id.isnull().sum()
45/73:
# Check for missing values in the dataset
df.id.isnull()
45/74:
# Check for missing values in the dataset
df.id.isnull().cont()
45/75:
# Check for missing values in the dataset
df.id.isnull().count()
45/76:
# Check for missing values in the dataset
df.id.isnull().value_counts()
45/77:
# Check for missing values in the dataset
df.id.value_counts().isnull()
45/78:
# Check for missing values in the dataset
df.id.value_counts().isnull().sum()
45/79:
# Check for missing values in the dataset
df.loc[df.id == 0]
45/80:
# Check for missing values in the dataset
df.loc[df.id.isnull()]
45/81:
# Check for missing values in the dataset
df.loc[df.id.isna()]
45/82:
# Check for missing values in the dataset
df.loc[df.isna()]
45/83:
# Check for missing values in the dataset
df.loc[df.isnull()]
45/84:
# Check for missing values in the dataset
df = df.loc[df.isnull()]
45/85:
# Check for missing values in the dataset
df = df.loc[df.id.isnull()]
45/86:
# Check for missing values in the dataset
df = df.loc[df.id.isnull()]
df
45/87:
# Check for missing values in the dataset
df.isnull()
45/88:
# Check for missing values in the dataset
df.isnull()
45/89:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
45/90:
# Check for missing values in the dataset
df.isnull()
45/91:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/92:
# View the data types of the columns of the Dataset
df.dtypes
45/93:
# View number of rows with duplicate values
df.duplicated().sum()
45/94:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/95:
# Preview the last five rows of the Dataset
df.tail()
45/96:
# View how many columns and rows are in the dataset.
df.info()
45/97: df.columns
45/98:
# View the data types of the columns of the Dataset
df.dtypes
45/99:
# View number of rows with duplicate values
df.duplicated().sum()
45/100:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
45/101:
# Check for missing values in the dataset
df.isnull()
45/102: df.id.max()
45/103: df.id.min()
45/104: df[df.id==5]
45/105: df[df.id==1]
45/106: df[df.id==1]
45/107:
# Check for missing values in the dataset
df.isnull().value_counts()
45/108:
# Check for missing values in the dataset
df.isnull().sum()
45/109: df.rename(df.production_companies, 'prod_company', inplace=True)
45/110: df =df['production_companies'] = 'prod_company'
45/111: df =df['production_companies'] = df['prod_company']
45/112: df['production_companies'] = 'prod_company'
45/113: df['production_companies'] = df['prod_company']
45/114: df.rename(columns= {'production_companies': 'prod_company'}, inplace=True)
45/115: df.rename(columns={'production_companies': 'prod_company'}, axis=1, inplace=True)
45/116:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
45/117:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/118:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/119: df.rename(columns={'production_companies': 'prod_company'}, axis=1, inplace=True)
45/120: df.rename(columns={'production_companies': 'prod_company'}, inplace=True)
45/121:
df.rename(columns={'production_companies': 'prod_company'}, inplace=True)
df
45/122:
df.rename(columns={'production_companies': 'prod_company'}, inplace=True)
df.columns
45/123:
df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
df.columns
45/124:
df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
df.columns
45/125:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/126:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/127:
df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
df.columns
45/128:
df.dropna()
df.isnull()
45/129:
df.dropna()
df.isnull().sum()
45/130:
df.dropna(inplace=True)
df.isnull().sum()
45/131:
df.drop_duplicates(inplace=True)
df.duplicated().sum()
45/132:
df.drop([0, 1], inplace=True)
df.info()
45/133:
df.drop([0, 1], inplace=True)
df.info()
45/134: df.info()
45/135:
df.drop([0, 1], axis =1, inplace=True)
df.info()
45/136:
df.drop(['id', 'imdb_id'], axis =1, inplace=True)
df.info()
45/137:
# Check if the columns have been dropped
df.head(1)
45/138:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/139:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/140:
# Check for missing values in the dataset
df.isnull().sum()
45/141:
df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
# check if the columns have been renamed
df.columns
45/142:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
45/143:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/144:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/145:
df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
# check if the column has been renamed
df.columns
45/146:
df.dropna(inplace=True)
# check if there are missing values
df.isnull().sum()
45/147:
df.drop_duplicates(inplace=True)
# check if there are duplicates
df.duplicated().sum()
45/148:
df.drop(['id', 'imdb_id'], axis =1, inplace=True)

# Check if the columns have been dropped
df.head(1)
45/149:
# group movies by high revenues
df.groupby('revenue', 'original_title')
45/150:
df.drop(['id', 'imdb_id'], axis =1, inplace=True)

# Check if the columns have been dropped
df.head(1)
45/151:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/152:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/153:
df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
# check if the column has been renamed
df.columns
45/154:
df.dropna(inplace=True)
# check if there are missing values
df.isnull().sum()
45/155:
df.drop_duplicates(inplace=True)
# check if there are duplicates
df.duplicated().sum()
45/156:
df.drop(['id', 'imdb_id'], axis =1, inplace=True)

# Check if the columns have been dropped
df.head(1)
45/157:
# group movies by high revenues
df.groupby('revenue', 'original_title')
45/158:
# group movies by high revenues
df.groupby(['revenue', 'original_title'])
45/159:
# group movies by high revenues
df.groupby(['revenue', 'original_title']).head(5)
45/160:
# group movies by high revenues
df.groupby(df.revenue.max())
45/161:
# group movies by high revenues
df.revenue.max()
45/162:
# group movies by high revenues
df.revenue.max(5)
45/163:
# group movies by high revenues
df.groupby(df.revenue.max())
45/164:
# group movies by high revenues
df.groupby([df.revenue.max()])
45/165:
# group movies by high revenues
df.groupby([df.revenue])
45/166:
# group movies by high revenues
df.groupby(df.revenue)
45/167:
# group movies by high revenues
revenues = df.groupby(df.revenue)
revenues.first()
45/168:
# group movies by high revenues
revenues = df.groupby(df.revenue)
revenues.last()
45/169:
# group movies by high revenues
revenues = df.groupby(df.revenue)
revenues.last(10)
45/170:
# group movies by high revenues
df_high_revenues = df.revenue.max()
45/171:
# group movies by high revenues
df_high_revenues = df.revenue.max()
df_high_revenues
45/172:
# group movies by high revenues
df_high_revenues = df.revenue.max()
df_high_revenues
45/173:
# group movies by high revenues
df_high_revenues = df.groupby('revenue')
df_high_revenues
45/174:
# group movies by high revenues
df_high_revenues = df.groupby('revenue')
df_high_revenues.first()
45/175:
# group movies by high revenues
df_high_revenues = df.groupby('revenue')
df_high_revenues.head()
45/176:
# group movies by high revenues
df_high_revenues = df.groupby('revenue')
df_high_revenues.head(10)
45/177:
# group movies by high revenues
df_high_revenues = df.groupby('revenue')
gr = df_high_revenues['original_title'].apply(list)
gr
45/178:
# group movies by high revenues
df_high_revenues = df.groupby('revenue')
gr = df_high_revenues['original_title'].apply(list)
gr.tail()
45/179:
# group movies by high revenues
df_high_revenues = df.groupby('revenue')
gr = df_high_revenues['original_title'].apply(list)
gr.tail(10)
45/180:
# group movies by high revenues
# df_high_revenues = df.groupby('revenue')
# gr = df_high_revenues['original_title'].apply(list)
# gr.tail(10)
df.revenue.describe()
45/181: df.revenue_adj
45/182: df.revenue_adj.describe()
45/183:
# highest revenue
df.revenue.max()
45/184: df.query(df.revenue == df.revenue.max())
45/185: df.query([df.revenue == df.revenue.max()])
45/186: df.query([df.revenue, df.revenue.max()])
45/187: high = df.query([df.revenue, df.revenue.max()])
45/188:
# highest revenue
df.revenue.max()
45/189: high = df.query(df.revenue, df.revenue.max())
45/190: high = df.query(df.revenue == df.revenue.max())
45/191: high = df.query('revenue' == df.revenue.max())
45/192:
max_r = df.revenue.max()
high = df.query('revenue' == max_r)
45/193:
max_r = df.revenue.max()
high = df.query('revenue == max_r')
45/194:
max_r = df.revenue.max()
high = df.query('revenue == $max_r')
45/195:
max_r = df.revenue.max()
high = df.query('revenue == 2781505847')
45/196:
max_r = df.revenue.max()
high = df.query('revenue == 2781505847')
high
45/197: df.revenue.median()
45/198:
highest_revenue_movie = df.query('revenue == 2781505847')
highest_revenue_movie
45/199: df.revenue_adj.max()
45/200:
df.revenue_adj.max()
highest_revenue_movie = df.query('revenue_adj == 2827123750.41189')
highest_revenue_movie
45/201: df.describe()
45/202: df.revenue()
45/203: df.revenue.describe()
45/204: df.groupby('original_title')['revenue']
45/205: df.groupby('original_title')['revenue'].head()
45/206: df.groupby('original_title')['revenue'].head(5)
45/207: df.groupby('revenue')['original_title'].head(5)
45/208: df.groupby('revenue')['original_title'].first(5)
45/209: df.groupby('original_title')['revenue'].first(5)
45/210: df.groupby('original_title')['revenue'].last(5)
45/211: df.groupby('original_title')['revenue'].head(5)
45/212:
g = df.groupby('original_title')['revenue']
g.count()
45/213:
g = df.groupby('original_title')['revenue']
g.value_count()
45/214:
g = df.groupby('original_title')['revenue']
g.value_counts()
45/215: df.groupby('revenue')
45/216:
f = df.groupby('revenue')
f.sort_index(ascending=False)
45/217:
f = df.groupby('revenue', sort=False)
f.sort_index(ascending=False)
45/218:
f = df.groupby('revenue', sort=False).sum()
f.sort_index(ascending=False)
45/219:
f = df.groupby('revenue', sort=False).sum()
f.sort_index(ascending=False).head(5)
45/220:
f = df.groupby('revenue', sort=False).count()
f.sort_index(ascending=False).head(5)
45/221:
f = df.groupby('revenue', sort=False).aggr()
f.sort_index(ascending=False).head(5)
45/222:
f = df.groupby('revenue', sort=False).aggregate()
f.sort_index(ascending=False).head(5)
45/223:
f = df.groupby('revenue', sort=False)
f.sort_index(ascending=False).head(5)
45/224:
f = df.groupby('revenue', sort=False)

f# f.sort_index(ascending=False).head(5)
45/225:
f = df.groupby('revenue', sort=False)

# f.sort_index(ascending=False).head(5)
f
45/226:
f = df.groupby('revenue', sort=True)

# f.sort_index(ascending=False).head(5)
f
45/227:
f = df.groupby('revenue', sort=True)

# f.sort_index(ascending=False).head(5)
f.sort_index(ascending=True)
45/228:
f = df.groupby('revenue', sort=True).sum()

# f.sort_index(ascending=False).head(5)
f.sort_index(ascending=True)
45/229:
f = df.groupby('revenue', sort=True).sum()

# f.sort_index(ascending=False).head(5)
f.sort_index(ascending=False)
45/230:
f = df.groupby('original_title')['revenue'].count().sort_values(ascending=False)

# f.sort_index(ascending=False).head(5)
f.sort_index(ascending=False)
45/231:
f = df.groupby('original_title')['revenue'].sum().sort_values(ascending=False)

# f.sort_index(ascending=False).head(5)
f.sort_index(ascending=False)
45/232:
f = df.groupby('original_title')['revenue'].sum().sort_values(ascending=False)

# f.sort_index(ascending=False).head(5)
f
45/233: df.groupby('original_title')['revenue'].sum().sort_values(ascending=False)
45/234:
df.groupby('original_title')['revenue']
    # sum().sort_values(ascending=False)
45/235:
df.groupby('original_title')['revenue'].max()
    # sum().sort_values(ascending=False)
45/236:
df.groupby('original_title')['revenue'].head()
    # sum().sort_values(ascending=False)
45/237:
df.groupby('original_title')['revenue'].sort_values(ascending=False)
    # sum().sort_values(ascending=False)
45/238:
df.groupby('original_title')['revenue'].max().sort_values(ascending=False)
    # sum().sort_values(ascending=False)
45/239:
df.groupby('original_title')['revenue'].max().sort_values(ascending=False).head(5)
    # sum().sort_values(ascending=False)
45/240: df.groupby('original_title')['revenue'].sum().sort_values(ascending=False).head(5)
45/241: df.groupby('original_title')['revenue'].sum()
45/242: df.groupby('original_title')['revenue'].sum().sort_values(ascending=False).head(5)
45/243: plt.plot(df.revenue)
45/244: plt.plot(df.revenue);
45/245: df.revenue.plot(kind='bar');
45/246:
plt.subplots(figsize=(8, 5))
plt.bar(df.revenue, df.original_title)
plt.title('Improvements in Fuel Economy from 2008 to 2018 by Vehicle Class')
plt.xlabel('Vehicle Class')
plt.ylabel('Increase in Average Combined MPG');
45/247:
plt.subplots(figsize=(8, 5))
plt.bar(df.original_title, df.revenue)
plt.title('Improvements in Fuel Economy from 2008 to 2018 by Vehicle Class')
plt.xlabel('Vehicle Class')
plt.ylabel('Increase in Average Combined MPG');
45/248: df.hist(figsize=(8,8))
45/249: df.hist(figsize=(8,8));
45/250: df.revenue.hist()
45/251:
mean = df.groupby('revenue').quality.mean()

low = mean['Low']
medium = mean['Medium']
medium_high = mean['Medium High']
high = mean['High']
mean
45/252:
mean = df.groupby('original_title').revenue.mean()

low = mean['Low']
medium = mean['Medium']
medium_high = mean['Medium High']
high = mean['High']
mean
45/253:
mean = df.groupby('original_title').revenue.mean()

# low = mean['Low']
# medium = mean['Medium']
# medium_high = mean['Medium High']
# high = mean['High']
mean
45/254:
mean = df.groupby('original_title').revenue.max()

# low = mean['Low']
# medium = mean['Medium']
# medium_high = mean['Medium High']
# high = mean['High']
mean
45/255:
mean = df.groupby('original_title').revenue.mean()

# low = mean['Low']
# medium = mean['Medium']
# medium_high = mean['Medium High']
# high = mean['High']
mean
45/256: plt.scatter(df.revenue, df.original_title);
45/257:
plt.scatter(df.revenue, df.original_title);
plt.plot(df.revenue);
45/258:
# plt.scatter(df.revenue, df.original_title);
# plt.plot(df.revenue);
grouped_by_rev = df.groupby('original_title').agg({'revenue': ['mean', 'min', 'max']})
grouped_by_rev
45/259:
# view statistics about the revenue column
df.genres.describe()
45/260:
# view statistics about the revenue column
df.genres.head(5)
45/261:
# view statistics about the revenue column
df.group.by('release_year', 'genres')
45/262:
# view statistics about the revenue column
df.groupby('release_year', 'genres')
45/263:
# view statistics about the revenue column
df.groupby(['release_year', 'genres'])
45/264:
# view statistics about the revenue column
df.groupby(['release_year', 'genres']).count()
45/265:
# view statistics about the revenue column
df.groupby(['release_year', 'genres']).popluarity.count()
45/266:
# view statistics about the revenue column
df.query(['release_year', 'genres']).popluarity.count()
45/267:
# view statistics about the revenue column
df.query('release_year', 'genres').popluarity.count()
45/268:
# view statistics about the revenue column
popularity_mean = df.popularity.mean()
df.query('release_year', 'genres').popluarity.count()
df.groupby(['release_year'], as_index=False)['popularity'].mean()
45/269:
# # view statistics about the revenue column
# popularity_mean = df.popularity.mean()
# df.query('release_year', 'genres').popluarity.count()
df.groupby(['release_year'], as_index=False)['popularity'].mean()
45/270:
# # view statistics about the revenue column
# popularity_mean = df.popularity.mean()
# df.query('release_year', 'genres').popluarity.count()
df.groupby(['release_year', 'original_title'], as_index=False)['popularity'].mean()
45/271:
# # view statistics about the revenue column
# popularity_mean = df.popularity.mean()
# df.query('release_year', 'genres').popluarity.count()
df.groupby(['original_title', 'release_year'], as_index=True)['popularity'].mean()
45/272:
# # view statistics about the revenue column
# popularity_mean = df.popularity.mean()
# df.query('release_year', 'genres').popluarity.count()
df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
45/273:
# # view statistics about the revenue column
# popularity_mean = df.popularity.mean()
# df.query('release_year', 'genres').popluarity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
min_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].min()
45/274:
# # view statistics about the revenue column
# popularity_mean = df.popularity.mean()
# df.query('release_year', 'genres').popluarity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
min_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].min()
45/275:
# # view statistics about the revenue column
# popularity_mean = df.popularity.mean()
# df.query('release_year', 'genres').popluarity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
min_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].min()
min_rated
45/276: df.popularity.mean()
45/277: df.popularity.describe()
45/278:

pop_mean = df.popularity.mean()
pop_min = df.popularity.mean()

# df.query('release_year', 'genres').popluarity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').mean().popularity
45/279:

pop_mean = df.popularity.mean()
pop_min = df.popularity.mean()

# df.query('release_year', 'genres').popluarity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean()
45/280:

pop_mean = df.popularity.mean()
pop_min = df.popularity.mean()

# df.query('release_year', 'genres').popluarity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean().sort_values(ascending=False)
45/281: df.popularity.median()
45/282:

low_popularity = df.query('popularity  < df.popularity.median()')
high_popularity = df.query('popularity > df.popluarity.median()')

# df.query('release_year', 'genres').popluarity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean().sort_values(ascending=False)
45/283:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity > 0.7742225')

# df.query('release_year', 'genres').popluarity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean().sort_values(ascending=False)

num_samples = df.shape[0]
num_samples == low_popularity['popularity'].count() + high_popularity['popularity'].count() # should be True
45/284: low_popularity.mean(), high_popularity.mean()
45/285:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity > 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean().sort_values(ascending=False)

num_samples = df.shape[0]
num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/286: low_popularity.mean(), high_popularity.mean()
45/287:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity > 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean().sort_values(ascending=False)

num_samples = df.shape[0]
num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
df.shape[0]
45/288:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity > 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean().sort_values(ascending=False)

num_samples = df.shape[0]
num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
df.info()
45/289:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity > 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean().sort_values(ascending=False)

num_samples = df.shape[0]
num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
df.isnull()
45/290:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity > 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean().sort_values(ascending=False)

num_samples = df.shape[0]
num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
df.isnull().sum()
45/291: low_popularity.mean()
45/292:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean().sort_values(ascending=False)

num_samples = df.shape[0]
num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/293: low_popularity.mean()
45/294: high_popularity.mean()
47/1:
# get mean quality rating for the low alcohol and high alcohol groups
low_alcohol.quality.mean()
45/295: high_popularity.revenue.mean()
45/296: high_popularity.revenue.mean(), low_popularity.revenue.mean()
45/297:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)[low_popularity.mean()]
# df.groupby('original_title').popularity.mean().sort_values(ascending=False)
# 
# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/298:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)[low_popularity.revenue.mean()]
# df.groupby('original_title').popularity.mean().sort_values(ascending=False)
#
# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/299:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

df.groupby(['original_title', 'release_year'], as_index=False)[low_popularity.revenue.mean()]
# df.groupby('original_title').popularity.mean().sort_values(ascending=False)
#
# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/300:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

# df.groupby(['original_title', 'release_year'], as_index=False)[low_popularity.revenue.mean()]
# df.groupby('original_title').popularity.mean().sort_values(ascending=False)
#
# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/301:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

df.groupby(['original_title', 'release_year'], as_index=False)[low_popularity.revenue.mean()]
# df.groupby('original_title').popularity.mean().sort_values(ascending=False)
#
# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/302:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title').popularity.mean().sort_values(ascending=False)

# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/303:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby('original_title', 'release_year').popularity.mean().sort_values(ascending=False)

# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/304:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby(['original_title', 'release_year']).popularity.mean().sort_values(ascending=False)

# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/305:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
g = df.groupby(['original_title', 'release_year']).popularity.mean().sort_values(ascending=False)
g.plot(figsize=(8,8))

# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/306:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
g = df.groupby(['genres', 'release_year']).popularity.mean().sort_values(ascending=False)
g.plot(figsize=(8,8));

# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/307:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
g = df.groupby(['genres', 'release_year']).popularity.mean().sort_values(ascending=False)


# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/308:

low_popularity = df.query('popularity  < 0.7742225')
high_popularity = df.query('popularity >= 0.7742225')

# df.query('release_year', 'genres').popularity.count()
mean_rated = df.groupby(['original_title', 'release_year'], as_index=False)['popularity'].mean()
df.groupby(['genres', 'release_year']).popularity.mean().sort_values(ascending=False)


# num_samples = df.shape[0]
# num_samples == low_popularity['revenue'].count() + high_popularity['revenue'].count() # should be True
45/309:
g = df.groupby('original_title')['revenue'].sum().sort_values(ascending=False).head(5)
g.plot(figsize=(8,8))
45/310:
g = df.groupby('original_title')['revenue'].mean().sort_values(ascending=False).head(5)
g.plot(figsize=(8,8));
45/311:
g = df.groupby('original_title')['revenue'].mean().sort_values(ascending=False).head(5)
g.plot(figsize=(15,15));
45/312:
g = df.groupby('original_title')['revenue'].mean().sort_values(ascending=False).head(10)
g.plot(figsize=(15,15));
45/313:
g = df.groupby('original_title')['revenue'].mean().sort_values(ascending=False).head(10)
g.plot(figsize=(15,15));
45/314:
g = df.groupby('original_title')['revenue'].mean().sort_values(ascending=False).head(80)
g.plot(figsize=(15,15));
high_revenue = df.query('revenue >= ')
45/315:
g = df.groupby('original_title')['revenue'].mean().sort_values(ascending=False).head(80)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/316:
g = df.groupby('original_title')['revenue'].mean().sort_values(ascending=False).head(10)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/317:
g = df.groupby('original_title')['revenue'].mean().sort_values(ascending=False).head(7)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/318:
g = df.groupby('release_year')['revenue'].mean().sort_values(ascending=False).head(7)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/319:
g = df.groupby('release_year')['original_title'].mean().sort_values(ascending=False).head(7)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/320:
g = df.groupby('popularity')['original_title'].mean().sort_values(ascending=False).head(7)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/321:
g = df.groupby('popularity')['revenue'].mean().sort_values(ascending=False).head(7)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/322:
g = df.groupby('original_title')['revenue'].mean().sort_values(ascending=False).head(7)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/323:
g = df.groupby('original_title')['popularity'].mean().sort_values(ascending=False).head(7)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/324:
# show relationship between revenue and original_title
df.revenue.hist(figsize=8,8);
45/325:
# show relationship between revenue and original_title
df.revenue.hist(figsize=(8,8));
45/326:
# show relationship between revenue and original_title
plt.scatter(df.revenue, df.original_title);
45/327:
# show relationship between revenue and original_title
plt.scatter(df.revenue, df.popularity);
45/328:
mean = df.groupby('popularity').quality.mean()
plt.plot(mean);
45/329:
mean = df.groupby('popularity').revenue.mean()
plt.plot(mean);
45/330:
# show relationship between revenue and original_title
plt.scatter(df.revenue, df.budget);
45/331:
# show relationship between revenue and original_title
plt.scatter(df.budget, df.revenue);
45/332:
# show relationship between revenue and original_title
plt.scatter(df.revenue, df.budget);
45/333: plt.scatter(df.revenue, df.runtime);
45/334: plt.scatter(df)
45/335: plt.scatter(df.revenue, df)
45/336: plt.scatter(df.revenue, df['release_date']);
45/337: plt.scatter(df.revenue, df['release_year']);
45/338: plt.scatter(df.release_year, df['revenue']);
45/339: plt.scatter(df.runtime, df.revenue);
45/340: plt.scatter(df.revenue, df.runtime);
45/341: plt.scatter(df.revenue, df['release_year']);
45/342: plt.scatter(df.revenue, df.release_year);
45/343: plt.scatter(df.release_year, df.revenue);
45/344: plt.scatter(df.revenue, df.release_year);
45/345:
colors = np.array([0, 10, 20, 30, 40, 45, 50, 55, 60, 70, 80, 90, 100])
plt.scatter(df.revenue, df.release_year, c=colors, cmap='viridis');
45/346:
colors = np.array([range(0, 10, 1992)])
plt.scatter(df.revenue, df.release_year, c=colors, cmap='viridis');
45/347:
colors = np.array([range(0,1992)])
plt.scatter(df.revenue, df.release_year, c=colors, cmap='viridis');
45/348:
colors = np.array([range(0,1992)])
plt.scatter(df.revenue, df.release_year, c=colors, cmap='viridis');
plt.colorbar()
45/349:
colors = np.array([range(0,1992)])
plt.scatter(df.revenue, df.release_year, c=colors, cmap='viridis');
plt.colorbar();
45/350:
colors = np.array([range(0,df.nrows())])
plt.scatter(df.revenue, df.release_year, c=colors, cmap='viridis');
plt.colorbar();
45/351:
colors = np.array([range(0,df.rows())])
plt.scatter(df.revenue, df.release_year, c=colors, cmap='viridis');
plt.colorbar();
45/352:
colors = np.array([range(0,df.rows)])
plt.scatter(df.revenue, df.release_year, c=colors, cmap='viridis');
plt.colorbar();
45/353:
colors = np.array([range(0, len(df.index))])
plt.scatter(df.revenue, df.release_year, c=colors, cmap='viridis');
plt.colorbar();
45/354: df.plot.scatter();
45/355:
# plt.scatter(df.revenue, df.runtime);
df.plot(x="runtime", y="revenue", kind="scatter");
45/356:
plt.scatter(df.revenue, df.runtime);
# df.plot(x="runtime", y="revenue", kind="scatter");
45/357: df.plot(x="runtime", y="revenue", kind="scatter");
45/358: #### Scatterplots of revenue against various features
45/359:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis');
plt.colorbar()
45/360:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis');
plt.colorbar();
45/361:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
df.plot(df.revenue, df.popularity, c=colors, cmap='viridis', kind='scatter')
plt.colorbar();
45/362:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
df.plot(x=df.revenue, y=df.popularity, c=colors, cmap='viridis', kind='scatter')
plt.colorbar();
45/363:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x='revenue')
plt.colorbar();
45/364:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_axis='revenue')
plt.colorbar();
45/365:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_ticks='revenue')
plt.colorbar();
45/366:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_ticks='revenue')
sns.scatterplot(x=df.popularity, y=df.revenue, data=df)
plt.colorbar();
45/367:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
# plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_ticks='revenue')
sns.scatterplot(x=df.popularity, y=df.revenue, data=df)
plt.colorbar();
45/368:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
sns.set(style='whitegrid')
data = sns.load_dataset(df)
# plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_ticks='revenue')
sns.scatterplot(x='popularity', y='revenue', hue='popularity', style='revenue', data=data)
plt.colorbar();
45/369:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
sns.set(style='whitegrid')
# plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_ticks='revenue')
sns.scatterplot(x='popularity', y='revenue', hue='popularity', style='revenue', data=df)
plt.colorbar();
45/370:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
sns.set(style='whitegrid')
# plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_ticks='revenue')
sns.scatterplot(x='popularity', y='revenue', style='popularity', hue='revenue', data=df);
# plt.colorbar();
45/371:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
sns.set(style='whitegrid')
# plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_ticks='revenue')
sns.scatterplot(x='popularity', y='revenue', data='revenue', hue='popularity');
# plt.colorbar();
45/372:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
sns.set(style='whitegrid')
# plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_ticks='revenue')
sns.scatterplot(x='popularity', y='revenue', data=df['revenue'], hue=df['popularity']);
# plt.colorbar();
45/373:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
sns.set(style='whitegrid')
# plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_ticks='revenue')
sns.scatterplot(x=df['popularity'], y='revenue', data=df['revenue'], hue=df['popularity']);
# plt.colorbar();
45/374:
# show relationship between revenue and original_title
colors = np.array([range(0, len(df.index))])
sns.set(style='whitegrid')
# plt.scatter(df.revenue, df.popularity, c=colors, cmap='viridis', x_ticks='revenue')
sns.scatterplot(x=df['popularity'], y=df['revenue'], data=df['revenue'], hue=df['popularity']);
# plt.colorbar();
45/375: plt.scatter(df.popularity, df.revenue);
45/376: plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis');
45/377:
plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis');
plt.colorbar()
45/378:
plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis')
plt.colorbar();
45/379: pd.plotting.scatter_matrix(df, figsize = (15,15));
45/380:
# show relationship between revenue and budget
colors = np.array([range(0, len(df.index))])
plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis')
plt.colorbar()
plt.legend()
45/381:
# show relationship between revenue and budget
colors = np.array([range(0, len(df.index))])
plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis')
plt.colorbar()
plt.legend();
45/382:
# show relationship between revenue and budget
colors = np.array([range(0, len(df.index))])
l = ['revenu', 'popularity']
plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis', label=l)
plt.colorbar()
plt.legend();
45/383:
# show relationship between revenue and budget
colors = np.array([range(0, len(df.index))])
l = ['revenue', 'popularity']
plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis', label=l)
plt.colorbar()
plt.legend();
45/384:
# show relationship between revenue and budget
colors = np.array([range(0, len(df.index))])
l = ['revenue', 'popularity']
plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis', x_label=l)
plt.colorbar()
plt.legend();
45/385:
# show relationship between revenue and budget
colors = np.array([range(0, len(df.index))])
plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis')
plt.colorbar()
plt.xlabel("popularity")
plt.ylabel("revenue")
45/386:
# show relationship between revenue and budget
colors = np.array([range(0, len(df.index))])
plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis')
plt.colorbar()
plt.xlabel("popularity")
plt.ylabel("revenue");
45/387:
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])
    plt.colorbar()
    return plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis')
45/388:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue");
45/389:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
45/390:
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])

    return plt.scatter(df.popularity, df.revenue, c=colors, cmap='viridis')
45/391:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
45/392:
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
45/393:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
45/394:
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])
    plt.colorbar()
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
45/395:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
# plt.colorbar();
45/396:
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
45/397:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
45/398:
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis', figsize=(15,15))
45/399:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
45/400:
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])
    plt.figure(figsize=(10,10))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
45/401:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
45/402:
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
45/403:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
45/404:
plot_scatter(df.budget, df.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
45/405:
# show relationship between revenue and runtime
plot_scatter(df.runtime, df.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
45/406:
# show relationship between vote_count and revenue
plot_scatter(df.vote_count, df.revenue)
plt.xlabel("vote_count")
plt.ylabel("revenue")
plt.colorbar();
45/407:
# show relationship between vote-average and revenue
plot_scatter(df.vote_average, df.revenue)
plt.xlabel("vote_average")
plt.ylabel("revenue")
plt.colorbar();
45/408:
# show relationship between release_year and revenue
plot_scatter(df.release_year, df.revenue)
plt.xlabel("release_year")
plt.ylabel("revenue")
plt.colorbar();
45/409:
# show relationship between release_year and revenue
plot_scatter(df.release_year.todatetime(), df.revenue)
plt.xlabel("release_year")
plt.ylabel("revenue")
plt.colorbar();
45/410:
# show relationship between release_year and revenue
plot_scatter(pd.to_datetime(df.release_year), df.revenue)
plt.xlabel("release_year")
plt.ylabel("revenue")
plt.colorbar();
45/411:
# show relationship between release_year and revenue
plot_scatter(df.release_year, df.revenue)
plt.xlabel("release_year")
plt.ylabel("revenue")
plt.colorbar();
45/412: pd.to_datetime(df.release_date)
45/413: df.release_date
45/414: pd.to_datetime(df.release_date).head()
45/415: df.release_date.head()
45/416: pd.to_datetime(df.release_date, format='%m%d%Y').head()
45/417: pd.to_datetime(df.release_date, format='%m%d%Y', errors=ignore).head()
45/418: pd.to_datetime(df.release_date, format='%m%d%Y', errors='ignore').head()
45/419: pd.to_datetime(df.release_date, format='%m%d%Y').head()
45/420: pd.to_datetime(df.release_date, format='%d%m%Y').head()
45/421: pd.to_datetime([df.release_date] * 1000)
45/422: pd.to_datetime(df.release_date * 1000)
45/423: pd.to_datetime(df.release_date).head()
45/424: df.info()
45/425: pd.to_datetime(df.release_date)
45/426: pd.to_datetime(df.release_year)
45/427: pd.to_datetime(df.release_year).head()
45/428:
plt.scatter(pd.to_datetime(df.release_year), df.revenue)
plt.xlabel("release_year")
plt.ylabel("revenue")
plt.colorbar();
45/429:
plt.scatter(pd.to_datetime(df.release_date), df.revenue)
plt.xlabel("release_year")
plt.ylabel("revenue")
plt.colorbar();
45/430:
date_ = pd.to_datetime(df.release_year)
df.plot(x=date_, y=df.revenue);
45/431:
date_ = pd.to_datetime(df.release_year)
df.plot(x=df.release_year, y=df.revenue);
45/432:

df.plot(x=df.release_year, y=df.revenue);
45/433: pd.to_datetime(df.release_year).head()
45/434:

df.plot(x=df.release_year, y=df.revenue);
45/435:

df.info()
45/436:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/437:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/438:
# View how many columns and rows are in the dataset.
df.info()
45/439: df.columns
45/440:
# View the data types of the columns of the Dataset
df.dtypes
45/441:
# View number of rows with duplicate values
df.duplicated().sum()
45/442:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
45/443:
# Check for missing values in the dataset
df.isnull().sum()
45/444:
df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
# check if the column has been renamed
df.columns
45/445:
df.dropna(inplace=True)
# check if there are missing values
df.isnull().sum()
45/446:
df.drop_duplicates(inplace=True)
# check if there are duplicates
df.duplicated().sum()
45/447:
df.drop(['id', 'imdb_id'], axis =1, inplace=True)

# Check if the columns have been dropped
df.head(1)
45/448:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/449:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/450:
# Preview the last five rows of the Dataset
df.tail()
45/451:
# View how many columns and rows are in the dataset.
df.info()
45/452: df.columns
45/453:
# View the data types of the columns of the Dataset
df.dtypes
45/454:
# View number of rows with duplicate values
df.duplicated().sum()
45/455:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
45/456:
# Check for missing values in the dataset
df.isnull().sum()
45/457:
# Check for missing values in the dataset
df.isnull().sum()
45/458:
df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
# check if the column has been renamed
df.columns
45/459:
df.dropna(inplace=True)
# check if there are missing values
df.isnull().sum()
45/460:
df.drop_duplicates(inplace=True)
# check if there are duplicates
df.duplicated().sum()
45/461:
df.drop(['id', 'imdb_id', 'homepage', 'overview'], axis =1, inplace=True)

# Check if the columns have been dropped
df.head(1)
45/462: df['release_date'] = pd.to_datetime(df.release_date)
45/463:
# check release_date datatype
df.release_date.dtypes
45/464: df.info
45/465: df.info()
45/466:
# check release_date datatype
df.release_date.dtypes
45/467:
# check release_date datatype
df.columns.dtypes
45/468:
# check release_date datatype
df.dtypes
45/469: df['release_year'] = pd.to_datetime(df.release_year)
45/470:
# check release_year datatype
df.dtypes
45/471: df.revenue.hist();
45/472: df.genres.hist();
45/473: df.popularity.hist();
45/474: plt.scatter(df.genres, df.popularity);
45/475: plt.scatter(df.release_date, df.popularity);
45/476: df.plot(df.release_date, df.popularity);
45/477: df.plot(df.release_year, df.popularity);
45/478:
axes = df.set_index('release_date', inplace=True)
axes.plot(y=df.popularity);
45/479:
df.set_index('release_date', inplace=True)
df.plot(y=df.popularity);
45/480:
df.set_index(df.release_year, inplace=True)
df.plot(y=df.popularity);
45/481: df.info()
45/482: df.head(2)
45/483:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/484:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/485:
# View how many columns and rows are in the dataset.
df.info()
45/486: df.columns
45/487:
# View the data types of the columns of the Dataset
df.dtypes
45/488:
# View number of rows with duplicate values
df.duplicated().sum()
45/489:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
45/490:
# Check for missing values in the dataset
df.isnull().sum()
45/491:
df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
# check if the column has been renamed
df.columns
45/492:
# check if the column has been renamed
df.columns
45/493:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
45/494:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
45/495:
# Preview the last five rows of the Dataset
df.tail()
45/496: df.columns
45/497: df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
45/498:
# check if the column has been renamed
df.columns
45/499: df.dropna(inplace=True)
45/500:
# check if there are missing values
df.isnull().sum()
45/501: df.drop_duplicates(inplace=True)
45/502:
# check if there are duplicates
df.duplicated().sum()
45/503: df.drop(['id', 'imdb_id', 'homepage', 'overview'], axis =1, inplace=True)
45/504:
# Check if the columns have been dropped
df.head(1)
45/505: df['release_date'] = pd.to_datetime(df.release_date)
45/506:
# check release_date datatype
df.dtypes
45/507: df['release_year'] = pd.to_datetime(df.release_year)
45/508:
# check release_year datatype
df.dtypes
45/509: df.head(2)
45/510: pd.plotting.scatter_matrix(df, figsize = (15,15));
45/511:
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
45/512:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
45/513:
# A method to create scatter plots
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
45/514:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
45/515:
# show relationship between revenue and runtime
plot_scatter(df.runtime, df.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
45/516:
# show relationship between vote_count and revenue
plot_scatter(df.vote_count, df.revenue)
plt.xlabel("vote_count")
plt.ylabel("revenue")
plt.colorbar();
45/517:
# show relationship between vote-average and revenue
plot_scatter(df.vote_average, df.revenue)
plt.xlabel("vote_average")
plt.ylabel("revenue")
plt.colorbar();
45/518:
# show relationship between release_year and revenue
plot_scatter(df.release_year, df.revenue)
plt.xlabel("release_year")
plt.ylabel("revenue")
plt.colorbar();
45/519:
# show relationship between revenue and runtime
plot_scatter(df.budget, df.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
45/520: df.query('runtime > 200')
45/521: df.runtime.describe()
45/522: sns.boxplot(df.runtime)
45/523: sns.boxplot(df['runtime']);
45/524: sns.boxplot(x=df['runtime']);
45/525: sns.boxplot(x=df.runtime);
45/526:

# IQR
Q1 = np.percentile(df['runtime'], 25,
                   interpolation = 'midpoint')

Q3 = np.percentile(df['runtime'], 75,
                   interpolation = 'midpoint')
IQR = Q3 - Q1
45/527:

# IQR
Q1 = np.percentile(df['runtime'], 25, interpolation = 'midpoint')

Q3 = np.percentile(df['runtime'], 75, interpolation = 'midpoint')
IQR = Q3 - Q1
IQR
45/528:

# IQR
Q1 = np.percentile(df['runtime'], 25, method = 'midpoint')

Q3 = np.percentile(df['runtime'], 75, method = 'midpoint')
IQR = Q3 - Q1
IQR
45/529:

# IQR
Q1 = np.percentile(df['runtime'], 25, method = 'median_unbiased')

Q3 = np.percentile(df['runtime'], 75, method = 'midpoint')
IQR = Q3 - Q1
IQR
45/530:

# IQR
Q1 = np.percentile(df['runtime'], 25, method = 'median_unbiased')

Q3 = np.percentile(df['runtime'], 75, method = 'median_unbiased')
IQR = Q3 - Q1
IQR
45/531:
upper = df['runtime']>= (Q3+1.5*IQR)
np.where(upper)
45/532:
upper = df['runtime']>= (Q3+1.5*IQR)
np.where("upper", upper)
45/533:
upper = df['runtime']>= (Q3+1.5*IQR)
np.where(upper)
45/534:
upper = df['runtime']>= (Q3+1.5*IQR)
np.where(upper)
lower = df['runtime'] <= (Q1-1.5*IQR)
45/535:
upper = df['runtime']>= (Q3+1.5*IQR)
np.where(upper)
lower = df['runtime'] <= (Q1-1.5*IQR)
lower
45/536:
upper = df['runtime']>= (Q3+1.5*IQR)
np.where(upper)
lower = df['runtime'] <= (Q1-1.5*IQR)
np.where(lower)
45/537:
np.where(df['runtime']>= (Q3+1.5*IQR))

lower = df['runtime'] <= (Q1-1.5*IQR)
np.where(lower)
45/538:
np.where(df['runtime']>= (Q3+1.5*IQR))

np.where(df['runtime'] <= (Q1-1.5*IQR))
45/539:

# IQR
Q1 = df.runtime(0.25)

Q3 = df.runtime(0.75)
IQR = Q3 - Q1
IQR
45/540:

# IQR
Q1 = df.runtime.quantile(0.25)

Q3 = df.runtime.quantile(0.75)
IQR = Q3 - Q1
IQR
45/541:

# IQR
Q1 = df.runtime.quantile(0.25)

Q3 = df.runtime.quantile(0.75)
IQR = Q3 - Q1
45/542:

# IQR
Q1 = df.runtime.quantile(0.25)

Q3 = df.runtime.quantile(0.75)
IQR = Q3 - Q1
print(IQR)
45/543:

# IQR
Q1 = df.runtime.quantile(0.25)

Q3 = df.runtime.quantile(0.75)
IQR = Q3 - Q1
df.runtime.IQR
45/544: np.where(df['runtime'] > (Q3+1.5*IQR)), np.where(df['runtime'] < (Q1-1.5*IQR))
45/545:
# show relationship between revenue and runtime
plot_scatter(df.director, df.revenue)
plt.xlabel("director")
plt.ylabel("revenue")
plt.colorbar();
45/546: df.director.hist()
45/547: df.director.hist();
45/548: df.groupby('director').popularity.mean()
45/549: df.groupby('director').popularity.sum()
45/550: df.groupby('director').popularity.mean().hist()
45/551: df.groupby(['director', 'original_title']).popularity.mean()
45/552: df.groupby(['director', 'original_title']).popularity.mean().sort_values(ascending=False)
45/553: df.groupby(['director', 'original_title', 'genres', 'revenue']).popularity.mean().sort_values(ascending=False)
45/554: df.groupby(['director', 'original_title', 'genres', 'revenue']).popularity.mean().sort_values(ascending=False).head(5)
45/555: df.revenue.describe()
45/556:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [0.00, 0.00, 2.57, 1.27, 2.78] # Fill in this list with five values you just found
45/557: bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
45/558:
g = df.groupby('release_date')['popularity'].mean().sort_values(ascending=False).head(7)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/559:
g = df.groupby('release_date')['revenue'].mean().sort_values(ascending=False).head(7)
g.plot(figsize=(15,15));
# high_revenue = df.query('revenue >= ')
45/560: plt.plot(df.revenue);
45/561: plt.bar(df.revenue, df.popularity);
45/562:
plt.bar(df.revenue, df.popularity);
plt.show()
45/563:
plt.bar(df.revenue, df.popularity)
plt.show();
45/564: sns.boxplot(x=df.revenue);
45/565: sns.boxplot(x=df.popularity);
45/566: sns.boxplot(x=df.budget);
45/567: df.groupby(['director', 'original_title', 'revenue']).popularity.mean().sort_values(ascending=False).head(5)
45/568: df.groupby(['director', 'original_title', 'revenue']).popularity.mean().sort_values(ascending=False).head(15)
45/569: df.groupby(['director', 'original_title', 'release_year', 'revenue']).popularity.mean().sort_values(ascending=False).head(15)
45/570: df.groupby(['director', 'original_title', 'release_date', 'revenue']).popularity.mean().sort_values(ascending=False).head(15)
45/571: df.groupby(['director', 'original_title', 'release_date', 'popularity']).revenue.mean().sort_values(ascending=False).head(15)
45/572: df.groupby(['director', 'original_title', 'release_date','genres', 'popularity']).revenue.mean().sort_values(ascending=False).head(15)
45/573: df.groupby(['director', 'original_title', 'release_date','genres', 'popularity', 'runtime']).revenue.mean().sort_values(ascending=False).head(15)
45/574: df.groupby(['director', 'original_title', 'release_date','genres', 'runtime','popularity']).revenue.mean().sort_values(ascending=False).head(15)
45/575:
plt.plot(df.director, df.popularity, label='pop')
plt.plot(df.director, df.revenue, label='rev')
plt.legend();
45/576:
plt.plot(df.director, df.popularity, label='pop')
plt.plot(df.director, df.revenue, label='rev')
plt.legend()
plt.show();
45/577:
plt.plot(df.director, df.popularity, label='pop')
plt.plot(df.director, df.revenue, label='rev')
plt.legend()
plt.show();
45/578:
plt.plot(df.popularity, df.director, label='pop')
plt.plot(df.director, df.revenue, label='rev')
plt.legend()
plt.show();
45/579:
plt.plot(df.popularity, df.director, label='pop')
plt.plot(df.director, df.revenue, label='rev')
plt.legend()
plt.show();
45/580: sns.catplot(x="director", y="revenue", data=df);
48/1: sns.catplot(x="director", y="revenue", kind='swarm', data=df);
48/2:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
48/3: sns.catplot(x="director", y="revenue", kind='swarm', data=df);
48/4:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
48/5:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
48/6:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
48/7:
# Preview the last five rows of the Dataset
df.tail()
48/8: df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
48/9:
# check if the column has been renamed
df.columns
48/10: df.dropna(inplace=True)
48/11:
# check if there are missing values
df.isnull().sum()
48/12: df.drop_duplicates(inplace=True)
48/13:
# check if there are duplicates
df.duplicated().sum()
48/14: df.drop(['id', 'imdb_id', 'homepage', 'overview'], axis =1, inplace=True)
48/15:
# Check if the columns have been dropped
df.head(1)
48/16: df['release_date'] = pd.to_datetime(df.release_date)
48/17:
# check release_date datatype
df.dtypes
48/18: df['release_year'] = pd.to_datetime(df.release_year)
48/19:
# check release_year datatype
df.dtypes
48/20: pd.plotting.scatter_matrix(df, figsize = (15,15));
48/21:
# A method to create scatter plots
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
48/22:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
48/23:
# show relationship between revenue and budget
plot_scatter(df.budget, df.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
48/24:
# show relationship between revenue and runtime
plot_scatter(df.runtime, df.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
48/25: sns.boxplot(x=df.runtime);
48/26: df.groupby(['director', 'original_title', 'release_date','genres', 'runtime','popularity']).revenue.mean().sort_values(ascending=False).head(15)
48/27:
plt.plot(df.popularity, df.director, label='pop')
plt.plot(df.director, df.revenue, label='rev')
plt.legend()
plt.show();
48/28:
plt.plot(df.popularity, df.revenue, label='pop')
plt.plot(df.popularity, df.budget, label='rev')
plt.legend()
plt.show();
48/29:
plt.plot(df.popularity, df.revenue, label='revenue')
plt.plot(df.popularity, df.budget, label='budget')
plt.legend()
plt.show();
48/30:
plt.plot(df.revenue, df.popularity, label='popularity')
plt.plot(df.revenue, df.budget, label='budget')
plt.legend()
plt.show();
48/31: sns.catplot(x="director", y="revenue", kind='swarm', data=df);
48/32: sns.catplot(x="popularity", y="revenue", kind='swarm', data=df);
48/33:
plt.plot(df.revenue, df.popularity, label='popularity')
plt.plot(df.revenue, df.budget, label='budget')
plt.plot(df.revenue, df.runtime, label='runtime')
plt.legend()
plt.show();
48/34: sns.catplot(x="popularity", y="revenue", kind='swarm', data=df);
48/35:
plt.plot(df.director, df.popularity, label='popularity')
plt.plot(df.revenue, df.budget, label='budget')
plt.plot(df.revenue, df.runtime, label='runtime')
plt.legend()
plt.show();
48/36:
plt.plot(df.popularity, df.revenue, label='revenue')
plt.plot(df.popularity, df.budget, label='budget')
plt.plot(df.popularity, df.runtime, label='runtime')
plt.legend()
plt.show();
48/37:
plt.plot(df.budget, df.revenue, label='revenue')
plt.plot(df.budget, df.popularity, label='popularity')
plt.plot(df.budget, df.runtime, label='runtime')
plt.legend()
plt.show();
48/38:
plt.plot(df.runtime, df.revenue, label='revenue')
plt.plot(df.runtime, df.budget, label='budget')
plt.plot(df.runtime, df.popularity, label='popularity')
plt.legend()
plt.show();
48/39: sns.catplot(x="popularity", y="revenue", data=df);
48/40:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [0.00, 0.00, 2.57, 1.27, 2.78] # Fill in this list with five values you just found
48/41: bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
48/42: df['revenue_rank'] = pd.cut(df['revenue'], bin_edges, labels=bin_names)
48/43:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [0.00, 0.01, 2.57, 1.27, 2.78] # Fill in this list with five values you just found
48/44: bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
48/45: df['revenue_rank'] = pd.cut(df['revenue'], bin_edges, labels=bin_names)
48/46: ##### There is no clear correlation between popularity and revenue. Furthermore, there are a few outliers in both features.
48/47: sns.boxplot(x=df.revenue);
48/48: sns.boxplot(x=df.popularity);
48/49: sns.boxplot(x=df.revenue);
48/50: sns.boxplot(x=df.popularity);
48/51: sns.boxplot(x=df.runtime);
48/52: sns.boxplot(x=df.bugdet);
48/53: sns.boxplot(x=df.budget);
48/54: df.boxplot('revenue')
48/55: df.boxplot('revenue');
48/56: df.boxplot('runtime');
48/57: df.boxplot('popularity');
48/58: df.boxplot('runtime');
48/59: sns.boxplot(x=df.popularity);
48/60: sns.boxplot(x=df.runtime);
48/61:
target_cols = [df.revenue, df.popularity, df.runtime, df.budget]
Q1 = df[target_cols].quantile(0.25)
Q3 = df[target_cols].quatile(0.75)
IQR = Q3 - Q1
48/62:
target_cols = [df.revenue, df.popularity, df.runtime, df.budget]
Q1 = df[target_cols].quantile(0.25)
Q3 = df[target_cols].quatile(0.75)
IQR = Q3 - Q1
IQR
48/63:
# target_cols = [df.revenue, df.popularity, df.runtime, df.budget]
Q1 = df.quantile(0.25)
Q3 = df.quatile(0.75)
IQR = Q3 - Q1
IQR
48/64:
# target_cols = [df.revenue, df.popularity, df.runtime, df.budget]
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/65:
target_cols = [df.revenue, df.popularity, df.runtime, df.budget]
Q1 = df[target_cols].quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/66:
target_cols = [df.revenue, df.popularity, df.runtime, df.budget]
Q1 = df[target_cols].quantile(0.25)
Q3 = df[target_cols].quantile(0.75)
IQR = Q3 - Q1
IQR
48/67:
target_cols = [df.revenue, df.popularity, df.runtime, df.budget]
Q1 = df.revenue.quantile(0.25)
Q3 = df.revenue.quantile(0.75)
IQR = Q3 - Q1
IQR
48/68:
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/69:
from scipy.stats import stats

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = df.apply(stats.iqr)
IQR
48/70:
import scipy.stats

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = df.apply(stats.iqr)
IQR
48/71:
import scipy.stats

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = df.apply(scipy.stats.iqr)
IQR
48/72:
import scipy.stats



Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = df.apply(scipy.stats.iqr)
IQR
numerical_values = df['revenue', 'popularity', 'budget', 'runtime']
numerical_values
48/73:
import scipy.stats



Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
# IQR = df.apply(scipy.stats.iqr)
# IQR
numerical_values = df['revenue', 'popularity', 'budget', 'runtime']
numerical_values
48/74:
import scipy.stats



Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
# IQR = df.apply(scipy.stats.iqr)
# IQR
numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
numerical_values
48/75:
import scipy.stats



Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
# IQR = df.apply(scipy.stats.iqr)
# IQR
numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
IQR = df.apply(scipy.stats.iqr)
48/76:
import scipy.stats



Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
# IQR = df.apply(scipy.stats.iqr)
# IQR
numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
IQR = df.apply(stats.iqr)
48/77:
from scipy import stats




Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
# IQR = df.apply(scipy.stats.iqr)
# IQR
numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
IQR = df.apply(stats.iqr)
48/78:
from scipy import stats



numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = numerical_values.quantile(0.25)
Q3 = numerical_values.quantile(0.75)
# IQR = df.apply(scipy.stats.iqr)
# IQR

IQR = Q3-Q1
48/79:
from scipy import stats



numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = numerical_values.quantile(0.25)
Q3 = numerical_values.quantile(0.75)
# IQR = df.apply(scipy.stats.iqr)
# IQR

IQR = Q3-Q1
48/80:
from scipy import stats



numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = numerical_values.quantile(0.25)
Q3 = numerical_values.quantile(0.75)
# IQR = df.apply(scipy.stats.iqr)
# IQR

IQR = Q3-Q1
IQR
48/81:

numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = numerical_values.quantile(0.25)
Q3 = numerical_values.quantile(0.75)


IQR = Q3-Q1
numerical_values = numerical_values[~((numerical_values < (Q1 - 1.5 * IQR)) |(numerical_values > (Q3 + 1.5 * IQR))).any(axis=1)]
48/82:

numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = numerical_values.quantile(0.25)
Q3 = numerical_values.quantile(0.75)


IQR = Q3-Q1
numerical_values = numerical_values[~((numerical_values < (Q1 - 1.5 * IQR)) |(numerical_values > (Q3 + 1.5 * IQR))).any(axis=1)]
numerical_values
48/83:

numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = numerical_values.quantile(0.25)
Q3 = numerical_values.quantile(0.75)


IQR = Q3-Q1
numerical_values = numerical_values[~((numerical_values < (Q1 - 1.5 * IQR)) |(numerical_values > (Q3 + 1.5 * IQR))).any(axis=1)]
numerical_values.revenue.max()
48/84: numerical_values.boxplot('revenue')
48/85: sns.boxplot(x=numerical_values.budget);
48/86: sns.boxplot(x=numerical_values.runtime);
48/87:
plt.plot(numerical_values.revenue, numerical_values.popularity, label='popularity')
plt.plot(numerical_values.revenue, numerical_values.budget, label='budget')
plt.plot(df.revenue, df.runtime, label='runtime')
plt.legend()
plt.show();
48/88:
plt.plot(numerical_values.revenue, numerical_values.popularity, label='popularity')
plt.plot(numerical_values.revenue, numerical_values.budget, label='budget')
plt.plot(numerical_values.revenue, numerical_values.runtime, label='runtime')
plt.legend()
plt.show();
48/89:
plot_scatter(numerical_values.runtime, numerical_values.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
48/90:
plt.scatter(numerical_values.runtime, numerical_values.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
48/91:
plt.scatter(numerical_values.budget, numerical_values.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
48/92: numerical_values.isna()
48/93: numerical_values.isna().sum()
48/94: numerical_values.isnull().sum()
48/95:
from scipy.stats import iqr

iqr(df, axis=0)
48/96:
from scipy.stats import iqr

iqr(df.revenue, axis=0)
48/97:
q1 = np.quantile(df,0.25)
q3 = np.quantile(df, 0.75)
iqr = q3 -q1
48/98:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = np.quantile(nums_df,0.25)
q3 = np.quantile(nums_df, 0.75)
iqr = q3 -q1
48/99:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = np.quantile(nums_df,0.25)
q3 = np.quantile(nums_df, 0.75)
iqr = q3 -q1
48/100:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = np.quantile(nums_df,0.25)
q3 = np.quantile(nums_df, 0.75)
iqr = q3 -q1
iqr
48/101: df.describe()
48/102:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.quantile(nums_df,0.25)
# q3 = np.quantile(nums_df, 0.75)
# iqr = q3 - q1
# iqr
nums_df.describe()
48/103:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.quantile(nums_df,0.25)
# q3 = np.quantile(nums_df, 0.75)
# iqr = q3 - q1
# iqr
nums_df.info()
48/104:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = np.quantile(nums_df,0.25)
q3 = np.quantile(nums_df, 0.75)
iqr = q3 - q1
iqr
# nums_df.info()
48/105:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = nums_df.quantile(0.25)
q3 = nums_df.quantile(0.75)
iqr = q3 - q1
iqr
# nums_df.info()
48/106:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q3, q1 = np.percentile(nums_df,[75, 25])
# q3 = nums_df.quantile(0.75)
# iqr = q3 - q1
iqr = q3 - q1
# nums_df.info()
48/107:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q3, q1 = np.percentile(nums_df,[75, 25])
# q3 = nums_df.quantile(0.75)
# iqr = q3 - q1
iqr = q3 - q1
iqr
# nums_df.info()
48/108:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q3, q1 = np.percentile(nums_df,[75, 25])
iqr = np.subtract(*np.percentile(nums_df, [75, 25]))
# q3 = nums_df.quantile(0.75)
# iqr = q3 - q1
iqr
# nums_df.info()
48/109:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q3, q1 = np.percentile(nums_df,[75, 25])
iqr = np.subtract(*np.percentile(df, [75, 25]))
# q3 = nums_df.quantile(0.75)
# iqr = q3 - q1
iqr
# nums_df.info()
48/110:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q3, q1 = np.percentile(nums_df,[75, 25])
iqr = np.subtract(*np.percentile(nums_df, [75, 25]))
# q3 = nums_df.quantile(0.75)
# iqr = q3 - q1
iqr
# nums_df.info()
48/111:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = np.median(nums_df[:10])
q3 = np.median(nums_df[10:])

# q3 = nums_df.quantile(0.75)
iqr = q3 - q1
iqr
# nums_df.info()
48/112:

numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = numerical_values.quantile(0.25)
Q3 = numerical_values.quantile(0.75)


IQR = Q3-Q1
# numerical_values = numerical_values[~((numerical_values < (Q1 - 1.5 * IQR)) |(numerical_values > (Q3 + 1.5 * IQR))).any(axis=1)]
# numerical_values.revenue.maxx()
48/113:

numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = numerical_values.quantile(0.25)
Q3 = numerical_values.quantile(0.75)


IQR = Q3-Q1
# numerical_values = numerical_values[~((numerical_values < (Q1 - 1.5 * IQR)) |(numerical_values > (Q3 + 1.5 * IQR))).any(axis=1)]
# numerical_values.revenue.maxx()
48/114:

numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = numerical_values.quantile(0.25)
Q3 = numerical_values.quantile(0.75)


IQR = Q3-Q1
IQR
# numerical_values = numerical_values[~((numerical_values < (Q1 - 1.5 * IQR)) |(numerical_values > (Q3 + 1.5 * IQR))).any(axis=1)]
# numerical_values.revenue.maxx()
48/115:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = nums_df.quantile(0.25)
q3 = nums_df.quantile(0.75)

# q3 = nums_df.quantile(0.75)
iqr = q3 - q1
iqr
# nums_df.info()
48/116:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = nums_df.quantile(0.25)
q3 = nums_df.quantile(0.75)

# q3 = nums_df.quantile(0.75)
iqr = q3 - q1
iqr
# nums_df.info()
48/117:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = nums_df.quantile(0.25)
q3 = nums_df.quantile(0.75)

# q3 = nums_df.quantile(0.75)
iqr = q3 - q1

# nums_df.info()
print(nums_df < (q1 - 1.5 * iqr) | nums_df > (q3 + 1.5 * iqr))
48/118: numerical_values.revenue.hist()
48/119:
Q1 = df.budget.quantile(0.25)
Q3 = df.budget.quantile(0.75)
IQR = Q3 - Q1
IQR
48/120:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = np.percentil(nums_df, 25)
q3 = np.percentile(nums_df, 75)

# q3 = nums_df.quantile(0.75)
iqr = q3 - q1
iqr
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/121:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = np.percentile(nums_df, 25)
q3 = np.percentile(nums_df, 75)

# q3 = nums_df.quantile(0.75)
iqr = q3 - q1
iqr
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/122:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = np.percentile(df.revenue, 25)
q3 = np.percentile(df.revenue, 75)

# q3 = nums_df.quantile(0.75)
iqr = q3 - q1
iqr
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/123:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = np.percentile(df.revenue, 25)
q3 = np.percentile(df.revenue, 75)

# q3 = nums_df.quantile(0.75)
# iqr = q3 - q1
# iqr
q3
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/124:
nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
q1 = np.percentile(df.revenue, 25)
q3 = np.percentile(df.revenue, 75)

# q3 = nums_df.quantile(0.75)
# iqr = q3 - q1
# iqr
q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/125:

numerical_values = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)


IQR = Q3-Q1
IQR
# numerical_values = numerical_values[~((numerical_values < (Q1 - 1.5 * IQR)) |(numerical_values > (Q3 + 1.5 * IQR))).any(axis=1)]
# numerical_values.revenue.maxx()
48/126:
Q1 = df.df.quantile(0.25)
Q3 = df.df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/127:
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/128:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar()
df.popularity.hist();
48/129:
# show relationship between revenue and budget
# plot_scatter(df.popularity, df.revenue)
# plt.xlabel("popularity")
# plt.ylabel("revenue")
# plt.colorbar()
df.popularity.hist();
48/130:
# show relationship between revenue and budget
# plot_scatter(df.popularity, df.revenue)
# plt.xlabel("popularity")
# plt.ylabel("revenue")
# plt.colorbar()
df.hist();
48/131:
# show relationship between revenue and budget
# plot_scatter(df.popularity, df.revenue)
# plt.xlabel("popularity")
# plt.ylabel("revenue")
# plt.colorbar()
df.hist(figsize=(8,8));
48/132:
# show relationship between revenue and budget
# plot_scatter(df.popularity, df.revenue)
# plt.xlabel("popularity")
# plt.ylabel("revenue")
# plt.colorbar()
df.hist(figsize=(10,8));
48/133:
# show relationship between revenue and budget
# plot_scatter(df.popularity, df.revenue)
# plt.xlabel("popularity")
# plt.ylabel("revenue")
# plt.colorbar()
df.hist(figsize=(10,10));
48/134:
# show relationship between revenue and budget
plot_scatter(df.vote_average, df.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
48/135: sns.boxplot(x=df.vote_average);
48/136:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar()
48/137:
# show relationship between revenue and budget
plot_scatter(df.popularity, df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
48/138:
# show relationship between budget and revenue
plot_scatter(df.budget, df.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
48/139:
Q1 = np.quantile(df, 0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/140:
Q1 = np.percentile(df, 0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/141:
Q1 = np.quantile(df, 0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/142:
Q1 = np.quantile(df, 25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/143:
Q1 = np.quantile(df, 25)
Q3 = df.quantile(75)
IQR = Q3 - Q1
IQR
48/144:
Q1 = np.percentile(df, 25)
Q3 = df.quantile(75)
IQR = Q3 - Q1
IQR
48/145:
Q1 = np.percentile(df, 25)
Q3 = np.percentile(df, 75)
IQR = Q3 - Q1
IQR
48/146:
nums = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = np.percentile(nums, 25)
Q3 = np.percentile(nums, 75)
IQR = Q3 - Q1
IQR
48/147:
nums = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = np.quantile(nums, 25)
Q3 = np.percentile(nums, 75)
IQR = Q3 - Q1
IQR
48/148:
nums = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = np.quantile(nums, 0.25)
Q3 = np.percentile(nums, 75)
IQR = Q3 - Q1
IQR
48/149:
nums = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = np.quantile(nums, 0.25)
Q3 = np.quantile(nums, 0.75)
IQR = Q3 - Q1
IQR
48/150:
nums = df[['revenue', 'popularity', 'budget', 'runtime']]
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/151:
below_iqr = df < Q1-1.5 * IQR
above_iqr = df > Q3 + 1.5 * IQR
below_iqr
# 
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
# 
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/152:
numerical_values = df[['revenue', 'popularity', 'budget', 'runtime', 'budget_adj', 'revenue_adj', 'vote_average', 'vote_count']]
below_iqr = numerical_values < Q1-1.5 * IQR
above_iqr = df > Q3 + 1.5 * IQR
below_iqr
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/153:
numerical_values = df[['revenue', 'popularity', 'budget', 'runtime', 'budget_adj', 'revenue_adj', 'vote_average', 'vote_count']]
below_iqr = numerical_values < Q1-1.5 * IQR
above_iqr = numerical_values > Q3 + 1.5 * IQR
below_iqr
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/154:
numerical_values = df[['revenue', 'popularity', 'budget', 'runtime', 'budget_adj', 'revenue_adj', 'vote_average', 'vote_count']]
below_iqr = numerical_values < Q1-1.5 * IQR
above_iqr = numerical_values > Q3 + 1.5 * IQR
below_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/155:
numerical_values = df[['revenue', 'popularity', 'budget', 'runtime', 'budget_adj', 'revenue_adj', 'vote_average', 'vote_count']]
below_iqr = numerical_values < Q1-1.5 * IQR
above_iqr = numerical_values > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/156:
df_copy= df.copy()
nums_df =  df_copy.drop('release_year', 'release_date', axis=1, inplace=True)
below_iqr = nums_df < Q1-1.5 * IQR
above_iqr = nums_df > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/157:
df_copy= df.copy()
nums_df =  df_copy.drop('release_year', 'release_date', inplace=True)
below_iqr = nums_df < Q1-1.5 * IQR
above_iqr = nums_df > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/158:
df_copy= df.copy()
df_copy
# nums_df =  df_copy.drop('release_year', 'release_date', inplace=True)
# below_iqr = nums_df < Q1-1.5 * IQR
# above_iqr = nums_df > Q3 + 1.5 * IQR
# above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/159:
df_copy= df.copy()

df_copy.drop('release_year', 'release_date', inplace=True)
below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/160:
df_copy= df.copy()

df_copy.drop(['release_year', 'release_date'], inplace=True)
below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/161:
df_copy= df.copy()

df_copy.drop('release_year', 'release_date', axis=0, inplace=True)
below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/162:
df_copy= df.copy()

df_copy.drop('release_year', 'release_date', inplace=True)
below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/163:
df_copy = df.copy()

df_copy.drop(df_copy.release_date, df_copy.release_date, inplace=True)
below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/164:
df_copy = df.copy()

df_copy.drop(df_copy.release_date, df_copy.release_date, axis=0, inplace=True)
below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/165:
df_copy = df.copy()

df_cpy = df_copy.drop(df_copy.release_date, df_copy.release_date, inplace=True)
below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/166:
df_copy = df.copy()

df_copy = df_copy.drop(df_copy.release_date, df_copy.release_date, inplace=True)
df_copy
# below_iqr = df_copy < Q1-1.5 * IQR
# above_iqr = df_copy > Q3 + 1.5 * IQR
# above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/167:
df_copy = df.copy()

df_copy = df_copy.drop('release_date', 'release_date', inplace=True)
df_copy
# below_iqr = df_copy < Q1-1.5 * IQR
# above_iqr = df_copy > Q3 + 1.5 * IQR
# above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/168:
df_copy = df.copy()

# df_copy = df_copy.drop('release_date', 'release_date', inplace=True)
df_copy
# below_iqr = df_copy < Q1-1.5 * IQR
# above_iqr = df_copy > Q3 + 1.5 * IQR
# above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/169:
df_copy = df.copy()
df_copy.drop(['release_date', 'release_date'], inplace=True)
df_copy
# below_iqr = df_copy < Q1-1.5 * IQR
# above_iqr = df_copy > Q3 + 1.5 * IQR
# above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/170:
df_copy = df.copy()
df_copy.drop(['release_date', 'release_date'], axis=1, inplace=True)
df_copy
# below_iqr = df_copy < Q1-1.5 * IQR
# above_iqr = df_copy > Q3 + 1.5 * IQR
# above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/171:
df_copy = df.copy()
df_copy.drop(['release_date', 'release_date'], axis=1, inplace=True)
below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/172:
df_copy = df.copy()
df_copy.drop(['release_date', 'release_date'], axis=1, inplace=True)
below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/173:
df_copy = df.copy()
df_copy.drop(['release_date', 'release_date'], axis=1, inplace=True)
48/174: df_copy.head(2)
48/175:
df_copy = df.copy()
df_copy.drop(['release_date','tagline', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/176: df_copy.head(2)
48/177:
df_copy = df.copy()
df_copy.drop(['release_date','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/178: df_copy.head(2)
48/179:
df_copy = df.copy()
df_copy.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/180: df_copy.head(2)
48/181:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
above_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/182:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
below_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/183:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
below_iqr
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/184:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
below_iqr.count_values()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/185:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
below_iqr.count()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/186:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
below_iqr
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/187:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
below_iqr == 'True'
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/188:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
below_iqr == True
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/189:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
below_iqr.sum()
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/190:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
print(below_iqr.sum())
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/191:

below_iqr = df_copy < Q1-1.5 * IQR
above_iqr = df_copy > Q3 + 1.5 * IQR
print(above_iqr.sum())
#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/192: df_numeric.drop([below_iqr], axis=1, inplace=True)
48/193:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/194: df_numeric.head(2)
48/195:

below_iqr = df_numeric < Q1-1.5 * IQR
above_iqr = df_numeric > Q3 + 1.5 * IQR


#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/196: df_numeric.drop([below_iqr], axis=1, inplace=True)
48/197: df_numeric.drop(below_iqr, axis=1, inplace=True)
48/198: df_numeric.describe()
48/199: df_numeri
48/200: df_numeric
48/201: df_numeric.info()
48/202:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/203: df_numeric.head(2)
48/204:

below_iqr = df_numeric < Q1-1.5 * IQR
above_iqr = df_numeric > Q3 + 1.5 * IQR


#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/205:

below_iqr = df_numeric < Q1-1.5 * IQR
above_iqr = df_numeric > Q3 + 1.5 * IQR
l = lambda x: (x < Q1-1.5 * IQR)
l


#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/206:

below_iqr = df_numeric < Q1-1.5 * IQR
above_iqr = df_numeric > Q3 + 1.5 * IQR
type(below_iqr)



#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/207:
# sns.boxplot(x=df.revenue);
sns.distplot(df['revenue']);
48/208:
# sns.boxplot(x=df.revenue);
sns.distplot(df['revenue']);
plt.show()
48/209:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(16,5))
sns.distplot(df['revenue'])
plt.show();
48/210:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(16,5))
sns.displot(df['revenue'])
plt.show();
48/211:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(16,5))
sns.displot(df['revenue'])
plt.show();
48/212:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(16,5))
sns.displot(df['popularity'])
sns.displot(df['revenue'])
plt.show();
48/213:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(16,5))
sns.displot(df['popularity'])
plt.subplot(1,2,2)
sns.displot(df['revenue'])
plt.show();
48/214:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(16,5))
sns.displot(df['revenue'])
plt.show();
48/215:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(16,5))
sns.histplot(df['revenue'])
plt.show();
48/216:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(10,5))
sns.histplot(df['revenue'])
plt.show();
48/217:
df_numeric['revenue'] = np.where(
    df['revenue']>above_iqr,
    above_iqr,
    np.where(
        df['revenue']<below_iqr,
        below_iqr,
        df['revenue']
    )
)
48/218:
df_numeric['revenue'] = np.where(
    df['revenue']>above_iqr,
    above_iqr,
    np.where(
        df_numeric['revenue']<below_iqr,
        below_iqr,
        df_numeric['revenue']
    )
)
48/219:
df_numeric['revenue'] = np.where(
    df_numeric['revenue']>above_iqr,
    above_iqr,
    np.where(
        df_numeric['revenue']<below_iqr,
        below_iqr,
        df_numeric['revenue']
    )
)
48/220:
df_numeric = ~ np.where(df_numeric > above_iqr)
df_numeric
48/221:
df_numeric =  [~ np.where(df_numeric > above_iqr)]
df_numeric
48/222:
df_numeric =  [np.where(df_numeric > above_iqr)]
df_numeric
48/223:
df_numeric =  [np.where(df_numeric > above_iqr)]
df_numeric.describe()
48/224:
df_numer = df_numeric.drop(above_iqr.index)
df_numer
48/225: df_numeric.head(2)
48/226:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/227: df_numeric.head(2)
48/228:

below_iqr = df_numeric < Q1-1.5 * IQR
above_iqr = df_numeric > Q3 + 1.5 * IQR
type(below_iqr)



#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/229:
df_numer = df_numeric.drop(above_iqr.index)
df_numer
48/230:
df_numer = df_numeric.drop(above_iqr.index)
df_numeric
48/231:
df_numer = df_numeric.drop(above_iqr.index)
df_numeric.describe()
48/232:
df_numer = df_numeric.drop(above_iqr.index, inplace=True)
df_numeric.describe()
48/233:
df_numer = df_numeric.drop(above_iqr.index, inplace=True)
df_numer.describe()
48/234:
df_numer = df_numeric.drop(above_iqr.index, inplace=True)
df_numer
48/235:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/236: df_numeric.head(2)
48/237:

below_iqr = df_numeric < Q1-1.5 * IQR
above_iqr = df_numeric > Q3 + 1.5 * IQR
type(below_iqr)



#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/238:
df_numer = df_numeric.drop(above_iqr.index, inplace=True)
df_numer
48/239:
df_numer = df_numeric.drop(above_iqr.index, inplace=True)
df_numer
48/240: df_numeric = df_numeric[~((df_numeric < below_iqr) | (df_numeric > above_iqr)).any(axis=1)]
48/241: df_numeric = df_numeric[~(below_iqr | above_iqr).any(axis=1)]
48/242: df_numeric = df_numeric[~(below_iqr | above_iqr)]
48/243: df_numeric
48/244: df_numeric.head(2)
48/245:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/246:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/247: df_numeric.head(2)
48/248:

below_iqr = df_numeric < Q1-1.5 * IQR
above_iqr = df_numeric > Q3 + 1.5 * IQR
type(below_iqr)



#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/249: df_numeric = df_numeric[~(below_iqr | above_iqr)]
48/250: df_numeric
48/251: df_numeric.describe()
48/252: df_numeric.info()
48/253: df_numeric.dropna(axis=1, inplace=True)
48/254:
df_numeric.dropna(axis=1, inplace=True)
df_numeric.describe()
48/255:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/256: df_numeric.head(2)
48/257:

below_iqr = df_numeric < Q1-1.5 * IQR
above_iqr = df_numeric > Q3 + 1.5 * IQR
type(below_iqr)



#
# nums_df = df[['revenue', 'popularity', 'budget', 'runtime']]
# q1 = np.percentile(df.revenue, 25)
# q3 = np.percentile(df.revenue, 75)
#
# # q3 = nums_df.quantile(0.75)
# # iqr = q3 - q1
# # iqr
# q1
# nums_df.info()
# print(nums_df < (q1 - 1.5 * iqr)) | nums_df > (q3 + 1.5 * iqr)))
48/258: df_numeric = df_numeric[~(below_iqr | above_iqr)]
48/259: df_numeric.describe()
48/260: df_numeric.isna().sum()
48/261: df_numeric.isnull().sum()
48/262: df_numeric.dropna().sum()
48/263: df_numeric.dropna()
48/264: df_numeric.dropna(inplace=True)
48/265: df_numeric.info()
48/266: df_numeric.head(1)
48/267: df_numeric.describe()
48/268:

q1 = df_numeric.quantile(0.25)
q3 = df_numeric.quantile(0.75)
iqr = q3 - q1
iqr
# numerical_values = numerical_values[~((numerical_values < (Q1 - 1.5 * IQR)) |(numerical_values > (Q3 + 1.5 * IQR))).any(axis=1)]
# numerical_values.revenue.maxx()
48/269: df_numeric.revenue.hist();
48/270: sns.boxplot(x=df_numeric.runtime);
48/271:
# sns.boxplot(x=df_numeric.runtime);
df_numeric.runtime.hist();
48/272:
plt.plot(df_numeric.revenue, df_numeric.popularity, label='popularity')
plt.plot(df_numeric.revenue, df_numeric.budget, label='budget')
plt.plot(df_numeric.revenue, df_numeric.runtime, label='runtime')
plt.legend()
plt.show();
48/273:
plt.plot(df_numeric.revenue, df_numeric.popularity, label='popularity')
plt.plot(df_numeric.revenue, df_numeric.budget, label='budget')
plt.plot(df_numeric.revenue, df_numeric.runtime, label='runtime')
plt.legend()
plt.show();
48/274:
plt.scatter(df_numeric.budget, df_numeric.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
48/275:
plt.plot(df_numeric.popularity, df_numeric.revenue, label='revenue')
plt.plot(df_numeric.popularity, df_numeric.budget, label='budget')
plt.plot(df_numeric.popularity, df_numeric.runtime, label='runtime')
plt.legend()
plt.show();
48/276:
plt.plot(df_numeric.budget, df_numeric.revenue, label='revenue')
plt.plot(df_numeric.budget, df_numeric.popularity, label='popularity')
plt.plot(df_numeric.budget, df_numeric.runtime, label='runtime')
plt.legend()
plt.show();
48/277:
plt.plot(df_numeric.revenue, df_numeric.budget, label='revenue')
plt.plot(df_numeric.popularity, df_numeric.budget, label='popularity')
plt.plot(df_numeric.runtime, df_numeric.budget, label='runtime')
plt.legend()
plt.show();
48/278:
plt.plot(df_numeric.runtime, df_numeric.revenue, label='revenue')
plt.plot(df_numeric.runtime, df_numeric.budget, label='budget')
plt.plot(df_numeric.runtime, df_numeric.popularity, label='popularity')
plt.legend()
plt.show();
48/279: sns.catplot(x=df_numeric.popularity, y=df_numeric.revenue, data=df_numeric);
48/280:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(8,5))
sns.histplot(df['revenue'])
plt.show();
48/281:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(8,8))
sns.histplot(df['revenue'])
plt.show();
48/282:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(5,5))
sns.histplot(df['revenue'])
plt.show();
48/283:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(10,9))
sns.histplot(df['revenue'])
plt.show();
48/284:
# sns.boxplot(x=df.revenue);
plt.figure(figsize=(8,10))
sns.histplot(df['revenue'])
plt.show();
48/285: sns.boxplot(x=df.revenue);
48/286: sns.boxplot(x=df.popularity);
48/287: sns.boxplot(x=df.runtime);
48/288: sns.boxplot(x=df.budget);
48/289:
# Use pandas describe method to see the distribution of values in every numeric column
df.describe()
48/290:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR
48/291:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/292: df_numeric.head(2)
48/293:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/294:
cols = df['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = df[cols].quantile(0.25)
q3 = df[cols].quantile(0.75)
iqr = q3 - q1
iqr
48/295:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = df[cols].quantile(0.25)
q3 = df[cols].quantile(0.75)
iqr = q3 - q1
iqr
48/296:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = df[cols].quantile(0.25)
q3 = df[cols].quantile(0.75)
iqr = q3 - q1

df = df[~((df[cols] < (Q1-1.5 * IQR)) | (df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]
df
48/297:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = df[cols].quantile(0.25)
q3 = df[cols].quantile(0.75)
iqr = q3 - q1

df = df[~((df[cols] < (Q1-1.5 * IQR)) | (df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]
df.describe()
48/298:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = df[cols].quantile(0.25)
q3 = df[cols].quantile(0.75)
iqr = q3 - q1

df = df.query(~((df[cols] < (Q1-1.5 * IQR)) | (df[cols] > (Q3 + 1.5 * IQR))).any(axis=1))
df.describe()
48/299:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = df[cols].quantile(0.25)
q3 = df[cols].quantile(0.75)
iqr = q3 - q1

df = df.query('~(%s(df[cols] < (Q1-1.5 * IQR)) | (df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)')
df.describe()
48/300:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = df.query('cols').quantile(0.25)
q3 = df[cols].quantile(0.75)
iqr = q3 - q1
iqr
# 
# df = df.query('~(%s(df[cols] < (Q1-1.5 * IQR)) | (df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)')
# df.describe()
48/301:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = df.query('revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average').quantile(0.25)
q3 = df[cols].quantile(0.75)
iqr = q3 - q1
iqr
#
# df = df.query('~(%s(df[cols] < (Q1-1.5 * IQR)) | (df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)')
# df.describe()
48/302:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = np.percentile(df[cols], 0.25)
q3 = df[cols].quantile(0.75)
iqr = q3 - q1

df = df[~((df[cols] < (Q1-1.5 * IQR)) | (df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]
df.describe()
48/303:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = np.percentile(df[cols], 0.25)
q3 = np.percentile(df[cols], 0.75)
iqr = q3 - q1

df = df[~((df[cols] < (Q1-1.5 * IQR)) | (df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]
df.describe()
48/304:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = np.percentile(df[cols], 0.25)
q3 = np.percentile(df[cols], 0.75)
iqr = q3 - q1

below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)

df = df[~((df[cols] < below_iqr) | (df[cols] > above_iqr))]
df.describe()
48/305:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = np.percentile(df[cols], 0.25)
q3 = np.percentile(df[cols], 0.75)
iqr = q3 - q1

below_iqr = df[cols] < (Q1-1.5 * IQR)
above_iqr = df[cols] > (Q3 + 1.5 * IQR)

df = df[~((df[cols] < below_iqr) | (df[cols] > above_iqr))]
df.describe()
48/306:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = np.percentile(df[cols], 0.25)
q3 = np.percentile(df[cols], 0.75)
iqr = q3 - q1

below_iqr = df[cols] < (q1-1.5 * iqr)
above_iqr = df[cols] > (q3 + 1.5 * iqr)

df = df[~((df[cols] < below_iqr) | (df[cols] > above_iqr))]
df.describe()
48/307:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = np.percentile(df[cols], 0.25)
q3 = np.percentile(df[cols], 0.75)
iqr = q3 - q1

below_iqr = df < (q1-1.5 * iqr)
above_iqr = df > (q3 + 1.5 * iqr)

df = df[~((df[cols] < below_iqr) | (df[cols] > above_iqr))]
df.describe()
48/308:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = np.percentile(df[cols], 0.25)
q3 = np.percentile(df[cols], 0.75)
iqr = q3 - q1

below_iqr = df[cols] < (q1-1.5 * iqr)
above_iqr = df[cols] > (q3 + 1.5 * iqr)

df = df[~(below_iqr | above_iqr)]
df.describe()
48/309:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
48/310:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
48/311:
# Use pandas describe method to see the distribution of values in every numeric column
df.describe()
48/312:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
48/313:
# Loading TMDB Movies Data
def load_data():
    data = pd.read_csv('tmdb-movies.csv')
    return data
48/314:
# Preview the first five rows of the Dataset
df = load_data()
df.head()
48/315:
# Preview the last five rows of the Dataset
df.tail()
48/316:
# View how many columns and rows are in the dataset.
df.info()
48/317: df.columns
48/318:
# View the data types of the columns of the Dataset
df.dtypes
48/319:
# View number of rows with duplicate values
df.duplicated().sum()
48/320: df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
48/321:
# check if the column has been renamed
df.columns
48/322: df.dropna(inplace=True)
48/323:
# check if there are missing values
df.isnull().sum()
48/324: df.drop_duplicates(inplace=True)
48/325:
# check if there are duplicates
df.duplicated().sum()
48/326: df.drop(['id', 'imdb_id', 'homepage', 'overview'], axis =1, inplace=True)
48/327:
# Check if the columns have been dropped
df.head(1)
48/328: df['release_date'] = pd.to_datetime(df.release_date)
48/329:
# check release_date datatype
df.dtypes
48/330: df['release_year'] = pd.to_datetime(df.release_year)
48/331:
# check release_year datatype
df.dtypes
48/332:
# Save cleaned data in another csv file
df.to_csv('tmdb_movies_cleaned.csv', index=False)
48/333:
data = pd.read_csv('tmdb-movies_cleaned.csv')
data.head()
48/334:
# Save cleaned data in another csv file
df.to_csv('tmdb_movies_cleaned.csv', index=False)
48/335:
# Save cleaned data in another csv file
df.to_csv('C:/Users/Carla/Workspace/Udacity/ALX/projects/project_1/tmdb_movies_cleaned.csv', index=False)
48/336:
data = pd.read_csv('tmdb-movies_cleaned.csv')
data.head()
48/337:
data = pd.read_csv('tmdb-movies_cleaned.csv')
data.head()
48/338:
data = pd.read_csv('tmdb-movies_cleaned.csv')
data.head()
48/339:
# Save cleaned data in another csv file
df.to_csv('C:/Users/Carla/Workspace/Udacity/ALX/projects/project_1/tmdb_movies_cleaned.csv', index=False)
48/340:
data = pd.read_csv('tmdb-movies_cleaned.csv')
data.head()
48/341:
data = pd.read_csv('C:/Users/Carla/Workspace/Udacity/ALX/projects/project_1/tmdb_movies_cleaned.csv')
data.head()
48/342:
# Save cleaned data in another csv file
df.to_csv('tmdb_movies_clean.csv', index=False)
48/343:
data = pd.read_csv('tmdb_movies_cleaned.csv')
data.head()
48/344:
df_cleaned = pd.read_csv('tmdb_movies_cleaned.csv')
df_cleaned.head()
48/345:
# A method to create scatter plots
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df_cleaned.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
48/346:
# show relationship between popularity and revenue
plot_scatter(df_cleaned.popularity, df_cleaned.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
48/347:
# show relationship between budget and revenue
plot_scatter(df_cleaned.budget, df_cleaned.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
48/348:
# show relationship between runtime and revenue
plot_scatter(df_cleaned.runtime, df_cleaned.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
48/349: sns.boxplot(x=df_cleaned.revenue);
48/350: sns.boxplot(x=df_cleaned.popularity);
48/351: sns.boxplot(x=df_cleaned.runtime);
48/352: sns.boxplot(x=df_cleaned.budget);
48/353:
# Use pandas describe method to see the distribution of values in every numeric column
df_cleaned.describe()
48/354:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df_cleaned.quantile(0.25)
Q3 = df_cleaned.quantile(0.75)
IQR = Q3 - Q1
IQR
48/355:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = np.percentile(df_cleaned[cols], 0.25)
q3 = np.percentile(df_cleaned[cols], 0.75)
iqr = q3 - q1

below_iqr = df_cleaned[cols] < (q1-1.5 * iqr)
above_iqr = df_cleaned[cols] > (q3 + 1.5 * iqr)

df_cleaned = df_cleaned[~(below_iqr | above_iqr)]
df_cleaned.describe()
48/356:
cols = ['revenue', 'budget', 'runtime', 'revenue_adj', 'budget_adj', 'popularity', 'vote_count', 'vote_average']
q1 = np.percentile(df_cleaned[cols], 0.25)
q3 = np.percentile(df_cleaned[cols], 0.75)
iqr = q3 - q1

below_iqr = df_cleaned[cols] < (q1-1.5 * iqr)
above_iqr = df_cleaned[cols] > (q3 + 1.5 * iqr)

df_cleaned = df_cleaned[~(below_iqr | above_iqr)]
df_cleaned.info()
48/357:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df_cleaned.quantile(0.25)
Q3 = df_cleaned.quantile(0.75)
IQR = Q3 - Q1
IQR
48/358:
df_cleaned = pd.read_csv('tmdb_movies_cleaned.csv')
df_cleaned.head()
48/359:
# Use pandas describe method to see the distribution of values in every numeric column
df_cleaned.describe()
48/360:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df_cleaned.quantile(0.25)
Q3 = df_cleaned.quantile(0.75)
IQR = Q3 - Q1
IQR
48/361:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/362:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df_cleaned.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/363: df_numeric.head(2)
48/364:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/365:
# Trim out the outliers by using a
df_numeric = df_numeric[~(below_iqr | above_iqr)]
48/366:
# View the distribution after removing outliers
df_numeric.describe()
48/367:
# View the structure of the dataset after trimming out outliers
df_numeric.info()
48/368:
# View the distribution after removing outliers
df_numeric.describe()
48/369: df_numeric.isnull()
48/370: df_numeric.isnull().sum()
48/371:
# Drop missing values
df_numeric.dropna(inplace=True)
48/372:
# check if dropping was successful
df_numeric.isnull().sum()
48/373: df_numeric.info()
48/374:
# View the distribution after removing outliers
df_numeric.describe()
48/375:
# df_numeric.revenue.hist();
plot_scatter(df_numeric.popularity, df_numeric.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
48/376:
def plots(x_value, y_value):
    colors = np.array([range(0, len(df_numeric.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
48/377:
# df_numeric.revenue.hist();
plots(df_numeric.popularity, df_numeric.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
48/378: df_numeric.hist(figsize=(8,8))
48/379: df_numeric.hist(figsize=(10,10));
48/380: pd.plotting.scatter_matrix(df_numeric, figsize = (15,15));
48/381:
sns.boxplot(x=df_numeric.runtime);
# df_numeric.runtime.hist();
48/382:
plots(df_numeric.runtime, df_numeric.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
48/383:
plt.plot(df_numeric.revenue, df_numeric.popularity, label='popularity')
plt.plot(df_numeric.revenue, df_numeric.budget, label='budget')

plt.legend()
plt.show();
48/384:
plt.plot(df_numeric.revenue, df_numeric.popularity, label='popularity')


plt.legend()
plt.show();
48/385:
plots(df_numeric.budget, df_numeric.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
48/386:
plots(df_cleaned.release_year, df_numeric.revenue)
plt.xlabel("release_year")
plt.ylabel("revenue")
plt.colorbar();
48/387:
plt.plot(df_cleaned.popularity, df_cleaned.revenue, label='revenue')
plt.plot(df_cleaned.popularity, df_cleaned.budget, label='budget')
plt.plot(df_cleaned.popularity, df_cleaned.runtime, label='runtime')
plt.legend()
plt.show();
48/388:
plt.plot(df_cleaned.release_year, df_cleaned.revenue, label='revenue')
plt.plot(df_cleaned.popularity, df_cleaned.budget, label='budget')
plt.plot(df_cleaned.popularity, df_cleaned.runtime, label='runtime')
plt.legend()
plt.show();
48/389:
plt.plot(df_cleaned.release_year, df_cleaned.revenue, label='year')
plt.plot(df_cleaned.popularity, df_cleaned.budget, label='budget')
plt.plot(df_cleaned.popularity, df_cleaned.runtime, label='runtime')
plt.legend()
plt.show();
48/390:
plt.plot(df_numeric.popularity, df_numeric.revenue, label='popularity')
plt.legend()
plt.show();
48/391:
plt.plot(df_cleaned.release_year, df_numeric.revenue, label='popularity')
plt.legend()
plt.show();
48/392:
plt.plot(df_cleaned.release_year, df_cleaned.revenue, label='popularity')
plt.legend()
plt.show();
48/393:
plt.plot(df_cleaned.revenue, df_cleaned.release_year, label='popularity')
plt.legend()
plt.show();
48/394:
plt.plot(df_cleaned.revenue, df_cleaned.director, label='popularity')
plt.legend()
plt.show();
48/395:
plt.plot(df_cleaned.director, df_cleaned.revenue, label='popularity')
plt.legend()
plt.show();
48/396: df_numeric.describe()
48/397:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [0.00, 0.00, 8.936663e+06, 6.154871e+07, 3.173750e+08] # Fill in this list with five values you just found
48/398: bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
48/399: df_numeric['popularity'] = pd.cut(df_numeric['revenue'], bin_edges, labels=bin_names)
48/400:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [0.000, 0.00, 8.936663e+06, 6.154871e+07, 3.173750e+08] # Fill in this list with five values you just found
48/401: bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
48/402: df_numeric['popularity'] = pd.cut(df_numeric['revenue'], bin_edges, labels=bin_names)
48/403:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [0.00062, 0.345, 0.604, 1.083, 3.260] # Fill in this list with five values you just found
48/404: bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
48/405: df_numeric['popularity'] = pd.cut(df_numeric['revenue'], bin_edges, labels=bin_names)
48/406:
df_numeric['popularity'] = pd.cut(df_numeric['revenue'], bin_edges, labels=bin_names)
df_numeric.head()
48/407:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [0.00062, 0.345, 0.604, 1.083, 3.260] # Fill in this list with five values you just found
48/408: bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
48/409:
df_numeric['revenue'] = pd.cut(df_numeric['popularity'], bin_edges, labels=bin_names)
df_numeric.head()
48/410:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df_cleaned.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/411:
# View the first rows to confirm that the non-numeric values have been dropped.
df_numeric.head(2)
48/412:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/413:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/414:
# View the structure of the dataset after trimming out outliers
df_numeric.info()
48/415:
# Check for missing values
df_numeric.isnull().sum()
48/416:
# Drop missing values
df_numeric.dropna(inplace=True)
48/417:
# check if dropping was successful
df_numeric.isnull().sum()
48/418:
# Confirm that there are no more null values
df_numeric.info()
48/419:
## function to create plots
def plots(x_value, y_value):
    colors = np.array([range(0, len(df_numeric.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
48/420:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [0.00062, 0.345, 0.604, 1.083, 3.260] # Fill in this list with five values you just found
48/421: bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
48/422: df_numeric.head()
48/423: df_numeric['revenue'] = pd.cut(df_numeric['popularity'], bin_edges, labels=bin_names)
48/424: df_numeric.head()
48/425: df_numeric['revenue_ranking'] = pd.cut(df_numeric['popularity'], bin_edges, labels=bin_names)
48/426: df_numeric.head()
48/427:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df_cleaned.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/428:
# View the first rows to confirm that the non-numeric values have been dropped.
df_numeric.head(2)
48/429:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/430:
# Trim out the outliers by using a
df_numeric = df_numeric[~(below_iqr | above_iqr)]
48/431:
# View the structure of the dataset after trimming out outliers
df_numeric.info()
48/432:
# Check for missing values
df_numeric.isnull().sum()
48/433:
# Drop missing values
df_numeric.dropna(inplace=True)
48/434:
# check if dropping was successful
df_numeric.isnull().sum()
48/435:
## function to create plots
def plots(x_value, y_value):
    colors = np.array([range(0, len(df_numeric.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
48/436:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [0.00062, 0.345, 0.604, 1.083, 3.260] # Fill in this list with five values you just found
48/437: bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
48/438: df_numeric['revenue_ranking'] = pd.cut(df_numeric['popularity'], bin_edges, labels=bin_names)
48/439: df_numeric.head()
48/440: df_numeric.groupby('revenue_ranking').mean()
48/441: df_numeric.groupby(['revenue_ranking']).mean()
48/442: df_numeric['revenue_ranking'] = df_numeric['popularity_ranking']
48/443: df_numeric.rename(columns={'revenue_ranking': 'popularity_ranking'})
48/444: df_numeric.rename(columns={'revenue_ranking': 'popularity_ranking'}, inplace=True)
48/445: df_numeric.head()
48/446: df_numeric.groupby(['popularity_ranking']).mean()
48/447: df_numeric.popularity_ranking.describe()
48/448: df_numeric.groupby('popularity_ranking').describe()
48/449: df_numeric.groupby('popularity_ranking').revenue.mean()
48/450: plt.plot(means);
48/451: means = df_numeric.groupby('popularity_ranking').revenue.mean()
48/452: plt.plot(means);
48/453:
plt.plot(means);
means.hist(figsize=(8,8))
48/454:
# plt.plot(means);
means.hist(figsize=(8,8));
48/455:
# plt.plot(means);
low = means['Low']
medium = means['Medium']
medium_high = means['Medium High']
high = means['High']
plt.bar([4,3,2,1], [low, medium, medium_high, high]);
48/456:
# plt.plot(means);
low = means['Low']
medium = means['Medium']
medium_high = means['Medium High']
high = means['High']
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
# labels = ['Low', 'Medium', 'Medium High', 'High']
labels = means.index.str.replace('_', ' ').str.title()
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Quality Ratings by Acidity Levels')
plt.xlabel('Acidity Levels')
plt.ylabel('Average Quality Rating');

# plt.bar([4,3,2,1], [low, medium, medium_high, high]);
48/457:
# plt.plot(means);
low = means['Low']
medium = means['Medium']
medium_high = means['Medium High']
high = means['High']
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
# labels = ['Low', 'Medium', 'Medium High', 'High']
labels = means.index.str.replace('_', ' ').str.title()
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Revenue by Popularity Ranking')
plt.xlabel('Popularity Ranking')
plt.ylabel('Average Revenue');

# plt.bar([4,3,2,1], [low, medium, medium_high, high]);
48/458:
means = df_numeric.groupby('popularity_ranking').revenue.mean()
mean
48/459:
means = df_numeric.groupby('popularity_ranking').revenue.mean()
means
48/460: df_numeric.revenue.hist(figsize=(8,8))
48/461: df_numeric.revenue.hist(figsize=(8,8));
48/462: df_numeric.query('revenue > Q3')
48/463:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df_cleaned.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/464: df_numeric.head()
48/465: df_numeric.head(1)
48/466: df_numeric.describe()
48/467:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df_cleaned.quantile(0.25)
Q3 = df_cleaned.quantile(0.75)
IQR = Q3 - Q1
IQR
48/468:
# Find the outliers above and below the iqr
below_iqr = df_cleaned < (Q1-1.5 * IQR)
above_iqr = df_cleaned > (Q3 + 1.5 * IQR)
48/469:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/470:
# Find the outliers above and below the iqr
below_iqr = df_numeric['revenue', 'budget', 'budget_adj', 'runtime', 'popularity', 'revenue_adj'] < (Q1-1.5 * IQR)
above_iqr = df_numeric['revenue', 'budget', 'budget_adj', 'runtime', 'popularity', 'revenue_adj'] > (Q3 + 1.5 * IQR)
48/471:
# Find the outliers above and below the iqr
below_iqr = df_numeric[['revenue', 'budget', 'budget_adj', 'runtime', 'popularity', 'revenue_adj']] < (Q1-1.5 * IQR)
above_iqr = df_numeric[['revenue', 'budget', 'budget_adj', 'runtime', 'popularity', 'revenue_adj']] > (Q3 + 1.5 * IQR)
48/472:
# Find the outliers above and below the iqr
below_iqr = df_numeric.loc['revenue', 'budget', 'budget_adj', 'runtime', 'popularity', 'revenue_adj'] < (Q1-1.5 * IQR)
above_iqr = df_numeric[['revenue', 'budget', 'budget_adj', 'runtime', 'popularity', 'revenue_adj']] > (Q3 + 1.5 * IQR)
48/473:
# Find the outliers above and below the iqr
below_iqr = df_numeric.loc['revenue', 'budget', 'budget_adj', 'runtime', 'popularity', 'revenue_adj'] < (Q1-1.5 * IQR)
above_iqr = df_numeric.loc['revenue', 'budget', 'budget_adj', 'runtime', 'popularity', 'revenue_adj'] > (Q3 + 1.5 * IQR)
48/474:
# Find the outliers above and below the iqr
below_iqr = df_numeric[df_numeric['revenue', 'budget', 'budget_adj', 'runtime', 'popularity', 'revenue_adj']]< (Q1-1.5 * IQR)
above_iqr = df_numeric[df['revenue', 'budget', 'budget_adj', 'runtime', 'popularity', 'revenue_adj'] ]> (Q3 + 1.5 * IQR)
48/475:
# Find the outliers above and below the iqr
rev_iqr = df_numeric.revenue < (Q1-1.5 * IQR)
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/476:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/477:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1
IQR
48/478:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/479:
# Trim out the outliers by using a
df_numeric = df_numeric[~(below_iqr | above_iqr)]
48/480:
# View the structure of the dataset after trimming out outliers
df_numeric.info()
48/481: pd.merge(df_numeric, df_cleaned, how='outer', on='revenue')
48/482: pd.merge(df_numeric, df_cleaned, how='inner', on='revenue')
48/483: pd.merge(df_numeric, df_cleaned, how='right', on='revenue')
48/484: pd.merge(df_numeric, df_cleaned, how='right')
48/485: df_numeric.info()
48/486: df_numeric = pd.merge(df_numeric, df_cleaned, how='right')
48/487: df_numeric.info()
48/488: df_numeric.describe()
48/489:
# Check for missing values
df_numeric.isnull().sum()
48/490: df_numeric.describe()
48/491:
# Check for missing values
df_numeric.isnull().sum()
48/492:
# Confirm that there are no more null values
df_numeric.info()
48/493:
# View the distribution stats after removing outliers
df_numeric.describe()
48/494: pd.merge(df_numeric, df_cleaned, how='right', inplace=True)
48/495: pd.merge(df_numeric, df_cleaned, how='left')
48/496: df_numeric.describe()
48/497:
# Trim out the outliers by using a
df_numeric = df_cleaned[~(below_iqr | above_iqr)]
48/498:
# View the structure of the dataset after trimming out outliers
df_numeric.info()
48/499:
# View the structure of the dataset after trimming out outliers
df_numeric.describe()
48/500:
# View the structure of the dataset after trimming out outliers
df_numeric.isnull()
48/501:
# View the structure of the dataset after trimming out outliers
df_numeric.isnull().sum()
48/502:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df_cleaned.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/503:
# View the first rows to confirm that the non-numeric values have been dropped.
df_numeric.head(2)
48/504:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df_cleaned.quantile(0.25)
Q3 = df_cleaned.quantile(0.75)
IQR = Q3 - Q1
IQR
48/505:
# Trim out the outliers by using a
df_cleaned = df_cleaned[~(below_iqr | above_iqr)]
48/506:
# View the structure of the dataset after trimming out outliers
df_numeric.isnull().sum()
48/507:
# Find the outliers above and below the iqr
below_iqr = df_cleaned < (Q1-1.5 * IQR)
above_iqr = df_cleaned > (Q3 + 1.5 * IQR)
48/508:
# Trim out the outliers by using a
df_cleaned = df_cleaned[~(below_iqr | above_iqr)]
48/509:
# View the structure of the dataset after trimming out outliers
df_numeric.isnull().sum()
48/510:
# Find the outliers above and below the iqr
below_iqr = np.array(df_cleaned < (Q1-1.5 * IQR))
above_iqr = np.array(df_cleaned > (Q3 + 1.5 * IQR))
48/511:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df_cleaned.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/512: df_numeric.describe()
48/513:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df_cleaned.quantile(0.25)
Q3 = df_cleaned.quantile(0.75)
IQR = Q3 - Q1
IQR
48/514:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1
IQR
48/515:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/516:
# Trim out the outliers by using a
df_cleaned = df_cleaned[~(below_iqr | above_iqr)]
48/517:
# View the structure of the dataset after trimming out outliers
df_numeric.isnull().sum()
48/518: pd.concat([df_numeric, df_cleaned])
48/519: df_numeric.info()
48/520: df_alt = df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime'])
48/521: df_alt = df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime'], axis=1, inplace=True)
48/522: df_alt.head()
48/523: df_alt
48/524: df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime'], axis=1, inplace=True)
48/525:
df_cleaned = pd.read_csv('tmdb_movies_cleaned.csv')
df_cleaned.head()
48/526:
# A method to create scatter plots
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df_cleaned.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
48/527:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df_cleaned.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/528:
# View the first rows to confirm that the non-numeric values have been dropped.
df_numeric.head(2)
48/529: df_numeric.describe()
48/530:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1
IQR
48/531:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/532:
# Trim out the outliers by using a
df_cleaned = df_cleaned[~(below_iqr | above_iqr)]
48/533:
df_cleaned = pd.read_csv('tmdb_movies_cleaned.csv')
df_cleaned.head()
48/534:
# A method to create scatter plots
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df_cleaned.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
48/535:
# Create a copy of the dataset and drop all non-numeric values so that we can perform mathematical operations
df_numeric = df_cleaned.copy()
df_numeric.drop(['release_year','tagline', 'genres', 'keywords', 'director', 'cast', 'original_title', 'prod_companies', 'release_date'], axis=1, inplace=True)
48/536:
# View the first rows to confirm that the non-numeric values have been dropped.
df_numeric.head(2)
48/537:
# Will use pandas to find the iqr of all features excluding non-numeric (the datetime values)
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1
IQR
48/538:
# Find the outliers above and below the iqr
below_iqr = df_numeric < (Q1-1.5 * IQR)
above_iqr = df_numeric > (Q3 + 1.5 * IQR)
48/539:
# Trim out the outliers by using a
df_numeric = df_numeric[~(below_iqr | above_iqr)]
48/540:
# View the structure of the dataset after trimming out outliers
df_numeric.isnull().sum()
48/541: df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime'], axis=1, inplace=True)
48/542: df_cleaned.info()
48/543: df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime', 'vote_count', 'vote_average'], axis=1, inplace=True)
48/544:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
48/545:
def load_any(file):
    return file
48/546:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
48/547:
# Preview the first five rows of the Dataset

df.head()
48/548:
df_cleaned = load_any(pd.read_csv('tmdb_movies_cleaned.csv'))
df_cleaned.head()
48/549:
def load_clean_data():
    clean_data = load_any(pd.read_csv('tmdb_movies_cleaned.csv'))
    return clean_data
48/550:
df_cleaned = load_clean_data()
df_cleaned.head()
48/551: df_cleaned = load_clean_data()
48/552: df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime', 'vote_count', 'vote_average'], axis=1, inplace=True)
48/553: df_cleaned.info()
48/554: df_numeric.info()
48/555: pd.concat([df_numeric, df_cleaned])
48/556: df_numeric.info()
48/557: df_cleaned = load_clean_data()
48/558: df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime', 'vote_count', 'vote_average'], axis=1, inplace=True)
48/559: df_cleaned.info()
48/560: df_numeric.info()
48/561: pd.concat([df_numeric, df_cleaned], axis='columns')
48/562: df_numeric.info()
48/563: df_cleaned.info()
48/564:
conc = pd.concat([df_numeric, df_cleaned], axis='columns')
type(conc)
48/565: conc.info()
48/566: concatenated_df = pd.concat([df_numeric, df_cleaned], axis='columns')
48/567: concatenated_df.info()
48/568: concatenated_df.head()
48/569: concatenated_df.isnull().sum()
48/570: concatenated_df.dropna(inplace=True)
48/571: concatenated_df.isnull()
48/572: concatenated_df.isnull().sum()
48/573: concatenated_df.describe()
48/574: concatenated_df.revenue().describe()
48/575: concatenated_df.revenue.describe()
48/576: concatenated_df.hist()
48/577: concatenated_df.hist(figsize=(8,8));
48/578:
# load data so here so that I don't have to go back and run past cells
df_cleaned = load_clean_data()
48/579:
# make sure it is loaded correctly
df_cleaned.head(2)
48/580:
# drop the numerical values
df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime', 'vote_count', 'vote_average'], axis=1, inplace=True)
48/581:
# confirm that they have been dropped
df_cleaned.head(1)
48/582:
# concatenate dfs
concatenated_df = pd.concat([df_numeric, df_cleaned], axis='columns')
48/583: concatenated_df.info()
48/584: len(df_numeric)
48/585: df_numeric.columns
48/586: df_numeric.columns, df_cleaned.columns
48/587:
# check for null values
concatenated_df.isnull().sum()
48/588: concatenated_df.dropna(inplace=True)
48/589:
# confirming dropping
concatenated_df.isnull().sum()
48/590: concatenated_df.describe()
48/591:
# check structure of the concatenated dataframe
concatenated_df.info()
48/592:
# View the distribution stats after removing outliers
concatenated_df.describe()
48/593:
## function to create plots
def plots(x_value, y_value):
    colors = np.array([range(0, len(concatenated_df.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
48/594:
# Revenue against popularity
plots(concatenated_df.popularity, concatenated_df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
48/595: concatenated_df.hist(fisize=(10,10));
48/596: concatenated_df.hist(figsize=(10,10));
48/597:
# Revenue against runtime
plots(df_numeric.runtime, df_numeric.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
48/598:
# Revenue against runtime
plots(concatenated_df.runtime, concatenated_df.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
48/599:
# Revenue against budget
plots(concatenated_df.budget, concatenated_df.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
48/600: concatenated_df.describe()
48/601:
# Revenue against popularity
plots(concatenated_df.revenue, concatenated_df.popularity)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
48/602:
# Revenue against popularity
plots(concatenated_df.revenue, concatenated_df.popularity)
plt.xlabel("revenue")
plt.ylabel("popularity")
plt.colorbar();
48/603:
# Revenue against popularity
plots(concatenated_df.popularity, concatenated_df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
48/604:
# Revenue against runtime
plots(concatenated_df.runtime, concatenated_df.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
48/605: concatenated_df.describe()
48/606:
# Bin edges that will be used to "cut" the data into groups
bin_edges = [0.00062, 0.345, 0.604, 1.083, 3.260]
48/607: bin_names = ['High','Medium High','Medium', 'Low' ] # Name each acidity level category
48/608: concatenated_df['popularity_ranking'] = pd.cut(concatenated_df['popularity'], bin_edges, labels=bin_names)
48/609: df_numeric.head()
48/610: concatenated_df.head()
48/611: df_numeric.groupby(['popularity_ranking']).mean()
48/612: concatenated_df.groupby(['popularity_ranking']).mean()
48/613:
means = concatenated_df.groupby('popularity_ranking').revenue.mean()
means
48/614:
# plt.plot(means);
low = means['Low']
medium = means['Medium']
medium_high = means['Medium High']
high = means['High']
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
# labels = ['Low', 'Medium', 'Medium High', 'High']
labels = means.index.str.replace('_', ' ').str.title()
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Revenue by Popularity Ranking')
plt.xlabel('Popularity Ranking')
plt.ylabel('Average Revenue');

# plt.bar([4,3,2,1], [low, medium, medium_high, high]);
48/615: plt.plot(means);
48/616: plt.plot(means, linewidth='20');
48/617: plt.plot(means, linewidth='10');
48/618: plt.plot(means, linewidth='7');
48/619: plt.plot(means, c='r');
48/620:
revenue_mean = concatenated_df.groupby('popularity_ranking').revenue.mean()
revenue_mean
48/621:
# plt.plot(means);
low = revenue_mean['Low']
medium = revenue_mean['Medium']
medium_high = revenue_mean['Medium High']
high = revenue_mean['High']
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
# labels = ['Low', 'Medium', 'Medium High', 'High']
labels = revenue_mean.index.str.replace('_', ' ').str.title()
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Revenue by Popularity Ranking')
plt.xlabel('Popularity Ranking')
plt.ylabel('Average Revenue');

# plt.bar([4,3,2,1], [low, medium, medium_high, high]);
48/622:
runtime_mean = concatenated_df.groupby('popularity_ranking').runtime.mean()
runtime_mean
48/623: plt.plot(runtime_mean);
48/624:
revenue_mean = concatenated_df.groupby('popularity_ranking').revenue.mean()
runtime_mean = concatenated_df.groupby('popularity_ranking').runtime.mean()
budget_mean = concatenated_df.groupby('popularity_ranking').budget.mean()
48/625:
plt.plot(revenue_mean, c='r')
plt.plot(runtime_mean)
plt.show()
48/626:
plt.plot(revenue_mean, c='r')
plt.plot(runtime_mean)
plt.plot(budget_mean)
plt.show();
48/627:
plt.plot(revenue_mean, c='r')
plt.plot(runtime_mean)
plt.plot(budget_mean)
plt.legend()
plt.show();
48/628:
plt.plot(revenue_mean, c='r', label='revenue')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean)
plt.legend()
plt.show();
48/629:
plt.plot(revenue_mean, c='r', label='revenue')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean, label='budget')
plt.legend()
plt.show();
48/630:
# plt.plot(means);
low = revenue_mean['Low']
medium = revenue_mean['Medium']
medium_high = revenue_mean['Medium High']
high = revenue_mean['High']
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
# labels = ['Low', 'Medium', 'Medium High', 'High']
labels = budget_mean.index.str.replace('_', ' ').str.title()
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Revenue by Popularity Ranking')
plt.xlabel('Popularity Ranking')
plt.ylabel('Average Revenue');

# plt.bar([4,3,2,1], [low, medium, medium_high, high]);
48/631:
# plt.plot(means);
low = budget_mean['Low']
medium = budget_mean['Medium']
medium_high = budget_mean['Medium High']
high = budget_mean['High']
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
# labels = ['Low', 'Medium', 'Medium High', 'High']
labels = budget_mean.index.str.replace('_', ' ').str.title()
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Revenue by Popularity Ranking')
plt.xlabel('Popularity Ranking')
plt.ylabel('Average Revenue');

# plt.bar([4,3,2,1], [low, medium, medium_high, high]);
48/632:
# # plt.plot(means);
# low = budget_mean['Low']
# medium = budget_mean['Medium']
# medium_high = budget_mean['Medium High']
# high = budget_mean['High']
# locations = [4,3,2,1]
# heights = [low, medium, medium_high, high]
# # labels = ['Low', 'Medium', 'Medium High', 'High']
# labels = budget_mean.index.str.replace('_', ' ').str.title()
# plt.bar(locations, heights, tick_label=labels)
# plt.title('Average Revenue by Popularity Ranking')
# plt.xlabel('Popularity Ranking')
# plt.ylabel('Average Revenue');
# 
# # plt.bar([4,3,2,1], [low, medium, medium_high, high]);
48/633:
plt.plot(revenue_mean, label='revenue')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean, label='budget')
plt.legend()
plt.show();
48/634:
plt.plot(concatenated_df.revenue, label='revenue')
plt.legend()
plt.show()
48/635:
plt.plot(concatenated_df.revenue.mean(), label='revenue')
plt.legend()
plt.show()
48/636:
rev_mean = concatenated_df.revenue.mean()
plt.plot(rev_mean, label='revenue')
plt.legend()
plt.show()
48/637:
rev_mean = concatenated_df.revenue.mean()
plt.plot(rev_mean, concatenated_df.popularity.mean(), label='revenue')
plt.legend()
plt.show()
48/638:
rev_mean = concatenated_df.revenue.mean()
plt.plot(rev_mean, concatenated_df.popularity.mean(), label='revenue')
plt.legend()
plt.show();
48/639:
rev_mean = concatenated_df.revenue.mean()
plt.plot(concatenated_df.revenue, concatenated_df.popularity, label='revenue')
plt.legend()
plt.show();
48/640:
rev_mean = concatenated_df.revenue.mean()
plt.plot(concatenated_df.popularity, label='revenue')
plt.legend()
plt.show();
48/641:
rev_mean = concatenated_df.revenue.mean()
plt.plot(concatenated_df, label='revenue')
plt.legend()
plt.show();
48/642:

plt.plot(concatenated_df.count_values(), label='revenue')
plt.legend()
plt.show();
48/643:

plt.plot(concatenated_df.value_counts(), label='revenue')
plt.legend()
plt.show();
48/644:

# plt.plot(concatenated_df.value_counts(), label='revenue')
# plt.legend()
# plt.show();
concatenated_df.director.value_counts()
48/645:

# plt.plot(concatenated_df.value_counts(), label='revenue')
# plt.legend()
# plt.show();
concatenated_df.groupby('original_title').director.value_counts()
48/646:

# plt.plot(concatenated_df.value_counts(), label='revenue')
# plt.legend()
# plt.show();
concatenated_df.groupby('director').director.value_counts()
48/647:

# plt.plot(concatenated_df.value_counts(), label='revenue')
# plt.legend()
# plt.show();
concatenated_df.groupby('director').original_title.value_counts()
48/648:

# plt.plot(concatenated_df.value_counts(), label='revenue')
# plt.legend()
# plt.show();
concatenated_df.groupby('director').value_counts()
48/649:

# plt.plot(concatenated_df.value_counts(), label='revenue')
# plt.legend()
# plt.show();
concatenated_df.groupby('director')director.value_counts()
48/650:

# plt.plot(concatenated_df.value_counts(), label='revenue')
# plt.legend()
# plt.show();
concatenated_df.groupby('director').director.value_counts()
48/651:

# plt.plot(concatenated_df.value_counts(), label='revenue')
# plt.legend()
# plt.show();
concatenated_df.director.value_counts()
48/652:

# plt.plot(concatenated_df.value_counts(), label='revenue')
# plt.legend()
# plt.show();
directors = concatenated_df.director.value_counts()
directors
48/653:

# plt.plot(concatenated_df.value_counts(), label='revenue')
# plt.legend()
# plt.show();
directors = concatenated_df.director.value_counts()
directors.describe()
48/654:

plt.plot(revenue_mean, concatenated_df.director, label='revenue')
plt.legend()
plt.show();
directors = concatenated_df.director.value_counts()
48/655:

plt.plot(concatenated_df.revenue, concatenated_df.director, label='revenue')
plt.legend()
plt.show();
directors = concatenated_df.director.value_counts()
48/656:

sns.barplot(x= 'revenue', y='director', data=concatenated_df, palette='plasma', estimator=np.std)
plt.legend()
plt.show();
# directors = concatenated_df.director.value_counts()
48/657:
sns.set_style('darkgrid')
sns.barplot(x= 'revenue', y='director', data=concatenated_df, palette='plasma', estimator=np.mean());
# plt.legend()
# plt.show();
# directors = concatenated_df.director.value_counts()
48/658:
sns.set_style('darkgrid')
sns.barplot(x= 'original_title', y='director', data=concatenated_df, palette='plasma', estimator=np.mean());
# plt.legend()
# plt.show();
# directors = concatenated_df.director.value_counts()
48/659:
sns.set_style('darkgrid')
sns.barplot(x= 'original_title', y='director', data=concatenated_df, palette='plasma', estimator=np.std());
# plt.legend()
# plt.show();
# directors = concatenated_df.director.value_counts()
48/660:
sns.set_style('darkgrid')
sns.barplot(x= 'original_title', y='director', data=concatenated_df, palette='plasma', estimator=np.std);
# plt.legend()
# plt.show();
# directors = concatenated_df.director.value_counts()
48/661:
sns.set_style('darkgrid')
sns.barplot(x= 'original_title', y='director', data=concatenated_df, palette='plasma', estimator=np.mean);
# plt.legend()
# plt.show();
# directors = concatenated_df.director.value_counts()
48/662:
sns.set_style('darkgrid')
sns.barplot(x= 'original_title', y='director', data=concatenated_df, palette='plasma');
# plt.legend()
# plt.show();
# directors = concatenated_df.director.value_counts()
48/663:
sns.set_style('darkgrid')
sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# plt.legend()
# plt.show();
# directors = concatenated_df.director.value_counts()
48/664: concatenated_df.groupby('director').revenue.mean()
48/665: concatenated_df.groupby('director').revenue.mean().head()
48/666: concatenated_df.groupby('director').revenue
48/667: concatenated_df.groupby('director').revenue.sum()
48/668: concatenated_df.groupby('director').revenue.mean()
48/669:
rev_by_director = concatenated_df.groupby('director').revenue.mean()
plt.plot(rev_by_director);
48/670:
# sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# # plt.legend()
# # plt.show();
# # directors = concatenated_df.director.value_counts()
pd.plotting.scatter_matrix(rev_by_director);
48/671:
# sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# # plt.legend()
# # plt.show();
# # directors = concatenated_df.director.value_counts()
pd.plotting.scatter_matrix(concatenated_df.director);
48/672:
# sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# # plt.legend()
# # plt.show();
# # directors = concatenated_df.director.value_counts()
pd.scatter(concatenated_df.director);
48/673:
# sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# # plt.legend()
# # plt.show();
# # directors = concatenated_df.director.value_counts()
plt.scatter(concatenated_df.director);
48/674:
# sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# # plt.legend()
# # plt.show();
# # directors = concatenated_df.director.value_counts()
plt.scatter(concatenated_df.director, concatenated_df.revenue);
48/675:
# sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# # plt.legend()
# # plt.show();
# # directors = concatenated_df.director.value_counts()
plt.scatter(rev_by_director, concatenated_df.revenue);
48/676:
# sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# # plt.legend()
# # plt.show();
# # directors = concatenated_df.director.value_counts()
plt.scatter(concatenated_df.release_date, concatenated_df.revenue);
48/677:
sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# # plt.legend()
# # plt.show();
# # directors = concatenated_df.director.value_counts()
sns.lineplot(data=concatenated_df, x='revenue',y='popularity', hue='director');
48/678:
sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# # plt.legend()
# # plt.show();
# # directors = concatenated_df.director.value_counts()
sns.lineplot(data=concatenated_df, x='revenue',y='popularity');
48/679:
sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
# # plt.legend()
# # plt.show();
# # directors = concatenated_df.director.value_counts()
sns.lineplot(data=concatenated_df, x='popularity',y='revenue');
48/680:
sns.set_style('darkgrid')
# sns.barplot(x= 'director', y='revenue', data=concatenated_df, palette='plasma');
plt.legend()

sns.lineplot(data=concatenated_df, x='popularity',y='revenue');
48/681: sns.lineplot(data=concatenated_df, x='popularity',y='revenue');
48/682:
sns.set_style('darkgrid')
sns.barplot(x= 'popularity', y='revenue', data=concatenated_df, palette='plasma');
plt.legend()
48/683:
sns.set_style('darkgrid')
sns.boxplot(x= 'popularity', y='revenue', data=concatenated_df, palette='plasma')
plt.legend();
48/684:
plt.plot(revenue_mean, label='revenue')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean, label='budget')
plt.legend()
plt.size(10, 8)
plt.show();
48/685:
plt.plot(revenue_mean, label='revenue')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean, label='budget')
plt.legend()
plt.figure(10, 8)
plt.show();
48/686:
plt.plot(revenue_mean, label='revenue')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean, label='budget')
plt.legend()
plt.figure(revenue_mean, figsize=(10, 8))
plt.show();
48/687:
plt.plot(revenue_mean, label='revenue')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean, label='budget')
plt.legend()
plt.show();
48/688: concatenated_df.head(1)
48/689: concatenated_df.popularity.describe()
48/690:
# Bin edges that will be used to "cut" the data into groups of
# (min, 25%, 50%, 75%, max)
bin_edges = [0.000620, 0.345226, 0.604388, 1.083957, 3.260581]
48/691:
# name each ranking category
bin_names = ['High','Medium High','Medium', 'Low' ]
48/692:
# add a new column for popularity ranking and cut popularity column data into labeled bins
concatenated_df['popularity_rank'] = pd.cut(concatenated_df['popularity'], bin_edges, labels=bin_names)
48/693:
# view the new column
concatenated_df.head()
48/694:
# add a new column for popularity ranking and cut popularity column data into labeled bins
concatenated_df['popularity_rank'] = pd.cut(concatenated_df['popularity'], bin_edges, labels=bin_names)
48/695:
# view the new column
concatenated_df.head()
48/696:
# load data so here so that I don't have to go back and run past cells
df_cleaned = load_clean_data()
48/697:
# make sure it is loaded correctly
df_cleaned.head(2)
48/698:
# drop the numerical values
df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime', 'vote_count', 'vote_average'], axis=1, inplace=True)
48/699:
# confirm that they have been dropped
df_cleaned.head(1)
48/700:
# concatenate dfs
concatenated_df = pd.concat([df_numeric, df_cleaned], axis='columns')
48/701:
# confirm tha it worked
concatenated_df.info()
48/702: df_numeric.columns, df_cleaned.columns
48/703:
# check for null values
concatenated_df.isnull().sum()
48/704:
# drop missing values
concatenated_df.dropna(inplace=True)
48/705:
# confirming dropping
concatenated_df.isnull().sum()
48/706:
# check structure of the concatenated dataframe
concatenated_df.info()
48/707:
# View the distribution stats after removing outliers
concatenated_df.describe()
48/708: concatenated_df.hist(figsize=(10,10));
48/709:
# distribution stats for the popularity column
concatenated_df.popularity.describe()
48/710:
# bin edges that will be used to "cut" the data into groups of
# (min, 25%, 50%, 75%, max)
bin_edges = [0.000620, 0.345226, 0.604388, 1.083957, 3.260581]
48/711:
# name each ranking category
bin_names = ['High','Medium High','Medium', 'Low' ]
48/712:
# add a new column for popularity ranking and cut popularity column data into labeled bins
concatenated_df['popularity_rank'] = pd.cut(concatenated_df['popularity'], bin_edges, labels=bin_names)
48/713:
# view the new column
concatenated_df.head()
48/714:
# distribution stats for the popularity column
concatenated_df.revenue.describe()
48/715:
# group features by their mean
revenue_mean = concatenated_df.groupby('popularity_rank').revenue.mean()
runtime_mean = concatenated_df.groupby('popularity_rank').runtime.mean()
budget_mean = concatenated_df.groupby('popularity_rank').budget.mean()
48/716:
plt.plot(revenue_mean, label='revenue')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean, label='budget')
plt.legend()
plt.show();
48/717: concatenated_df.revenue_adj.describe()
48/718: concatenated_df.revenue.describe()
48/719: bin_edges = [0.0000, 0.0001, 8.9366, 6.1548, 3.1737]
48/720:
bin_edges = [0.0000, 0.0001, 8.9366, 6.1548, 3.1737]
bin_names = ['High','Medium High','Medium', 'Low' ]
48/721: concatenated_df['revenue_rank'] = pd.cut(concatenated_df['revenue'], bin_edges, labels=bin_names)
48/722:
bin_edges = [0.0000, 0.0010, 8.9366, 6.1548, 3.1737]
bin_names = ['High','Medium High','Medium', 'Low' ]
48/723: concatenated_df['revenue_rank'] = pd.cut(concatenated_df['revenue'], bin_edges, labels=bin_names)
48/724:
bin_edges = [0.0000, 0.0010, 8.93663e06, 6.154871e07, 3.173750e08]
bin_names = ['High','Medium High','Medium', 'Low' ]
48/725: concatenated_df['revenue_rank'] = pd.cut(concatenated_df['revenue'], bin_edges, labels=bin_names)
48/726: concatenated_df.head()
48/727:
popularity_mean = concatenated_df.groupby('revenue_rank').popularity.mean()
runtime_mean = concatenated_df.groupby('revenue_rank').runtime.mean()
budget_mean = concatenated_df.groupby('revenue_rank').budget.mean()
48/728:
plt.plot(popularity_mean, label='popularity')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean, label='budget')
plt.legend()
plt.show();
48/729:
low = popularity_mean['Low']
medium = popularity_mean['Medium']
medium_high = popularity_mean['Medium High']
high = popularity_mean['High']
plt.bar([4,3,2,1], [low, medium, medium_high, high]);
48/730:
low = popularity_mean['Low']
medium = popularity_mean['Medium']
medium_high = popularity_mean['Medium High']
high = popularity_mean['High']
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
# labels = ['Low', 'Medium', 'Medium High', 'High']
labels = popularity_mean.index.str.replace('_', ' ').str.title()
# plt.bar([4,3,2,1], [low, medium, medium_high, high]);
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Revenue')
plt.xlabel('Revenue')
plt.ylabel('Average popularity');
48/731: lt.plot(popularity_mean, label='popularity')
48/732: plt.plot(popularity_mean, label='popularity');
48/733: popularity_mean['Low']
48/734: popularity_mean['High']
48/735: concatenated_df.info()
48/736:
# load data so here so that I don't have to go back and run past cells
df_cleaned = load_clean_data()
48/737:
# make sure it is loaded correctly
df_cleaned.head(2)
48/738:
# drop the numerical values
df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime', 'vote_count', 'vote_average'], axis=1, inplace=True)
48/739:
# confirm that they have been dropped
df_cleaned.head(1)
48/740:
# concatenate dfs
concatenated_df = pd.concat([df_numeric, df_cleaned], axis='columns')
48/741:
# confirm tha it worked
concatenated_df.info()
48/742: df_numeric.columns, df_cleaned.columns
48/743:
# check for null values
concatenated_df.isnull().sum()
48/744:
# drop missing values
concatenated_df.dropna(inplace=True)
48/745:
# confirming dropping
concatenated_df.isnull().sum()
48/746:
# check structure of the concatenated dataframe
concatenated_df.info()
48/747:
# View the distribution stats after removing outliers
concatenated_df.describe()
48/748: concatenated_df.hist(figsize=(10,10));
48/749:
# distribution stats for the popularity column
concatenated_df.popularity.describe()
48/750:
# bin edges that will be used to "cut" the data into groups of
# (min, 25%, 50%, 75%, max)
bin_edges = [0.000620, 0.345226, 0.604388, 1.083957, 3.260581]
48/751:
# name each ranking category
bin_names = ['High','Medium High','Medium', 'Low' ]
48/752:
# add a new column for popularity ranking and cut popularity column data into labeled bins
concatenated_df['popularity_rank'] = pd.cut(concatenated_df['popularity'], bin_edges, labels=bin_names)
48/753:
# view the new column
concatenated_df.head()
48/754:
# group features by their mean
revenue_mean = concatenated_df.groupby('popularity_rank').revenue.mean()
runtime_mean = concatenated_df.groupby('popularity_rank').runtime.mean()
budget_mean = concatenated_df.groupby('popularity_rank').budget.mean()
48/755:
plt.plot(revenue_mean, label='revenue')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean, label='budget')
plt.legend()
plt.show();
48/756: concatenated_df.info()
48/757: sns.lineplot(data=concatenated_df, x='popularity',y='revenue');
48/758: concatenated_df.groupby(['director', 'original_title', 'release_date','genres', 'runtime','popularity']).revenue.mean().sort_values(ascending=False).head(15)
48/759: concatenated_df.director.value_counts()
48/760: concatenated_df.director.value_counts().sum()
48/761: concatenated_df.director.sum()
48/762: concatenated_df.director.sum().value_counts()
48/763: concatenated_df.director.value_counts()
48/764:
group1 = concatenated_df.query('director == John Carpenter')
group1
48/765:
concatenated_df.query('director == John Carpenter')
concatenated_df
48/766: concatenated_df.query('director == "John Carpenter"')
48/767:
new_df = concatenated_df.query('revenue == "0"')
new_df
48/768:
new_df = concatenated_df.query('revenue == "0.0"')
new_df
48/769: concatenated_df.query('revenue == "0.0"')
48/770: df_numeric.revenue == 0.0
48/771: df_numeric.revenue == 0.0.sum()
48/772: df_numeric.revenue == 0.0
48/773:
new_df = pd.concat([df_numeric, df_cleaned], axis='columns')
new_df
48/774:
new_df = pd.concat([df_numeric, df_cleaned], axis='columns')
new_df.dropna()
48/775:
new_df = pd.concat([df_numeric, df_cleaned], axis='columns')
new_df.dropna(inplace=True)
new_df.drop(new_df[new_df['revenue'] == 0.0].index, inplace=True)
48/776:
new_df = pd.concat([df_numeric, df_cleaned], axis='columns')
new_df.dropna(inplace=True)
new_df.drop(new_df[new_df['revenue'] == 0.0].index, inplace=True)
new_df
48/777:
new_df = pd.concat([df_numeric, df_cleaned], axis='columns')
new_df.dropna(inplace=True)
new_df.drop(new_df[new_df['revenue'] == 0.0].index, inplace=True)
new_df.query('director == "John Carpenter"')
48/778: pd.plotting.scatter_matrix(new_df, figsize=(10,10));
48/779: pd.plotting.scatter_matrix(new_df, figsize=(15,15));
48/780: plt.scatter(new_df, figsize=(8,8));
48/781: plt.scatter(new_df.popularity, new_df.revenue, figsize=(8,8));
48/782: plt.scatter(new_df.popularity, new_df.revenue);
48/783: new_df.describe()
48/784: concatenated_df.info()
48/785:
# load data so here so that I don't have to go back and run past cells
df_cleaned = load_clean_data()
48/786:
# make sure it is loaded correctly
df_cleaned.head(2)
48/787:
# drop the numerical values
df_cleaned.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 'runtime', 'vote_count', 'vote_average'], axis=1, inplace=True)
48/788:
# confirm that they have been dropped
df_cleaned.head(1)
48/789:
# concatenate dfs
concatenated_df = pd.concat([df_numeric, df_cleaned], axis='columns')
48/790:
# confirm tha it worked
concatenated_df.info()
48/791:
# check for null values
concatenated_df.isnull().sum()
48/792:
# drop missing values
concatenated_df.dropna(inplace=True)
48/793:
# confirming dropping
concatenated_df.isnull().sum()
48/794:
# check structure of the concatenated dataframe
concatenated_df.info()
48/795:
# View the distribution stats after removing outliers
concatenated_df.describe()
48/796: concatenated_df.info()
48/797: concatenated_df.revenue == 0.0
48/798: concatenated_df.query('revenue == "0.0"').sum()
48/799: concatenated_df.query('revenue == "0.0"').count()
48/800: concatenated_df.query('revenue == "0.0"').value_counts()
48/801: concatenated_df.query('revenue == "0.0"').value_counts()
48/802: concatenated_df.query(['revenue == "0.0"']).value_counts()
48/803: concatenated_df.groupby(['revenue == "0.0"']).value_counts()
48/804: concatenated_df.groupby(['revenue' == 0.0]).value_counts()
48/805: concatenated_df.groupby(['revenue' == 0.0])
48/806: concatenated_df.groupby(['revenue'])
48/807: concatenated_df.groupby(['revenue']).sum()
48/808:
# View the distribution stats after removing outliers
concatenated_df.describe()
48/809: concatenated_df.query('revenue == "0.0"')
48/810: concatenated_df.query('revenue == "0.0"').sum()
48/811: concatenated_df.query('revenue == "0.0"').value_counts()
48/812: concatenated_df.query(['revenue == "0.0"']).value_counts()
48/813: concatenated_df.query('revenue == "0.0"').count()
48/814: concatenated_df.revenue == 0.0
48/815: concatenated_df.drop(concatenated_df[concatenated_df['revenue'] == 0.0].index, inplace=True)
48/816: concatenated_df.info()
48/817: concatenated_df.revenue == 0.0
48/818: concatenated_df.describe.revenue()
48/819: concatenated_df.revenue.describe()
48/820:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
48/821:
def load_any(file):
    return file
48/822:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
48/823:
# Preview the first five rows of the Dataset

df.head()
48/824:
# Preview the last five rows of the Dataset
df.tail()
48/825: df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
50/1:
# check if the column has been renamed
df.columns
50/2:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
49/1:
def load_any(file):
    return file
49/2:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
49/3:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
49/4:
def load_any(file):
    return file
49/5:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
49/6:
# Preview the first five rows of the Dataset

df.head()
49/7:
# Preview the last five rows of the Dataset
df.tail()
49/8:
# View how many columns and rows are in the dataset.
df.info()
49/9: df.columns
49/10:
# View the data types of the columns of the Dataset
df.dtypes
49/11:
# View number of rows with duplicate values
df.duplicated().sum()
49/12:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
49/13:
# Check for missing values in the dataset
df.isnull().sum()
49/14: df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
49/15:
# check if the column has been renamed
df.columns
49/16: df.dropna(inplace=True)
49/17:
# check if there are missing values
df.isnull().sum()
49/18: df.drop_duplicates(inplace=True)
49/19:
# check if there are duplicates
df.duplicated().sum()
49/20: df.drop(['id', 'imdb_id', 'homepage', 'overview'], axis =1, inplace=True)
49/21:
# Check if the columns have been dropped
df.head(1)
49/22: df['release_date'] = pd.to_datetime(df.release_date)
49/23:
# check release_date datatype
df.dtypes
49/24: df['release_year'] = pd.to_datetime(df.release_year)
49/25:
# check release_year datatype
df.dtypes
49/26:
# Save cleaned data in another csv file
df.to_csv('tmdb_movies_cleaned.csv', index=False)
49/27:
def load_clean_data():
    clean_data = load_any(pd.read_csv('tmdb_movies_cleaned.csv'))
    return clean_data
49/28:
df_cleaned = load_clean_data()
df_cleaned.head()
49/29:
# A method to create scatter plots
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(df_cleaned.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
49/30:
# show relationship between popularity and revenue
plot_scatter(df_cleaned.popularity, df_cleaned.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue")
plt.colorbar();
49/31:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
49/32:
def load_any(file):
    return file
49/33:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
49/34:
# Preview the first five rows of the Dataset

df.head()
49/35:
# Preview the last five rows of the Dataset
df.tail()
49/36:
# View how many columns and rows are in the dataset.
df.info()
49/37: df.columns
49/38:
# View number of rows with duplicate values
df.duplicated().sum()
49/39:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
49/40:
# Check for missing values in the dataset
df.isnull().sum()
49/41: df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
49/42:
# check if the column has been renamed
df.columns
49/43: df.dropna(inplace=True)
49/44:
# check if there are missing values
df.isnull().sum()
49/45: df.drop_duplicates(inplace=True)
49/46:
# check if there are duplicates
df.duplicated().sum()
49/47: df.drop(['id', 'imdb_id', 'homepage', 'overview'], axis =1, inplace=True)
49/48:
# Check if the columns have been dropped
df.head(1)
49/49: df['release_date'] = pd.to_datetime(df.release_date)
49/50: df['budget', 'revenue'] = df['budget', 'revenue'].astype(float)
49/51: df['budget'] = df['budget].astype(float)
49/52: df['budget'] = df['budget'].astype(float)
49/53: df['revenue'] = df['revenue'].astype(float)
49/54:
# confirm successful datatype convertion
df.dtypes
49/55: df['release_year'] = pd.to_datetime(df.release_year)
49/56:
# confirm successful datatype convertion
df.dtypes
49/57: df.info()
49/58: df.drop(df[df['revenue'] == 0.0].index, inplace=True)
49/59: df.info()
49/60: df.describe()
49/61:
# find the movie that has a runtime of 705 minutes
df.runtime == 705
49/62:
# find the movie that has a runtime of 705 minutes
df.runtime.max()
49/63:
# find the movie that has a runtime of 705 minutes
df.query('runtime == 705')
49/64: df.drop(df[df['budget'] == 0.0].index, inplace=True)
49/65: df.info()
49/66: df.describe()
49/67:
# find the movie that has a runtime of 705 minutes
df.query('runtime == 705')
49/68: df.hist(figsize=(10,10));
49/69: sns.boxplot(x=df.revenue);
49/70: sns.boxplot(x=df.budget);
49/71: sns.boxplot(x=df.runtime);
49/72: sns.boxplot(x=df.revenue_adj);
49/73: sns.boxplot(x=df.budget_adj);
49/74: sns.boxplot(x=df.popularity);
49/75: sns.boxplot(x=df.vote_count);
49/76: sns.boxplot(x=df.vote_average);
49/77:
def make_boxplots(x_value):
    box_plot = sns.boxplot(x_value)
    return box_plot
49/78:
def make_boxplot(x_value):
    box_plot = sns.boxplot(x_value)
    return box_plot
49/79:
def make_boxplot(x_value):
    box_plot = sns.boxplot(x= x_value)
    return box_plot
49/80: make_boxplot(df.revenue);
49/81: make_boxplot(df.budget);
49/82: make_boxplot(df.runtime);
49/83: make_boxplot(df.revenue_adj);
49/84: make_boxplot(df.budget_adj);
49/85: make_boxplot(df.popularity);
49/86: make_boxplot(df.vote_count);
49/87: make_boxplot(df.vote_average);
49/88:
def find_iqr(lower, upper):
    Q1 = lower.quantile(0.25)
    Q3 = upper.quantile(0.75)
    return Q3 -Q1
49/89:
iqr_range = find_iqr(df, df)
iqr_range
49/90:
iqr = find_iqr(df, df)
iqr
49/91:
def find_iqr(lower, upper):
    Q1 = lower.quantile(0.25)
    Q3 = upper.quantile(0.75)
    IQR = Q3 - Q1
    
    below_iqr = df < (Q1 - 1.5 * IQR)
    above_iqr = df < (Q3 + 1.5 * IQR)
    
    df = df~[(below_iqr | above_iqr)]
    
    return df
49/92:
def find_iqr(lower, upper):
    Q1 = lower.quantile(0.25)
    Q3 = upper.quantile(0.75)
    IQR = Q3 - Q1
    
    below_iqr = df < (Q1 - 1.5 * IQR)
    above_iqr = df < (Q3 + 1.5 * IQR)
    
    df = df[~(below_iqr | above_iqr)]
    
    return df
49/93: df.isnull()
49/94: df.isnull().sum()
49/95:
def find_iqr(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    
    below_iqr = data < (Q1 - 1.5 * IQR)
    above_iqr = data < (Q3 + 1.5 * IQR)
    
    data = data[~(below_iqr | above_iqr)]
    
    return data
49/96: find_iqr(df)
49/97: numerical_df = df.copy()
49/98: numerical_df.head(1)
49/99:
def remove_outliers(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    
    below_iqr = data < (Q1 - 1.5 * IQR)
    above_iqr = data < (Q3 + 1.5 * IQR)
    
    
    data = data[~(below_iqr | above_iqr)]
    
    return data
49/100:
# a function that takes in the x value to be plotted
def make_boxplot(x_value):
    
    # assigns a value to the result of the sns boxplot function
    box_plot = sns.boxplot(x= x_value)
    
    #returns a boxplot
    return box_plot
49/101: make_boxplot(df.revenue);
49/102:
def remove_outliers(data):
    # calculate the 25% and 75% distributions using pandas
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    
    # subract the 25% from the 75% to get the interquartile range
    IQR = Q3 - Q1
    
    # define a condition to filter out outliers
    # if the value is less than 25%-(1.5IQR) then it's an outlier on the lower end
    # and if the value is more than 75%+(1.5IQR) then it's an outlier on the upper end
    
    below_iqr = data < (Q1 - 1.5 * IQR)
    above_iqr = data < (Q3 + 1.5 * IQR)
    
    # we generate a new dataframe, deleting values that are below or above the iqr
    data = data[~(below_iqr | above_iqr)]
    
    # returns the dataframe we just created
    return data
49/103: numerical_df = df.copy()
49/104: numerical_df.head(1)
49/105:
# call the function that removes outliers
numerical_df = remove_outliers(numerical_df)
49/106:
# drop all non-numerical values from the dataframe
numerical_df.drop(['release_year','tagline', 'genres', 
                   'keywords', 'director', 'cast', 'original_title', 
                   'prod_companies', 'release_date'], axis=1, inplace=True)
49/107: numerical_df.head(1)
49/108:
# confirm that we indeed only have numerical values
numerical_df.head(1)
49/109:
# call the function that removes outliers on the numerical_df
numerical_df = remove_outliers(numerical_df)
49/110:
# check that it worked correctly
numerical_df.describe()
49/111:
# check that it worked correctly
numerical_df.isnull().sum()
49/112: numerical_df.info()
49/113: numerical_df = df.copy()
49/114: numerical_df.head(1)
49/115:
# drop all non-numerical values from the dataframe
numerical_df.drop(['release_year','tagline', 'genres', 
                   'keywords', 'director', 'cast', 'original_title', 
                   'prod_companies', 'release_date'], axis=1, inplace=True)
49/116: numerical_df.info()
49/117:
def remove_outliers(data):
    # calculate the 25% and 75% distributions using pandas
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    
    # subract the 25% from the 75% to get the interquartile range
    IQR = Q3 - Q1
    
    # define a condition to filter out outliers
    # if the value is less than 25%-(1.5IQR) then it's an outlier on the lower end
    # and if the value is more than 75%+(1.5IQR) then it's an outlier on the upper end
    
    below_iqr = data < (Q1 - 1.5 * IQR)
    above_iqr = data > (Q3 + 1.5 * IQR)
    
    # we generate a new dataframe, deleting values that are below or above the iqr
    data = data[~(below_iqr | above_iqr)]
    
    # returns the dataframe we just created
    return data
49/118:
# call the function that removes outliers on the numerical_df
numerical_df = remove_outliers(numerical_df)
49/119: numerical_df.info()
49/120:
# check that it worked correctly
numerical_df.isnull().sum()
49/121:
# check that it worked correctly
numerical_df.describe()
49/122:
# confirm that we indeed only have numerical values
numerical_df.dtypes
49/123:
# let's check the number of rows and columns in our dataset 
# to see if it changes when we remove the outliers
numerical_df.info()
49/124: numerical_df = df.copy()
49/125: numerical_df.head(1)
49/126:
# drop all non-numerical values from the dataframe
numerical_df.drop(['release_year','tagline', 'genres', 
                   'keywords', 'director', 'cast', 'original_title', 
                   'prod_companies', 'release_date'], axis=1, inplace=True)
49/127:
# confirm that we indeed only have numerical values
numerical_df.dtypes
49/128:
# let's check the number of rows and columns in our dataset 
# to see if it changes when we remove the outliers
numerical_df.info()
49/129:
# call the function that removes outliers on the numerical_df
numerical_df = remove_outliers(numerical_df)
49/130:
# confirm the number of samples in the dataset has changed
# after removing the outliers
numerical_df.info()
49/131:
# check that it worked correctly
numerical_df.describe()
49/132:
# confirm if we have missing values
numerical_df.isnull().sum()
49/133:
# drop missing values
numerical_df.dropna(inplace=True)
49/134: numerical_df.isnull().sum()
49/135:
# check that it worked correctly
numerical_df.describe()
49/136: numerical_df = df.copy()
49/137: numerical_df.head(1)
49/138:
# drop all non-numerical values from the dataframe
numerical_df.drop(['release_year','tagline', 'genres', 
                   'keywords', 'director', 'cast', 'original_title', 
                   'prod_companies', 'release_date'], axis=1, inplace=True)
49/139:
# confirm that we indeed only have numerical values
numerical_df.dtypes
49/140:
# let's check the number of rows and columns in our dataset 
# to see if it changes when we remove the outliers
numerical_df.info()
49/141:
# call the function that removes outliers on the numerical_df
numerical_df = remove_outliers(numerical_df)
49/142:
# confirm the number of samples in the dataset has changed
# after removing the outliers
numerical_df.info()
49/143:
# confirm if we have missing values
numerical_df.isnull().sum()
49/144:
# drop missing values
numerical_df.dropna(inplace=True)
49/145: numerical_df.isnull().sum()
49/146:
# check that it worked correctly
numerical_df.describe()
49/147: nemerical.df.revenue.describe()
49/148: numerical.df.revenue.describe()
49/149: numerical_df.revenue.describe()
49/150: numerical_df.popularity.describe()
49/151: numerical_df.hist(figsize=(8,8))
49/152: pltl.scatter(numerical_df.revenue)
49/153: plt.scatter(numerical_df.revenue)
49/154: plt.scatter(numerical_df.popularity, numerical_df.revenue);
49/155: numerical_df = df.copy()
49/156: numerical_df.head(1)
49/157:
# drop all non-numerical values from the dataframe
numerical_df.drop(['release_year','tagline', 'genres', 
                   'keywords', 'director', 'cast', 'original_title', 
                   'prod_companies', 'release_date'], axis=1, inplace=True)
49/158:
# confirm that we indeed only have numerical values
numerical_df.dtypes
49/159:
# let's check the number of rows and columns in our dataset 
# to see if it changes when we remove the outliers
numerical_df.info()
49/160:
# call the function that removes outliers on the numerical_df
numerical_df = remove_outliers(numerical_df)
49/161:
# confirm the number of samples in the dataset has changed
# after removing the outliers
numerical_df.info()
49/162:
# confirm if we have missing values
numerical_df.isnull().sum()
49/163:
# confirm the number of samples in the dataset has changed
# after removing the outliers
numerical_df.shape
49/164:
# confirm the number of samples in the dataset has changed
# after removing the outliers
numerical_df.info()
49/165: df.shape
49/166:
df = df.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 
              'runtime', 'vote_count', 'vote_average'], axis=1, inplace=True)
49/167: df.shape
49/168:
df.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 
              'runtime', 'vote_count', 'vote_average'], axis=1, inplace=True)
49/169:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
49/170:
def load_any(file):
    return file
49/171:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
49/172:
# Preview the first five rows of the Dataset

df.head()
49/173:
# View the data types of the columns of the Dataset
df.dtypes
49/174: df.rename(columns={'production_companies': 'prod_companies'}, inplace=True)
49/175:
# check if the column has been renamed
df.columns
49/176: df.dropna(inplace=True)
49/177:
# check if there are missing values
df.isnull().sum()
49/178: df.drop_duplicates(inplace=True)
49/179:
# check if there are duplicates
df.duplicated().sum()
49/180: df.drop(['id', 'imdb_id', 'homepage', 'overview'], axis =1, inplace=True)
49/181:
# Check if the columns have been dropped
df.head(1)
49/182: df['release_date'] = pd.to_datetime(df.release_date)
49/183: df['release_year'] = pd.to_datetime(df.release_year)
49/184: df['budget'] = df['budget'].astype(float)
49/185: df['revenue'] = df['revenue'].astype(float)
49/186:
# confirm successful datatype convertion
df.dtypes
49/187: df.shape
49/188: df.drop(df[df['revenue'] == 0.0].index, inplace=True)
49/189: df.drop(df[df['budget'] == 0.0].index, inplace=True)
49/190: df.shape
49/191: df.describe()
49/192:
# a function that takes in the x value to be plotted
def make_boxplot(x_value):
    
    # assigns a value to the result of the sns boxplot function
    box_plot = sns.boxplot(x= x_value)
    
    #returns a boxplot
    return box_plot
49/193: make_boxplot(df.revenue);
49/194: make_boxplot(df.budget);
49/195: make_boxplot(df.runtime);
49/196: make_boxplot(df.revenue_adj);
49/197: make_boxplot(df.budget_adj);
49/198: make_boxplot(df.popularity);
49/199: make_boxplot(df.vote_count);
49/200: make_boxplot(df.vote_average);
49/201:
def remove_outliers(data):
    # calculate the 25% and 75% distributions using pandas
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    
    # subract the 25% from the 75% to get the interquartile range
    IQR = Q3 - Q1
    
    # define a condition to filter out outliers
    # if the value is less than 25%-(1.5IQR) then it's an outlier on the lower end
    # and if the value is more than 75%+(1.5IQR) then it's an outlier on the upper end
    
    below_iqr = data < (Q1 - 1.5 * IQR)
    above_iqr = data > (Q3 + 1.5 * IQR)
    
    # we generate a new dataframe, deleting values that are below or above the iqr
    data = data[~(below_iqr | above_iqr)]
    
    # returns the dataframe we just created
    return data
49/202: numerical_df = df.copy()
49/203: numerical_df.head(1)
49/204:
# drop all non-numerical values from the dataframe
numerical_df.drop(['release_year','tagline', 'genres', 
                   'keywords', 'director', 'cast', 'original_title', 
                   'prod_companies', 'release_date'], axis=1, inplace=True)
49/205:
# confirm that we indeed only have numerical values
numerical_df.dtypes
49/206:
# let's check the structure of our dataset 
# to see if it changes when we remove the outliers
numerical_df.shape
49/207: numerical_df.shape
49/208:
# call the function that removes outliers on the numerical_df
numerical_df = remove_outliers(numerical_df)
49/209:
# confirm the number of samples in the dataset has changed
# after removing the outliers
numerical_df.info()
49/210:
# confirm if we have missing values
numerical_df.isnull().sum()
49/211: df.shape
49/212:
df.drop(['revenue', 'revenue_adj', 'budget_adj', 'budget', 'popularity', 
              'runtime', 'vote_count', 'vote_average'], axis=1, inplace=True)
49/213: df.shape
49/214: Now we can combine the dataframes, it should go back to having 17 columns.
49/215:
# use concat to combine dataframes
concatenated_df = pd.concat([df_numeric, df], axis='columns')
49/216:
# use concat to combine dataframes
df_concatenated = pd.concat([numerical_df, df], axis='columns')
49/217:
# confirm tha it worked
df_concatenated.info()
49/218:
# confirm tha it worked
df_concatenated.shape
49/219: numerical_df.info()
49/220:
# drop missing values
numerical_df.dropna(inplace=True)
49/221: numerical_df.isnull().sum()
49/222: numerical_df.info()
49/223:
# use concat to combine dataframes
df_concatenated = pd.concat([numerical_df, df], axis='columns')
49/224:
# confirm tha it worked
df_concatenated.shape
49/225: df_contatenated.info()
49/226: df_contenated.info()
49/227: df_concatenated.info()
49/228:
# drop missing values
df_concatenated.dropna(inplace=True)
49/229: df_concatenated.isnull().sum()
49/230: df_concatenated.info()
49/231:
# check that trimming outliers worked correctly
df_concatenated.describe()
49/232:
# Save cleaned data in another csv file
df_concatenated.to_csv('tmdb_movies_version2.csv', index=False)
49/233:
# load previously cleaned data
def load_clean_data():
    clean_data = load_any(pd.read_csv('tmdb_movies_version2.csv'))
    return clean_data
49/234:
movies_df = load_clean_data()
movies_df.head()
49/235:
movies_df = load_clean_data()
movies_df.info()
49/236:
# A method to create scatter plots
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(movies_df.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
49/237:
# show relationship between popularity and revenue
plot_scatter(movies_df.popularity, movies_df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue");
49/238:
# A method to create scatter plots
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(movies_df.index))])
    plt.figure(figsize=(8,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
49/239:
# show relationship between popularity and revenue
plot_scatter(movies_df.popularity, movies_df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue");
49/240:
# show relationship between popularity and revenue
plot_scatter(movies_df.popularity, movies_df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue");
plt.colorbar();
49/241:
# A method to create scatter plots
def plot_scatter(x_value, y_value):
    colors = np.array([range(0, len(movies_df.index))])
    plt.figure(figsize=(10,8))
    return plt.scatter(x_value, y_value, c=colors, cmap='viridis')
49/242:
# show relationship between popularity and revenue
plot_scatter(movies_df.popularity, movies_df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue");
plt.colorbar();
49/243: movies_df.revenue.sum()
49/244: movies_df.popularity.sum()
49/245:
# show relationship between popularity and revenue
plot_scatter(movies_df.popularity, movies_df.revenue)
plt.xlabel("revenue")
plt.ylabel("popularity");
plt.colorbar();
49/246:
# show relationship between popularity and revenue
plot_scatter(movies_df.popularity, movies_df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue");
plt.colorbar();
49/247:
# show relationship between popularity and revenue
plot_scatter(movies_df.revenue, movies_df.popularity)
plt.xlabel("popularity")
plt.ylabel("revenue");
plt.colorbar();
49/248:
# show relationship between popularity and revenue
plot_scatter(movies_df.revenue, movies_df.popularity)
plt.xlabel("revenue")
plt.ylabel("popularity");
plt.colorbar();
49/249:
# show relationship between popularity and revenue
plot_scatter(movies_df.popularity, movies_df.revenue)
plt.xlabel("popularity")
plt.ylabel("revenue");
plt.colorbar();
49/250:
# show relationship between budget and revenue
plot_scatter(movies_df.budget, movies_df.revenue)
plt.xlabel("budget")
plt.ylabel("revenue")
plt.colorbar();
49/251:
# distribution stats for the popularity column
movies_df.revenue.describe()
49/252:
# distribution stats for the revenue column
movies_df.revenue.describe()
49/253:
# view the new column
movies_df.head()
49/254: plt.plot(movies_df.revenue, label='revenue');
49/255:
# view the new column
movies_df.head(1)
49/256:
# show relationship between runtime and revenue
plot_scatter(movies_df.runtime, movies_df.revenue)
plt.xlabel("runtime")
plt.ylabel("revenue")
plt.colorbar();
49/257:
# bin edges that will be used to "cut" the data into groups of
# (min, 25%, 50%, 75%, max)
bin_edges = [4.300000e+01, 1.785102e+07, 5.919213e+07, 1.325476e+08, 4.846358e+08]
49/258:
# name each ranking category
bin_names = ['High','Medium High','Medium', 'Low' ]
49/259:
# add a new column for popularity ranking and cut popularity column data into labeled bins
concatenated_df['revenue_rank'] = pd.cut(movies_df['revenue'], bin_edges, labels=bin_names)
49/260:
# add a new column for popularity ranking and cut popularity column data into labeled bins
movies_df['revenue_rank'] = pd.cut(movies_df['revenue'], bin_edges, labels=bin_names)
49/261:
# view the new column
movies_df.head(1)
49/262:
# group features by their mean
popularity_mean = movies_df.groupby('revenue_rank').popularity.mean()
runtime_mean = movies_df.groupby('revenue_rank').runtime.mean()
budget_mean = movies_df.groupby('revenue_rank').budget.mean()
49/263:
plt.plot(popularity_mean, label='popularity')
plt.plot(runtime_mean, label='runtime')
plt.plot(budget_mean, label='budget')
plt.legend()
plt.show();
49/264:
plt.plot(movies_df.revenue, label='revenue')
plt.plot(movies_df.budget, label='budget');
49/265:
plt.plot(movies_df.revenue, label='revenue')
plt.legen()
plt.plot(movies_df.budget, label='budget');
49/266:
plt.plot(movies_df.revenue, label='revenue')
plt.legend()
plt.plot(movies_df.budget, label='budget');
49/267:
plt.plot(movies_df.revenue, label='revenue')

plt.plot(movies_df.budget, label='budget')
plt.legend();
49/268:
plt.plot(movies_df.revenue, label='revenue')
plt.plot(movies_df.popularity, label='popularity')

plt.plot(movies_df.budget, label='budget')
plt.legend();
49/269:
plt.plot(movies_df.revenue, label='revenue')
plt.plot(movies_df.popularity, label='popularity')
plt.plot(movies_f.runtime, label='runtime')
plt.plot(movies_df.budget, label='budget')
plt.legend();
49/270:
plt.plot(movies_df.revenue, label='revenue')
plt.plot(movies_df.popularity, label='popularity')
plt.plot(movies_df.runtime, label='runtime')
plt.plot(movies_df.budget, label='budget')
plt.legend();
49/271:
plt.plot(movies_df.revenue, label='revenue')
plt.plot(movies_df.budget, label='budget')
plt.plot(movies_df.popularity, label='popularity')
plt.plot(movies_df.runtime, label='runtime')

plt.legend();
49/272: sns.lineplot(data=movies_df, x='popularity',y='revenue');
49/273: sns.lineplot(data=movies_df, x='budget',y='revenue');
49/274: sns.lineplot(data=movies_df, x='budget',y='revenue');
49/275: sns.lineplot(data=movies_df, x='runtime',y='revenue');
49/276: sns.lineplot(data=movies_df, x='popularity',y='revenue');
49/277:
low = popularity_mean['Low']
medium = popularity_mean['Medium']
medium_high = popularity_mean['Medium High']
high = popularity_mean['High']
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
# labels = ['Low', 'Medium', 'Medium High', 'High']
labels = mean.index.str.replace('_', ' ').str.title()
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Popularity Ratings by Revenue')
plt.xlabel('Revenue')
plt.ylabel('Average Popularity Rating');
49/278:
low = popularity_mean['Low']
medium = popularity_mean['Medium']
medium_high = popularity_mean['Medium High']
high = popularity_mean['High']
locations = [4,3,2,1]
heights = [low, medium, medium_high, high]
# labels = ['Low', 'Medium', 'Medium High', 'High']
labels = popularity_mean.index.str.replace('_', ' ').str.title()
plt.bar(locations, heights, tick_label=labels)
plt.title('Average Popularity Ratings by Revenue')
plt.xlabel('Revenue')
plt.ylabel('Average Popularity Rating');
49/279: plt.plot(popularity_mean);
49/280: plt.plot(runtime_mean);
49/281: plt.plot(budget_mean);
49/282: movies_data.directors.unique()
49/283: movies_df.directors.unique()
49/284: movies_df.director.unique()
49/285: movies_df.director.unique().sum()
49/286: movies_df.director.unique().value_counts()
49/287: movies_df.director.value_counts()
49/288: movies_df.genre.value_counts()
49/289: movies_df.genres.value_counts()
49/290: movies_df.revenue.value_counts()
49/291: movies_df.revenue.unique()
49/292: movies_df.keywords.unique()
49/293: movies_df.keywords.value_counts()
49/294: movies_df.tagline.value_counts()
49/295: movies_df.cast.value_counts()
49/296: movies_df.cast.unique()
49/297: movies_df.cast.unique().sum()
49/298: movies_df.cast.unique()
49/299:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
49/300:
def load_any(file):
    return file
49/301:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
49/302:
# Preview the first five rows of the Dataset

df.head()
49/303:
# Preview the last five rows of the Dataset
df.tail()
49/304:
# view unique values in the director column
movies_df.director.nunique()
49/305:
# view unique values in the director column
movies_df.director.nunique().sum()
49/306:
# view unique values in the director column
movies_df.director.nunique()
49/307:
# view unique values in the director column
movies_df.director.unique()
49/308:
# view unique values in the director column
movies_df.director.unique().value_counts()
49/309:
# view unique values in the director column
movies_df.director.unique()
49/310:
# view unique values in the director column
lenn(movies_df.director.unique())
49/311:
# view unique values in the director column
len(movies_df.director.unique())
49/312:
# view unique values in the director column
movies_df.director.unique()
49/313: movies_df.director.nunique()
49/314:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
49/315:
def load_any(file):
    return file
49/316:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
49/317:
# Preview the first five rows of the Dataset

df.head()
49/318:
# Preview the last five rows of the Dataset
df.tail()
49/319:
# View how many columns and rows are in the dataset.
df.info()
49/320: df.columns
49/321:
# View the data types of the columns of the Dataset
df.dtypes
49/322:
# View number of rows with duplicate values
df.duplicated().sum()
49/323:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
49/324:
# Check for missing values in the dataset
df.isnull().sum()
49/325:
# view unique values in the director column
movies_df.director.unique()
49/326: movies_df.director.nunique()
49/327:
# check number of unique directors
movies_df.director.nunique()
49/328:
# view unique cast members
movies_df.cast.unique()
49/329:
def load_any(file):
    return file
49/330:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
49/331:
# Preview the first five rows of the Dataset

df.head()
49/332:
# Preview the last five rows of the Dataset
df.tail()
49/333:
# View how many columns and rows are in the dataset.
df.info()
49/334: df.columns
49/335:
# View the data types of the columns of the Dataset
df.dtypes
49/336:
# View number of rows with duplicate values
df.duplicated().sum()
49/337:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
49/338:
# Check for missing values in the dataset
df.isnull().sum()
49/339:
# view unique values in the director column
df.director.unique()
49/340:
# view unique values in the director column
df.director.unique()
49/341:
# Import Packages

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
49/342:
def load_any(file):
    return file
49/343:
# Loading TMDB Movies Data

df = load_any(pd.read_csv('tmdb-movies.csv'))
49/344:
# Preview the first five rows of the Dataset

df.head()
49/345:
# Preview the last five rows of the Dataset
df.tail()
49/346:
# View how many columns and rows are in the dataset.
df.info()
49/347: df.columns
49/348:
# View the data types of the columns of the Dataset
df.dtypes
49/349:
# View number of rows with duplicate values
df.duplicated().sum()
49/350:
# Filter duplicate row in all columns
df.loc[df.original_title == 'TEKKEN']
49/351:
# Check for missing values in the dataset
df.isnull().sum()
49/352:
# view unique values in the director column
df.director.unique()
49/353:
# check number of unique directors
movies_df.director.nunique()
49/354:
# check number of unique directors
df.director.nunique()
49/355:
# view unique values in the director column
df.director.unique()
49/356:
# view unique values in the director column
df.director.unique()
49/357:
# check number of unique directors
df.director.nunique()
49/358:
# view unique cast members
movies_df.cast.unique()
49/359:
# view unique cast members
df.cast.unique()
49/360:
# check number of unique cast members
df.cast.nunique()
49/361: df.cast
49/362: df.head(10)
49/363: df.original_title.unique()
49/364: df.original_title.nunique()
49/365:
# view unique genres
df.genres.unique()
49/366:
# check number of unique genres
df.genres.nunique()
   1: %history -g -f history.txt
